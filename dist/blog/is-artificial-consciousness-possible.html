<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

    <meta name="author" content="Sentience Institute" />

    <meta property="og:site_name" content="Sentience Institute" />
    <meta property="fb:app_id" content="302735083502826" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:site" content="@sentienceinst" />

    
    

    
    <meta property="description" content="Many philosophers and scientists have written about whether artificial sentience or consciousness is possible. In this blog post we summarize discussions of the topic from 15 books." />
    <meta property="og:description" content="Many philosophers and scientists have written about whether artificial sentience or consciousness is possible. In this blog post we summarize discussions of the topic from 15 books." />
    
    
    <title>Sentience Institute | Is Artificial Consciousness Possible? A Summary of Selected Books</title>
    <meta property="title" content="Is Artificial Consciousness Possible? A Summary of Selected Books" />
    <meta property="og:title" content="Is Artificial Consciousness Possible? A Summary of Selected Books" />
    
    
    
    
    <meta property="og:url" content="http://www.sentienceinstitute.org/blog/is-artificial-consciousness-possible" />
    <meta property="og:image" content="http://www.sentienceinstitute.org/img/blog/20220613.png" />
    


    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico?v=1">
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,600" rel="stylesheet">
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="../css/sentienceinstitute.css?v=2.0.1" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <link href="../css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <nav class="navbar navbar-inverse navbar-toggleable-sm fixed-top">
      <div class="container">
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarText" aria-controls="navbarText" aria-expanded="false">
          <span class="navbar-toggler-icon"></span>
        </button>
        <a class="navbar-brand" href="/">
          <!-- <img class="nav-logo" src="../img/logo/SI_logo_white_200px.png"/> -->
          <img class="nav-logo-brandmark" src="../img/logo/SI_brandmark_white_heavier_web.png"/>
          <div class="nav-logo-text">
            <span>Sentience</span>
            <span class="nav-logo-text-institute">Institute</span>
          </div>
        </a>
        <div id="navbarText" class="collapse navbar-collapse">
          <ul class="navbar-nav">
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">
                Research<span class="caret"></span>
              </a>
              <ul class="dropdown-menu">
                <li><a href="/research-agenda">Agenda</a></li>
                <li><a href="/foundational-questions-summaries">Foundational Questions for Animal Advocacy</a></li>
                <li><a href="/research">Reports</a></li>
                <!-- <li><a href="/press">Press Releases</a></li> -->
              </ul>
            </li>
            <li class="nav-item"><a class="nav-link" destination="/media">Media</a></li>
            <li class="nav-item"><a class="nav-link" destination="/podcast">Podcast</a></li>
            <li class="nav-item"><a class="nav-link" destination="/blog">Blog</a></li>
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">
                About Us<span class="caret"></span>
              </a>
              <ul class="dropdown-menu">
                <li><a href="/mission">Our Mission</a></li>
                <li><a href="/perspective">Our Perspective</a></li>
                <li><a href="/team">Our Team</a></li>
                <li class="nav-item"><a class="nav-link" href="/get-involved">Get Involved</a></li>
                <li><a href="/transparency">Transparency</a></li>
                <li><a href="/faq">FAQ</a></li>
              </ul>
            </li>
            <li class="nav-item nav-donate"><a class="nav-link" destination="/donate">Donate</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>
    




<div class="container image-container" img-id="20220613" style="background-image: url(/img/blog/20220613.png);">
  
  
  

<div data-nosnippet class="container image-info-container">
  <div class="image-credit">
    <i class="material-icons photo-icon">photo_camera</i>
    <span>
      Infralist.com / Unsplash
    </span>
  </div>
  <div class="image-title">
    <div class="image-title-text">
      Computer motherboard
    </div>
    <div class="arrow-down"></div>
  </div>
</div>


  
</div>

<div class="container first-container gdoc-html-container blog-container is-artificial-consciousness-possible-container">
  <div class="title">
    Is Artificial Consciousness Possible? A Summary of Selected Books
  </div>
  <div class="author-info">
    
    <div class="author">
      <div class="author-img"><img src="../img/team/ali.png"/></div>
      <div class="author-name-and-role">
      <div class="author-name">Ali Ladak</div>
      <div class="author-role">Researcher</div>
    </div>
  </div>
  
  </div>
  <div class="date">
    June 13, 2022
  </div>
  <html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"></head><body class="c26"><p class="c4"><span class="c3 c14 c19">Many thanks to Tobias Baumann, Max Carpendale, and Michael St. Jules for their helpful comments and discussion. Thanks to Jacy Reese Anthis and Janet Pauketat for more extensive discussion and editing.</span></p><h1 class="c6" id="h.1fob9te"><span class="c17 c5">Table of Contents</span></h1><p class="c15"><span class="c2 c12"><a class="c1" href="#h.3znysh7">Introduction</a></span></p><p class="c15"><span class="c2 c12"><a class="c1" href="#h.2et92p0">John Searle, </a></span><span class="c2 c10 c14"><a class="c1" href="#h.2et92p0">The Rediscovery of the Mind</a></span><span class="c2 c12"><a class="c1" href="#h.2et92p0">, 1992</a></span></p><p class="c15"><span class="c2 c12"><a class="c1" href="#h.tyjcwt">Daniel Dennett, </a></span><span class="c2 c14 c10"><a class="c1" href="#h.tyjcwt">Consciousness Explained</a></span><span class="c2 c12"><a class="c1" href="#h.tyjcwt">, 1993</a></span></p><p class="c15"><span class="c2 c12"><a class="c1" href="#h.3dy6vkm">David Chalmers, </a></span><span class="c2 c14 c10"><a class="c1" href="#h.3dy6vkm">The Conscious Mind: In Search of a Fundamental Theory</a></span><span class="c2 c12"><a class="c1" href="#h.3dy6vkm">, 1995</a></span></p><p class="c15"><span class="c2 c12"><a class="c1" href="#h.1t3h5sf">Thomas Metzinger, </a></span><span class="c2 c14 c10"><a class="c1" href="#h.1t3h5sf">The Ego Tunnel: The Science of the Mind and the Myth of the Self</a></span><span class="c2 c12"><a class="c1" href="#h.1t3h5sf">, 2010</a></span></p><p class="c15"><span class="c2 c12"><a class="c1" href="#h.4d34og8">Stanislas Dehaene, </a></span><span class="c2 c14 c10"><a class="c1" href="#h.4d34og8">Consciousness and the Brain: Deciphering How the Brain Codes Our Thoughts</a></span><span class="c2 c12"><a class="c1" href="#h.4d34og8">, 2014</a></span></p><p class="c15"><span class="c2 c12"><a class="c1" href="#h.2s8eyo1">Michael Tye, </a></span><span class="c2 c14 c10"><a class="c1" href="#h.2s8eyo1">Tense Bees and Shell-Shocked Crabs: Are Animals Conscious?</a></span><span class="c2 c12"><a class="c1" href="#h.2s8eyo1">, 2017</a></span></p><p class="c15"><span class="c2 c12"><a class="c1" href="#h.17dp8vu">Susan Blackmore, </a></span><span class="c2 c14 c10"><a class="c1" href="#h.17dp8vu">Consciousness: An Introduction</a></span><span class="c2 c12"><a class="c1" href="#h.17dp8vu">, 2018</a></span></p><p class="c15"><span class="c2 c12"><a class="c1" href="#h.3rdcrjn">Susan Schneider, </a></span><span class="c2 c14 c10"><a class="c1" href="#h.3rdcrjn">Artificial You: AI and the Future of Your Mind</a></span><span class="c2 c12"><a class="c1" href="#h.3rdcrjn">, 2019</a></span></p><p class="c15"><span class="c2 c12"><a class="c1" href="#h.26in1rg">Michael Graziano, </a></span><span class="c2 c14 c10"><a class="c1" href="#h.26in1rg">Rethinking Consciousness: A Scientific Theory of Subjective Experience</a></span><span class="c2 c12"><a class="c1" href="#h.26in1rg">, 2019</a></span></p><p class="c15"><span class="c2 c12"><a class="c1" href="#h.lnxbz9">Philip Goff, </a></span><span class="c2 c14 c10"><a class="c1" href="#h.lnxbz9">Galileo&rsquo;s Error: Foundations for a New Science of Consciousness</a></span><span class="c2 c12"><a class="c1" href="#h.lnxbz9">, 2019</a></span></p><p class="c15"><span class="c2 c12"><a class="c1" href="#h.35nkun2">Simona Ginsburg and Eva Jablonka, </a></span><span class="c2 c14 c10"><a class="c1" href="#h.35nkun2">The Evolution of the Sensitive Soul: Learning and the Origins of Consciousness</a></span><span class="c2 c12"><a class="c1" href="#h.35nkun2">, 2019</a></span></p><p class="c15"><span class="c2 c12"><a class="c1" href="#h.1ksv4uv">Peter Godfrey-Smith, </a></span><span class="c2 c14 c10"><a class="c1" href="#h.1ksv4uv">Metazoa: Animal Life and the Birth of the Mind</a></span><span class="c2 c12"><a class="c1" href="#h.1ksv4uv">, 2020</a></span></p><p class="c15"><span class="c2 c12"><a class="c1" href="#h.44sinio">Kristof Koch, </a></span><span class="c2 c14 c10"><a class="c1" href="#h.44sinio">The Feeling of Life Itself: Why Consciousness Is Widespread but Can&#39;t Be Computed</a></span><span class="c2 c12"><a class="c1" href="#h.44sinio">, 2020</a></span></p><p class="c15"><span class="c2 c12"><a class="c1" href="#h.2jxsxqh">Anil Seth, </a></span><span class="c2 c14 c10"><a class="c1" href="#h.2jxsxqh">Being You: A New Science of Consciousness</a></span><span class="c2 c12"><a class="c1" href="#h.2jxsxqh">, 2021</a></span></p><p class="c15"><span class="c2 c12"><a class="c1" href="#h.z337ya">David Chalmers, </a></span><span class="c2 c14 c10"><a class="c1" href="#h.z337ya">Reality+: Virtual Worlds and the Problems of Philosophy</a></span><span class="c2 c12"><a class="c1" href="#h.z337ya">, 2022</a></span></p><h1 class="c6" id="h.3znysh7"><span class="c17 c5">Introduction</span></h1><p class="c4"><span class="c5">Many philosophers and scientists have written about whether artificial sentience or consciousness is possible.</span><sup class="c13"><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup><span class="c5">&nbsp;In this blog post we summarize discussions of the topic from 15 books.</span><sup class="c13"><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup><span class="c0">&nbsp;The books were chosen based on their popularity and representation of a range of perspectives on artificial consciousness. They were not randomly sampled from all of the books written on the topic. For brevity, we simply summarize the claims made by the authors, rather than critique or respond to them.</span></p><p class="c4"><span class="c5">While the books contain a wide variety of terminology, we can categorize the ways they assess the possibility of artificial consciousness into three broad approaches:</span><sup class="c13"><a href="#ftnt3" id="ftnt_ref3">[3]</a></sup><sup class="c13"><a href="#ftnt4" id="ftnt_ref4">[4]</a></sup></p><ol class="c25 lst-kix_list_2-0 start" start="1"><li class="c9 li-bullet-0"><span class="c0">The </span><span class="c3 c19 c14">computational</span><span class="c0">&nbsp;approach abstracts away from the specific implementation details of a cognitive system, such as whether it is implemented in carbon versus silicon substrate. Instead, it focuses on a higher level of analysis: the computations, algorithms, or programs that a cognitive system runs to generate its behavior. Another way of putting this is that it focuses on the software a system is running, rather than on the system&rsquo;s hardware. The computational approach is standard in the field of cognitive science (e.g., </span><span class="c2 c12"><a class="c1" href="https://www.wiley.com/en-us/The%2BPhilosophy%2Bof%2BCognitive%2BScience-p-9780745646572">Cain, 2015</a></span><span class="c0">) and suggests that if artificial entities implement certain computations, they will be conscious. The specific algorithms or computations that are thought to give rise to or be constitutive of consciousness differ. For example, </span><span class="c2 c12"><a class="c1" href="#h.1t3h5sf">Metzinger (2010)</a></span><span class="c0">&nbsp;emphasizes the importance of an internal self-model, whereas </span><span class="c2 c12"><a class="c1" href="#h.4d34og8">Dehaene (2014)</a></span><span class="c0">&nbsp;emphasizes the importance of a &ldquo;global workspace,&rdquo; in which information becomes available for use by multiple </span><span class="c5">subsystems</span><span class="c0">. Out of the three approaches, the computational approach typically projects the largest number of conscious artificial entities existing in the future because computational criteria are arguably easiest for an AI system to achieve. </span></li><li class="c9 li-bullet-0"><span class="c0">The </span><span class="c3 c19 c14">physical </span><span class="c0">approach focuses on the physical details of how a cognitive system is implemented; that is, it focuses on a system&rsquo;s hardware rather than its software.</span><sup class="c19 c5 c13 c20"><a href="#ftnt5" id="ftnt_ref5">[5]</a></sup><span class="c0">&nbsp;For example, </span><span class="c2 c12"><a class="c1" href="#h.44sinio">Koch (2020)</a></span><span class="c0">&nbsp;defends Integrated Information Theory (IIT), in which the degree of consciousness in a system depends on its degree of integrated information, that is, the degree to which the system is causally interconnected such that it is not reducible to its individual components. This integrated information needs to be present at the physical, hardware level of a system.</span><sup class="c19 c5 c20 c13"><a href="#ftnt6" id="ftnt_ref6">[6]</a></sup><span class="c0">&nbsp;According to Koch, the hardware of current digital computers has very little integrated information, so they could not be conscious no matter what cognitive system they implement at the software level (e.g., a </span><span class="c2 c12"><a class="c1" href="https://link.springer.com/chapter/10.1007/978-3-642-31674-6_19">whole brain emulation</a></span><span class="c0">). However, only the physical organization matters, not the specific substrate the system is implemented in. Thus, although artificial consciousness is possible on the physical approach, it typically predicts fewer conscious artificial entities than the computational approach.</span><sup class="c19 c18 c20 c13"><a href="#ftnt7" id="ftnt_ref7">[7]</a></sup></li><li class="c4 c28 li-bullet-0"><span class="c0">The </span><span class="c3 c19 c14">biological</span><span class="c0">&nbsp;approach also focuses on the physical details of how a cognitive system is implemented, but it additionally emphasizes some specific aspect of biology as important for consciousness. For example, </span><span class="c2 c12"><a class="c1" href="#h.1ksv4uv">Godfrey-Smith (2020)</a></span><span class="c0">&nbsp;suggests that it would be very difficult to have a conscious system that isn&rsquo;t physically very similar to the brain because of some of the dynamic patterns involved in consciousness in brains. However, when pressed, even these views tend to allow for the possibility of artificial consciousness. Godfrey-Smith says that future robots with &ldquo;genuinely brain-like control systems&rdquo; could be conscious, and John Searle, perhaps the most well-known proponent of a biological approach, has </span><span class="c2 c12"><a class="c1" href="https://en.wikipedia.org/wiki/Biological_naturalism">said</a></span><span class="c2 c12">,</span><span class="c0">&nbsp;&ldquo;The fact that brain processes cause consciousness does not imply that only brains can be conscious. The brain is a biological machine, and we might build an artificial machine that was conscious; just as the heart is a machine, and we have built artificial hearts. Because we do not know exactly how the brain does it we are not yet in a position to know how to do it artificially.&rdquo; Still, the biological approach is skeptical of the possibility of artificial consciousness and the number of future conscious artificial entities is predicted to be smaller than on both the computational and physical approaches; a physical system would need to closely resemble biological brains to be conscious.</span></li></ol><p class="c4"><span class="c0">Overall, there is a broad consensus among the books that artificial consciousness is possible. According to the computational approach, which is the mainstream view in cognitive science, artificial consciousness is not only possible, but is likely to come about in the future, potentially in very large numbers. The physical and biological approaches predict that artificial consciousness will be far less widespread. Artificial sentience as an effective altruism cause area is, therefore, more likely to be promising if one favors the computational approach over the physical and biological approaches.</span></p><p class="c4"><span class="c5">Which approach should we favor? Several of the books provide arguments. For example, </span><span class="c2"><a class="c1" href="#h.3dy6vkm">Chalmers (1995)</a></span><span class="c5">&nbsp;uses a </span><span class="c2"><a class="c1" href="http://consc.net/papers/qualia.html">Silicon Chip Replacement</a></span><span class="c5">&nbsp;thought experiment to argue that a functionally identical silicon copy of a human brain would have the same conscious experience as a biological human brain, and from there goes on to defend a general computational account. </span><span class="c2"><a class="c1" href="#h.2et92p0">Searle (1992)</a></span><span class="c5">&nbsp;uses the </span><span class="c2"><a class="c1" href="https://en.wikipedia.org/wiki/Chinese_room">Chinese Room thought experiment</a></span><span class="c5">&nbsp;to argue that computational accounts necessarily leave out some aspects of our mental lives, such as understanding. </span><span class="c2"><a class="c1" href="#h.3rdcrjn">Schneider (2019)</a></span><span class="c0">&nbsp;argues that we don&rsquo;t yet have enough information to decide between different approaches and advocates for a &ldquo;wait and see&rdquo; approach. The approach that one subscribes to will depend on how convincing they find these and other arguments.</span></p><p class="c4"><span class="c5">Many of the perspectives summarized in this post consider the ethical implications of creating artificial consciousness. In a popular textbook on consciousness, </span><span class="c2"><a class="c1" href="#h.17dp8vu">Blackmore (2018)</a></span><span class="c5">&nbsp;argues that if we create artificial sentience, they will be capable of suffering, and we will therefore have moral responsibilities towards them. Practical suggestions from the books for how to deal with the ethical issues range from an outright ban on developing artificial consciousness until we have more information (</span><span class="c2"><a class="c1" href="#h.1t3h5sf">Metzinger, 2010</a></span><span class="c5">), to the view that we should deliberately try to implement consciousness in AI as a way of reducing the likelihood that future powerful AI systems will cause us harm (</span><span class="c2"><a class="c1" href="#h.26in1rg">Graziano, 2019</a></span><span class="c0">). Figuring out which of these and other strategies will be most beneficial is an important topic for future research.</span></p><h1 class="c6" id="h.2et92p0"><span>John Searle, </span><span class="c10">The Rediscovery of the Mind</span><span class="c17 c5">, 1992</span></h1><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 33%; float: right; padding: 5px; height: auto;"><img alt="" src="images/is-artificial-consciousness-possible/image7.png" style="width: 33%; float: right; padding: 5px; height: auto; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h1><p class="c4"><span class="c5">Searle argues against the computational approach to the mind. His most well-known objection is the </span><span class="c2"><a class="c1" href="https://en.wikipedia.org/wiki/Chinese_room">Chinese Room argument</a></span><span class="c0">. The argument asks us to imagine a non-Chinese speaker locked in a room with a large batch of (unbeknown to them) Chinese writing and a set of instructions written in a language they understand. The instructions tell the person how to match up and respond to inputs arriving through a slot in the door to the room, which are questions in Chinese. As the person responds with the appropriate outputs based on the instructions, and becomes increasingly good at this, it appears from the outside like they understand Chinese. However, Searle claims that the person clearly does not truly understand Chinese; from their perspective they are merely manipulating meaningless symbols based on syntactic rules. Since computer programs work in essentially the same way &mdash; operating at the level of syntax &mdash; they cannot have true understanding either. Searle concludes that the computational approach leaves out key aspects of the mind, such as understanding.</span></p><p class="c4"><span class="c0">Searle has a second claim that, on standard definitions of computation, we do not discover physical systems to be carrying out computations, rather we assign this interpretation to them. That is, he argues that computation is not &ldquo;intrinsic to physics,&rdquo; rather, it is observer-relative. Searle argues that if an observer is required to assign a computational interpretation to a system, we cannot discover that the brain intrinsically carries out computations. The field of cognitive science in its current form, and computational approaches in general, therefore cannot explain how brains or minds work intrinsically.</span></p><p class="c4"><span class="c0">While Searle does not outright deny the possibility of artificial consciousness, he concludes that a theory of consciousness should be focused on the neurobiological rather than computational level. His view therefore falls under the biological approach on our three categories to the question of artificial consciousness. </span></p><h1 class="c6" id="h.tyjcwt"><span>Daniel Dennett, </span><span class="c10">Consciousness Explained</span><span class="c17 c5">, 1993</span></h1><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 33%; float: right; padding: 5px; height: auto;"><img alt="" src="images/is-artificial-consciousness-possible/image4.png" style="width: 33%; float: right; padding: 5px; height: auto; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><p class="c4"><span class="c5">Dennett puts forward the &ldquo;</span><span class="c2"><a class="c1" href="http://www.scholarpedia.org/article/Multiple_drafts_model">Multiple Drafts</a></span><span class="c2">&rdquo;</span><span class="c5">&nbsp;model of consciousness, which proposes an alternative to the arguably commonly held idea that the various aspects of conscious experience come together and are projected in a &ldquo;</span><span class="c2"><a class="c1" href="https://en.wikipedia.org/wiki/Cartesian_theater">Cartesian Theater</a></span><span class="c0">&rdquo; in the brain. Instead, Dennett proposes that information from various sensory inputs arrives in the brain and is processed by various subsystems in the brain at different times. Once processed, the information becomes immediately available for use without the need for further centralized processing, and out of these &ldquo;drafts&rdquo; a final draft that we call consciousness is selected depending on what information is recruited for use (e.g., information to respond to a question). Dennett considers that if his theory is correct, a computer that implements the right program would be conscious. Hence, his view falls under the computational approach on our three categories of approaches to the question of artificial consciousness.</span></p><p class="c4"><span class="c5">Dennett addresses critics who say that they &ldquo;can&rsquo;t imagine a conscious robot!&rdquo; First, he suggests we do imagine it when we think of fictional robots, such as </span><span class="c2"><a class="c1" href="https://en.wikipedia.org/wiki/HAL_9000">HAL</a></span><span class="c5">&nbsp;from </span><span class="c2"><a class="c1" href="https://en.wikipedia.org/wiki/2001:_A_Space_Odyssey_(film)">2001: A Space Odyssey</a></span><span class="c5">. He suggests that we mean we can&rsquo;t imagine </span><span class="c3">how</span><span class="c0">&nbsp;a silicon brain could give rise to consciousness. He counters this assumption by suggesting that this is also true for biological brains, and yet we do it. Dennett also considers Searle&rsquo;s Chinese Room argument against strong AI, suggesting that the thought experiment asks us to imagine an extremely simplified version of the kind of computations that brains do. When we imagine the system with its full complexity, it&rsquo;s no longer obvious that it does not understand Chinese. According to Dennett, complexity matters, otherwise the simple argument that a hand calculator has no understanding would suffice to show that no computer, however advanced, can have understanding. </span></p><h1 class="c6" id="h.3dy6vkm"><span>David Chalmers, </span><span class="c10">The Conscious Mind: In Search of a Fundamental Theory</span><span class="c5 c17">, 1995</span></h1><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 33%; float: right; padding: 5px; height: auto;"><img alt="" src="images/is-artificial-consciousness-possible/image6.png" style="width: 33%; float: right; padding: 5px; height: auto; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><p class="c4"><span class="c5">Chalmers argues in favor of the computational approach to the mind and the possibility of strong AI. His argument has two steps. First, he argues in favor of the &ldquo;principle of organizational invariance,&rdquo; that two systems with the same fine-grained functional organization will have identical conscious experiences.</span><sup class="c13"><a href="#ftnt8" id="ftnt_ref8">[8]</a></sup><span class="c5">&nbsp;He argues for this conclusion using two variations of the </span><span class="c2"><a class="c1" href="http://consc.net/papers/qualia.html">Silicon Chip Replacement</a></span><span class="c5">&nbsp;thought experiment: Fading Qualia, which suggests that an entity whose brain is replaced with functionally equivalent silicon chips will have some kind of experience, and Dancing Qualia, which argues that the silicon-alternative brain will also have the same experience as the original biological brain.</span><sup class="c13"><a href="#ftnt9" id="ftnt_ref9">[9]</a></sup><sup class="c13"><a href="#ftnt10" id="ftnt_ref10">[10]</a></sup><span class="c5">&nbsp;After arguing that maintaining a conscious system&rsquo;s functional organization is sufficient for maintaining its conscious experience, Chalmers provides a technical account of what it means for a physical system to implement a computation, which he argues avoids the observer-relative nature of computation as argued by </span><span class="c2"><a class="c1" href="#h.2et92p0">John Searle</a></span><span class="c5">.</span><sup class="c13"><a href="#ftnt11" id="ftnt_ref11">[11]</a></sup><span class="c0">&nbsp;Chalmers suggests that a system&rsquo;s functional organization is maintained when it is implemented computationally, for example, in a digital computer. If the system in question is a human brain, its computational implementation will therefore have the same mental states as a human brain, including consciousness.</span></p><p class="c4"><span class="c0">He considers several objections, including Searle&rsquo;s Chinese Room argument, to which he responds with variants on the Fading Qualia and Dancing Qualia arguments where a real Chinese speaker has their neurons gradually replaced to what eventually becomes the equivalent of the Chinese Room. As with the initial arguments, Chalmers considers the only plausible outcome to be the one where the entity&rsquo;s conscious experience stays the same. He also addresses the question of whether the computed mind would be just a simulation rather than a replication of a mind with real mental states. He argues that with organizational invariants, such as minds, which are defined by their functional organization, simulations are the same as replications. This is why simulated minds are real minds, but simulated hurricanes are not real hurricanes.</span></p><p class="c4"><span class="c0">Chalmers also responds to the criticism that his arguments only establish a weak form of strong AI, one that is closely tied to biology, because his arguments rely on replicating the neuron-level functional organization of the brain. Chalmers considers that his arguments remove the in-principle argument against computational approaches, and as a result, &ldquo;the floodgates are then opened to a whole range of programs that might be candidates to support conscious experience.&rdquo;</span></p><h1 class="c6" id="h.1t3h5sf"><span>Thomas Metzinger, </span><span class="c10">The Ego Tunnel: The Science of the Mind and the Myth of the Self</span><span class="c17 c5">, 2010</span></h1><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 33%; float: right; padding: 5px; height: auto;"><img alt="" src="images/is-artificial-consciousness-possible/image3.png" style="width: 33%; float: right; padding: 5px; height: auto; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><p class="c4"><span class="c5">According to Metzinger&rsquo;s theory, our experience of consciousness is due to our brain&rsquo;s construction of a model of external reality and a self-model designed to help us interact with the world in a holistic way. These models are &ldquo;transparent&rdquo; in the sense that we cannot see them, and so we take them to be real. According to Metzinger, an AI that has the right kinds of models of external reality and its self would be conscious. Metzinger&rsquo;s approach to the question of artificial consciousness in our categorization is, therefore, computational. However, he notes that engineering such an entity is a difficult technical challenge. He considers the self-model to be crucial &mdash; without it, there may be a constructed world but there would not be anyone to experience it.</span><sup class="c13"><a href="#ftnt12" id="ftnt_ref12">[12]</a></sup><span class="c0">&nbsp;He notes that the implementation of consciousness in artificial entities turns them into entities that can suffer, and they therefore become objects of moral concern.</span></p><p class="c4"><span class="c0">Metzinger thinks that the first sentient AIs that are built will likely have all kinds of deficits due to design errors as engineers refine their processes, and they will likely suffer greatly as a result. For example, early systems will likely have perceptual deficits, making it difficult to perceive themselves or the world around them. In some cases, we may not be able to recognize or understand their suffering; such systems may have the capacity to suffer in ways or degrees completely unimaginable to us. Metzinger argues that because of the probability of their suffering, we should avoid trying to create artificial consciousness, and he suggests that our attention would be better directed at understanding and neutralizing our own suffering.</span></p><p class="c4"><span class="c0">He also asks whether if we could, we should increase the overall amount of positive experience in the universe by colonizing it with artificial &ldquo;bliss machines.&rdquo; He argues that we should not, on the basis that there is more to an existence worth having than positive subjective experiences. He also considers whether we should have a broader pessimism about conscious experience &mdash; that the type of consciousness humans have is net negative in value, and that the evolution of consciousness has led to the expansion of suffering in the universe where before there was none. Metzinger considers that the fact that we do not have clear answers to considerations such as these gives us additional reasons to avoid trying to create artificial consciousness right now.</span></p><h1 class="c6" id="h.4d34og8"><span>Stanislas Dehaene, </span><span class="c10">Consciousness and the Brain: Deciphering How the Brain Codes Our Thoughts</span><span class="c17 c5">, 2014</span></h1><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 33%; float: right; padding: 5px; height: auto;"><img alt="" src="images/is-artificial-consciousness-possible/image12.png" style="width: 33%; float: right; padding: 5px; height: auto; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><p class="c4"><span class="c5">Dehaene outlines the </span><span class="c2"><a class="c1" href="https://en.wikipedia.org/wiki/Global_workspace_theory#Global_neuronal_workspace">Global Neuronal Workspace</a></span><span class="c5">&nbsp;theory of consciousness, which states that we become conscious when information enters a &ldquo;global workspace&rdquo; in the brain where the information is made available for use by various cognitive subsystems such as perception, action, and memory.</span><sup class="c13"><a href="#ftnt13" id="ftnt_ref13">[13]</a></sup><span class="c0">&nbsp;Dehaene sees no logical problem with the possibility of artificial consciousness, favoring a computational approach to the mind. He suggests that we are nowhere near having the capacity to build conscious machines today, but that this is an exciting avenue of scientific research for the coming decades. He thinks there are at least three key functions that are still lacking from current computers: flexible communication between subsystems, the ability to learn and adapt to their environments, and having greater autonomy to decide what actions to take to achieve their goals.</span></p><p class="c4"><span class="c5">He considers objections made by philosophers such as Ned Block and David Chalmers that he only explains access consciousness and not phenomenal consciousness.</span><sup class="c13"><a href="#ftnt14" id="ftnt_ref14">[14]</a></sup><span class="c5">&nbsp;He argues that as science continues to make progress on understanding access consciousness, the more intractable problem of phenomenal consciousness will dissolve, similar to how the notion of a &ldquo;</span><span class="c2"><a class="c1" href="https://en.wikipedia.org/wiki/Vitalism">life force</a></span><span class="c0">&rdquo; dissolved as biologists made progress in understanding the mechanics of life. He also considers the argument that his account leaves out free will. He argues that the variety of free will worth wanting is simply having the freedom and capacity to make decisions based on your higher-order thoughts, beliefs, values, and so on, and that this capacity can be implemented in a computer. He concludes that neither phenomenal consciousness nor free will pose an obstacle for the creation of artificial consciousness.</span></p><h1 class="c6" id="h.2s8eyo1"><span>Michael Tye, </span><span class="c10">Tense Bees and Shell-Shocked Crabs: Are Animals Conscious?</span><span class="c17 c5">, 2017</span></h1><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 33%; float: right; padding: 5px; height: auto;"><img alt="" src="images/is-artificial-consciousness-possible/image1.png" style="width: 33%; float: right; padding: 5px; height: auto; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><p class="c4"><span class="c0">Tye uses &ldquo;Newton&rsquo;s Rule&rdquo; &mdash; for two same outcomes, we are entitled to infer the same cause, unless there is evidence that defeats the inference &mdash; to reason about the likelihood of consciousness in nonhuman animals. In one chapter he applies this process to two artificial entities previously discussed in the philosophy literature: Commander Data from Star Trek, enhanced so that he is a silicon-brained functional isomorph of a human, and &ldquo;Robot Rabbit,&rdquo; a silicon-brained functional isomorph of a rabbit. He argues that in both cases, the functional similarity is a reason to favor that they are conscious, and the physical difference weakens this inference but does not defeat it. He cites Chalmers&rsquo; Silicon Chip Replacement arguments in favor of the view that functional isomorphs of conscious brains are also conscious. Therefore, he considers it is rational to prefer the view that both Commander Data and Robot Rabbit are conscious.</span></p><p class="c4"><span class="c0">Tye further considers a thought experiment where we learn that humans are actually designed by an alien species with four different &ldquo;brain types&rdquo; (analogous to blood types), functionally identical but made of different substrates. On learning this, he asks whether it would be rational to consider that someone whose brain is made of a different substrate to yours is not conscious. He claims it would not be, and therefore that physical difference does not override functional similarity. However, Tye only considers cases of functional isomorphs and does not specify how he would judge entities with functional differences.</span></p><h1 class="c6" id="h.17dp8vu"><span>Susan Blackmore, </span><span class="c10">Consciousness: An Introduction</span><span class="c17 c5">, 2018</span></h1><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 33%; float: right; padding: 5px; height: auto;"><img alt="" src="images/is-artificial-consciousness-possible/image10.png" style="width: 33%; float: right; padding: 5px; height: auto; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><p class="c4"><span class="c5">Blackmore states that a machine trivially has the ability to be conscious because the brain is a machine and it is conscious. So, she refines the question: Can an </span><span class="c3">artificial</span><span class="c5">&nbsp;machine be conscious, and can we </span><span class="c3">make</span><span class="c0">&nbsp;one? She suggests the question matters because if artificial machines are conscious, they could suffer, and so we would have moral responsibilities towards them. </span></p><p class="c4"><span class="c5">She considers several arguments of philosophers and scientists that artificial consciousness is impossible: consciousness is nonphysical and we can&rsquo;t give something nonphysical to a machine; consciousness relies necessarily on biology; there are some things that machines can&rsquo;t do, such as original thinking. She considers Searle&rsquo;s Chinese Room argument and notes various objections, such as the Systems Reply, on which the whole system rather than the individual in the room would have a true understanding of Chinese, and the Robot Reply, which considers that if the system is attached to a body that can </span><span class="c2"><a class="c1" href="https://philpapers.org/rec/HARTSG">ground</a></span><span class="c5">&nbsp;the symbols to objects in the real world, it would have true understanding of Chinese.</span><span class="c5 c13">&nbsp;</span></p><p class="c4"><span class="c0">Blackmore discusses the possibility that artificial entities are already conscious &mdash; even a thermostat could be said to have beliefs (it&rsquo;s too hot or cold in this room); similarly, artificial intelligences could be said to already have beliefs and other mental states. She then considers several approaches to building conscious machines: looking for criteria associated with consciousness and building them into artificial entities, building AI based on existing theories of consciousness, and building the illusion of consciousness into AIs.</span></p><h1 class="c6" id="h.3rdcrjn"><span>Susan Schneider, </span><span class="c10">Artificial You: AI and the Future of Your Mind</span><span class="c17 c5">, 2019</span></h1><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 33%; float: right; padding: 5px; height: auto;"><img alt="" src="images/is-artificial-consciousness-possible/image13.png" style="width: 33%; float: right; padding: 5px; height: auto; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><p class="c4"><span class="c0">Schneider considers two views of consciousness: &ldquo;biological naturalism&rdquo; and the &ldquo;techno-optimist view.&rdquo; She defines biological naturalism as the view that consciousness depends on some specific feature that biological systems have and that non-biological systems lack. Schneider is skeptical of biological naturalism. She notes that no such special feature has been discovered, and even if a such a feature was discovered in biological systems, there could be some other feature that gives rise to consciousness in non-biological systems. She considers John Searle&rsquo;s Chinese Room argument, responding with a version of the Systems Reply and concluding that it does not provide an argument in favor of biological naturalism.</span></p><p class="c4"><span class="c5">Schneider defines techno-optimism as the view that when humans develop highly sophisticated, general-purpose AI, the AI will be conscious. Schneider notes that while this view derives from thought experiments such as those described by </span><span class="c2"><a class="c1" href="#h.3dy6vkm">Chalmers (1995)</a></span><span class="c5">&nbsp;and hence allows for the possibility of artificial consciousness, Chalmer&rsquo;s arguments only apply to systems that are functionally identical to human brains. AI systems aren&rsquo;t and generally won&rsquo;t be functionally identical to brains, so the techno-optimist view is too optimistic. Schneider is skeptical of the view that the mind is software, arguing that minds cannot </span><span class="c3">be</span><span class="c5">&nbsp;software because software is abstract, it can&rsquo;t have any effects in the real world. She considers the view that the mind results when the right kind of software is physically implemented in hardware to be an improvement (e.g., </span><span class="c2"><a class="c1" href="#h.3dy6vkm">Chalmers, 1995</a></span><span class="c5">) but notes that this approach doesn&rsquo;t help resolve deeper problems in philosophy about the nature of the mind (i.e., the </span><span class="c2"><a class="c1" href="https://en.wikipedia.org/wiki/Mind%25E2%2580%2593body_problem">mind-body problem</a></span><span class="c2">)</span><span class="c0">.</span></p><p class="c4"><span class="c5">Schneider advocates for a middle approach that she terms the &ldquo;Wait and See Approach.&rdquo; &nbsp;There are several possible outcomes regarding artificial consciousness that may arise in the future: consciousness may be present in some systems but not others; it may need to be deliberately engineered into systems; as AI systems become more advanced there may be less need for consciousness in them; and the development of consciousness in artificial systems may be slowed down due to public relations considerations in organizations. Schneider suggests ways to test for artificial consciousness: asking AIs a variety of questions referring to their internal experiential states and actually carrying out a form of the </span><span class="c2"><a class="c1" href="#h.3dy6vkm">Silicon Chip Replacement</a></span><span class="c0">&nbsp;thought experiments while asking whether any aspect of their experience changes.</span></p><h1 class="c6" id="h.26in1rg"><span>Michael Graziano, </span><span class="c10">Rethinking Consciousness: A Scientific Theory of Subjective Experience</span><span class="c17 c5">, 2019</span></h1><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 33%; float: right; padding: 5px; height: auto;"><img alt="" src="images/is-artificial-consciousness-possible/image15.png" style="width: 33%; float: right; padding: 5px; height: auto; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><p class="c4"><span class="c5">Graziano outlines the </span><span class="c2"><a class="c1" href="https://en.wikipedia.org/wiki/Attention_schema_theory">Attention Schema Theory</a></span><span class="c2">&nbsp;(AST)</span><span class="c0">&nbsp;of consciousness, according to which our brains construct a simplified internal model of our attention (an attention schema), and we claim to be conscious as a result of the information provided when the attention schema is accessed by cognitive and linguistic systems. Graziano suggests that if you build a machine according to this theory, putting in the correct internal models and giving it cognitive and linguistic access to those models, the machine will believe and claim it has consciousness, and will do so with a high degree of certainty. On Graziano&rsquo;s account, this is what consciousness is, including in humans. Therefore, this is a computational approach. He claims that this would need to be a relatively sophisticated model rather than a simple internal monitor of attention, which can already be programmed into computers. </span></p><p class="c4"><span class="c0">Graziano thinks that we could witness artificial consciousness in the next decade, though this would be limited, for example, to the artificial entities having visual experiences only (e.g., experiencing colors). He thinks machines with human-like consciousness are realistically 50 years away. He thinks the biggest question is not how we will treat artificial entities but how they will treat us; extremely powerful AI systems who can recognize consciousness in us are less likely to harm us. Graziano thinks that implementing consciousness in machines will lead to a better future with AI, and that AST sets out a path for building artificial consciousness.</span></p><p class="c4"><span class="c5">On Graziano&rsquo;s theory, mind uploading is also possible. He thinks that given the complexity of human brains, this technology is perhaps 100 years away or more, but he thinks it will definitely come at some point. He identifies several ethical considerations associated with mind uploading, including the simulations that may be subjected to immense harm as the technology is developed and refined (also a consideration in </span><span class="c2"><a class="c1" href="#h.1t3h5sf">Metzinger, 2010</a></span><span class="c0">), the potential high turnover of artificial minds due to technological progress rendering earlier versions obsolete, and the possibility of the technology being put to harmful political uses. He thinks that mind uploading will be the technology that enables space travel, since uploads don&rsquo;t face various limitations biological humans face, and he thinks that this will result in the exploration and dispersion of life across the galaxy, potentially for millions of years.</span></p><h1 class="c6" id="h.lnxbz9"><span>Philip Goff, </span><span class="c10">Galileo&rsquo;s Error: Foundations for a New Science of Consciousness</span><span class="c17 c5">, 2019</span></h1><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 33%; float: right; padding: 5px; height: auto;"><img alt="" src="images/is-artificial-consciousness-possible/image2.png" style="width: 33%; float: right; padding: 5px; height: auto; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><p class="c4"><span class="c5">Goff outlines his </span><span class="c2"><a class="c1" href="https://plato.stanford.edu/entries/panpsychism/">panpsychist</a></span><span class="c5">&nbsp;view of consciousness on which all physical entities, down to the most fundamental building blocks of the universe, have some basic form of consciousness. He argues that panpsychism resolves problems associated with both </span><span class="c2"><a class="c1" href="https://plato.stanford.edu/entries/dualism/">dualism</a></span><span class="c2">,</span><span class="c5">&nbsp;such as the problem of how the (purportedly) non-physical mind interacts with the physical brain and world in general, and </span><span class="c2"><a class="c1" href="https://plato.stanford.edu/entries/physicalism/">materialism</a></span><span class="c0">, such as that material explanations seem to exclude consciousness itself. </span></p><p class="c4"><span class="c5">Goff briefly addresses the question of artificial consciousness. He considers Searle&rsquo;s Chinese Room argument and argues that is possible for a nonconscious computer program to implement the Chinese Room, which shows that computation and consciousness are separable &mdash; computation does not </span><span class="c3">require </span><span class="c5">consciousness. He emphasizes, however, that if human brains are conscious, there is no reason to think artificial entities cannot also be conscious. He considers the case where a computer is programmed to believe that it is conscious. </span><span class="c2"><a class="c1" href="https://www.keithfrankish.com/illusionism-as-a-theory-of-consciousness/">Illusionists</a></span><span class="c5">&nbsp;tend to argue that this is all there is to consciousness, even in humans (this view is related to </span><span class="c2"><a class="c1" href="#h.26in1rg">Graziano, 2019</a></span><span class="c0">). While he considers the illusionist perspective to be coherent, he argues against illusionism for separate reasons. For example, he argues that any evidence provided by illusionists that consciousness is an illusion appears in our conscious experience, undermining the (supposed) evidence for illusionism. Goff&rsquo;s theory does not clearly fit into the three categories outlined above, though if all physical entities have consciousness that would include artificial entities.</span></p><h1 class="c6" id="h.35nkun2"><span>Simona Ginsburg and Eva Jablonka, </span><span class="c10">The Evolution of the Sensitive Soul: Learning and the Origins of Consciousness</span><span class="c17 c5">, 2019</span></h1><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 33%; float: right; padding: 5px; height: auto;"><img alt="" src="images/is-artificial-consciousness-possible/image14.png" style="width: 33%; float: right; padding: 5px; height: auto; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><p class="c4"><span class="c5">Ginsburg and Jablonka outline an evolutionary approach for understanding which biological entities are conscious. They identify seven criteria that they consider to be agreed upon by neurobiologists as being jointly sufficient for consciousness.</span><sup class="c13"><a href="#ftnt15" id="ftnt_ref15">[15]</a></sup><span class="c5">&nbsp;They then look to identify an &ldquo;evolutionary transition marker,&rdquo; a trait that arose in evolutionary history that requires the presence of those seven criteria and thus indicates a transition from non-conscious to conscious life. The transition marker they propose is &ldquo;unlimited associative learning&rdquo; (UAL). Associative learning is where a subject learns to make an association between a stimulus and another stimulus or response behavior, such as in </span><span class="c2"><a class="c1" href="https://dictionary.apa.org/classical-conditioning">classical</a></span><span class="c5">&nbsp;and </span><span class="c2"><a class="c1" href="https://dictionary.apa.org/operant-conditioning">operant</a></span><span class="c5">&nbsp;conditioning. In UAL, the possible forms of associative learning are open-ended, because they include, for example, compound stimuli, second-order conditioning, and trace conditioning.</span><sup class="c13"><a href="#ftnt16" id="ftnt_ref16">[16]</a></sup><span class="c0">&nbsp;On the basis of this transition maker, they conclude that consciousness arose twice in evolutionary history, first in vertebrates and arthropods during the Cambrian Explosion, and then 250 million years later in mollusks.</span></p><p class="c4"><span class="c5">They do not think that an AI that has the capacity for UAL would necessarily be conscious; they stress that their theory refers to biological entities, and it may be possible to implement UAL in an AI without the seven criteria for consciousness. They emphasize the importance of a body for consciousness, though they note that it may be possible for such a body to be virtual, interacting in a virtual environment. They consider the requirement for a body and complex cognition to be a strong constraint on the development of conscious AI, and they expect that relatively few will be created. They do not think that mass-produced conscious AI will be a reality, and they think that conscious AI would need to go through a learning process in the way that animals do. They note that the field of developmental and evolutionary robotics take an approach that they consider more feasible. Given that they emphasize various aspects of biology, their approach is probably best seen as biological on our categorization of approaches to the possibility of artificial consciousness.</span><sup class="c13"><a href="#ftnt17" id="ftnt_ref17">[17]</a></sup><span class="c0">&nbsp;They worry about the ethical implications of building conscious AI, noting that &ldquo;human record of horrendous and shameless cruelty towards other humans and towards conscious animals does not bode well for future conscious robots.&rdquo;</span></p><h1 class="c6" id="h.1ksv4uv"><span>Peter Godfrey-Smith, </span><span class="c10">Metazoa: Animal Life and the Birth of the Mind</span><span class="c17 c5">, 2020</span></h1><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 33%; float: right; padding: 5px; height: auto;"><img alt="" src="images/is-artificial-consciousness-possible/image11.png" style="width: 33%; float: right; padding: 5px; height: auto; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><p class="c4"><span class="c0">Godfrey-Smith considers the evolution of consciousness in animals, including a short section on artificial consciousness. Godfrey-Smith suggests that if the ideas in the book are right, you can&rsquo;t create a mind through modelling brains on a computer. He argues that minds are tied to particular types of physical and biological substrate. You need to do more than represent or simulate the activity in brains; the interactions between the parts actually need to be physically present. He claims that this would be more difficult with some of the higher-level dynamic patterns of the brain and suggests that it could be very difficult to have a system with anything like the brain&rsquo;s dynamic patterns that isn&rsquo;t otherwise physically very similar to the brain.</span></p><p class="c4"><span class="c0">He suggests that current computers can create the illusion of agency and consciousness well, but that they are currently completely different devices to brains. His biggest disagreement is with the notion of simulated brains, such as in mind uploading &mdash; biological beings are (physically) very different to a simulation of a biological being implemented on a present-day computer. On the other end of spectrum, he considers future robots with &ldquo;genuinely brain-like control systems,&rdquo; suggesting that these could be conscious. Depending on how brain-like such control systems must be, this view could be classified as either a physical or biological approach in our categorization of approaches to artificial consciousness.</span></p><h1 class="c6" id="h.44sinio"><span>Kristof Koch, </span><span class="c10">The Feeling of Life Itself: Why Consciousness Is Widespread but Can&#39;t Be Computed</span><span class="c17 c5">, 2020</span></h1><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 33%; float: right; padding: 5px; height: auto;"><img alt="" src="images/is-artificial-consciousness-possible/image5.png" style="width: 33%; float: right; padding: 5px; height: auto; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><p class="c4"><span class="c5">Koch defends the </span><span class="c2"><a class="c1" href="http://www.scholarpedia.org/article/Integrated_information_theory">Integrated Information Theory</a></span><span class="c2">&nbsp;(IIT)</span><span class="c0">&nbsp;of consciousness, which states that the degree of consciousness in a system depends on its degree of &ldquo;integrated information,&rdquo; which can be understood as the degree to which a system is causally interconnected such that it is not reducible to its individual components. Koch is skeptical of artificial consciousness and computationalism as an approach to studying the mind more generally, as the subtitle of the book indicates.</span></p><p class="c4"><span class="c5">He notes several differences between biological organisms and existing classical computers, such as the vastly more complex design of biological organisms, and differences on dimensions such as signal type, speed, connectivity, and robustness. He claims that today&rsquo;s most successful artificial neural networks are </span><span class="c2"><a class="c1" href="https://en.wikipedia.org/wiki/Feedforward_neural_network">feedforward networks</a></span><span class="c5">, with information flowing in only one direction, but the networks in the cortex involve a great deal of feedback processing. He claims that feedforward networks have no integrated information, and so according to IIT have no consciousness.</span><sup class="c13"><a href="#ftnt18" id="ftnt_ref18">[18]</a></sup><span class="c0">&nbsp;Networks with feedback loops have high integrated information. In addition, he notes that present day digital computers operate with very little integrated information. Even if they were used to simulate systems with a high degree of integrated information, such as a human brain, the computers themselves would have minimal integrated information and so would only be minimally conscious.</span></p><p class="c4"><span class="c0">In principle, however, Koch considers artificial consciousness to be possible. It would just require computers with a very different design to today&rsquo;s computers &mdash; what he calls &ldquo;neuromorphic electronic hardware,&rdquo; where &ldquo;individual logic gates receive inputs from tens of thousands of logic gates&rdquo; and &ldquo;these massive input and output streams would overlap and feed back onto each other.&rdquo; The emphasis that Koch places on the physical organization of the system but not on any specific biological features puts his approach under the physical approach in our categorization of approaches to artificial consciousness.</span></p><h1 class="c6" id="h.2jxsxqh"><span>Anil Seth, </span><span class="c10">Being You: A New Science of Consciousness</span><span class="c17 c5">, 2021</span></h1><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 33%; float: right; padding: 5px; height: auto;"><img alt="" src="images/is-artificial-consciousness-possible/image9.png" style="width: 33%; float: right; padding: 5px; height: auto; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><p class="c4"><span class="c5">Seth details a theory of consciousness grounded in the biological drive of embodied, living beings towards staying alive. Seth considers that artificial consciousness depends on two assumptions: (1) </span><span class="c2"><a class="c1" href="https://iep.utm.edu/functism/">functionalism</a></span><span class="c0">, the view that mental states are defined by the role they play in a cognitive system, which he holds a stance of &ldquo;suspicious agnosticism&rdquo; towards, and (2) that the kind of computation that gives rise to artificial intelligence is the same as that which would give rise to consciousness. He thinks issues associated with these views are glossed over, and artificial consciousness is unlikely to be &ldquo;around the corner.&rdquo; He describes a silicon-based machine that is functionally equivalent to a human. In the theory developed in the book he says he is uncertain about whether the silicon-based machine would be conscious, but he doubts that it would be. He notes, however, that he is relying somewhat on his intuition that consciousness depends on some aspect of biology rather than on computation.</span></p><p class="c4"><span class="c5">While Seth&rsquo;s approach seems closest to a purely biological approach of all the books considered, he still does not completely rule out artificial consciousness and considers the question to be an important one to be concerned about. He makes a distinction between how we ought to treat entities that appear conscious but are not and what we should do if true artificial sentience is created. With the former, there may be unintended consequences such as granting them moral consideration at the expense of conscious entities, such as nonhuman animals. With the latter, he thinks that we would be obligated to minimize their suffering. He also notes that there is a problem that we might not understand what AI conscious states are like. For example, they may experience entirely new forms of suffering for which we have no conception, or some may have no conception or distinction between positive and negative experiential states. He considers whether biotechnology rather than AI is what will bring us closest to artificial consciousness, asking whether &ldquo;</span><span class="c2"><a class="c1" href="https://en.wikipedia.org/wiki/Cerebral_organoid">cerebral organoids</a></span><span class="c0">&rdquo; &mdash; brain like structures made of real neurons used as models to study brains &mdash; could have some basic level of consciousness. He thinks ethical considerations around cerebral organoids could be very important due to the number that are created and the possibility that they become more complex in the future.</span></p><h1 class="c6" id="h.z337ya"><span>David Chalmers, </span><span class="c10">Reality+: Virtual Worlds and the Problems of Philosophy</span><span class="c17 c5">, 2022</span></h1><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 33%; float: right; padding: 5px; height: auto;"><img alt="" src="images/is-artificial-consciousness-possible/image8.png" style="width: 33%; float: right; padding: 5px; height: auto; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><p class="c4"><span class="c5">In this book Chalmers defends the ideas that virtual realities are genuine realities, that we cannot know whether we are in such a reality, and that it is possible to live meaningful lives in virtual realities. A key issue relating to these ideas is whether entirely simulated entities living in virtual words will be conscious. Chalmers discusses the ethical implications of this question: if such entities are conscious, then shutting down a virtual world containing many simulated brains would be an atrocity; on the other hand, if the simulated brains are not conscious, it would seem to be no worse than turning off an ordinary computer game. To understand whether simulated entities would be conscious, Chalmers asks us to first consider the case of a perfect simulation of a human brain running on a digital computer. He then uses the Fading Qualia argument,</span><sup class="c13"><a href="#ftnt19" id="ftnt_ref19">[19]</a></sup><span class="c0">&nbsp;adapted to the case of simulations, to argue that a gradually uploaded simulation of a human brain would be conscious. As with his original Fading Qualia argument, Chalmers argues that since the gradually uploaded simulation of a human brain would be conscious, we can know there is no in-principle reason that simulated entities can&rsquo;t be conscious, and that this &ldquo;opens the floodgates&rdquo; to many different possible types of conscious simulated entities. These claims fall under the computational approach.</span></p><p class="c4 c27"><span class="c0"></span></p><hr class="c24"><div><p class="c8"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c7 c18">&nbsp;</span><span class="c7 c5">While we are mainly concerned about artificial sentience, many books consider the question of artificial consciousness. These terms are used in different ways, but we prefer to use &ldquo;sentience&rdquo; to refer to the capacity for positive and/or negative experiences and &ldquo;consciousness&rdquo; as a broader term for this and other experiences (e.g., visual experience). See our blog post on the </span><span class="c2 c16"><a class="c1" href="https://www.sentienceinstitute.org/blog/artificial-sentience-terminology">terminology of artificial sentience</a></span><span class="c7 c5">&nbsp;for more details about how different terms have been used by various stakeholders.</span></p></div><div><p class="c8"><a href="#ftnt_ref2" id="ftnt2">[2]</a><span class="c7 c18">&nbsp;</span><span class="c7 c5">Note that in some cases we only read the relevant sections of the book rather than the whole book cover-to-cover. In most cases the topic of artificial sentience is not central to the books; the summaries should therefore be read as summaries of the specific points relevant to the topic of artificial sentience.</span></p></div><div><p class="c8"><a href="#ftnt_ref3" id="ftnt3">[3]</a><span class="c7 c5">&nbsp;Not every perspective falls under one of these three categories. For example, </span><span class="c2 c16"><a class="c1" href="#h.2s8eyo1">Tye (2017)</a></span><span class="c7 c5">&nbsp;uses a more general methodology rather than relying on a specific theory or approach that can be categorized in this way. These categories are also not made precise in most works. Another way to think of them is as two spectrums, one from an emphasis on low-level criteria to high-level criteria, and one from an emphasis on the contingencies of biology (particularly low-level biology) to an emphasis on a priori reasoning about consciousness. Each perspective can thus be placed somewhere in this two-dimensional space, as well as on other similar dimensions. Thanks to Jacy Reese Anthis for making this point.</span></p></div><div><p class="c8"><a href="#ftnt_ref4" id="ftnt4">[4]</a><span class="c7 c18">&nbsp;</span><span class="c7 c5">Our categories do not make commitments about the metaphysical nature of consciousness. For example, each of the three approaches are compatible with </span><span class="c2 c16"><a class="c1" href="https://plato.stanford.edu/entries/dualism/">dualism</a></span><span class="c7 c5">.</span></p></div><div><p class="c8"><a href="#ftnt_ref5" id="ftnt5">[5]</a><span class="c7 c18">&nbsp;</span><span class="c5 c21">We are using &ldquo;physical&rdquo; to primarily refer to the level of analysis or description a cognitive system is evaluated at (e.g., hardware versus software) rather than in a contrast of the physical and nonphysical world in a </span><span class="c5 c21 c22"><a class="c1" href="https://plato.stanford.edu/entries/dualism/">dualist</a></span><span class="c5 c21">&nbsp;sense, though these are not always clearly distinguished in this literature.</span></p></div><div><p class="c8"><a href="#ftnt_ref6" id="ftnt6">[6]</a><span class="c7 c18">&nbsp;</span><span class="c7 c5">This is the approach taken in chapter 13 of </span><span class="c2 c16"><a class="c1" href="https://mitpress.mit.edu/books/feeling-life-itself">Koch (2020)</a></span><span class="c7 c5">&nbsp;and sections 5g and 5h of </span><span class="c2 c16"><a class="c1" href="https://royalsocietypublishing.org/doi/10.1098/rstb.2014.0167#d3e1370">Tononi and Koch (2015)</a></span><span class="c7 c5">, though it is ambiguous whether this is a core requirement of IIT or just some applications or interpretations of it.</span></p></div><div><p class="c8"><a href="#ftnt_ref7" id="ftnt7">[7]</a><span class="c7 c5">&nbsp;This category includes approaches that rely on quantum physics, such as those of </span><span class="c2 c16"><a class="c1" href="https://magnusvinding.com/2021/02/15/conversation-with-david-pearce/">David Pearce (2021) </a></span><span class="c7 c5">&nbsp;and </span><span class="c2 c16"><a class="c1" href="https://global.oup.com/ukhe/product/the-emperors-new-mind-9780198784920?cc%3Dgb%26lang%3Den%26">Roger Penrose (1989)</a></span><span class="c7 c5">, since these theories suggest we should look at the physical make-up of systems rather than the higher computational level to understand whether they are conscious. We have not covered any of these views in this post.</span></p></div><div><p class="c8"><a href="#ftnt_ref8" id="ftnt8">[8]</a><span class="c7 c5">&nbsp;&ldquo;Functional organization&rdquo; refers to the position in philosophy of mind known as </span><span class="c2 c16"><a class="c1" href="https://plato.stanford.edu/entries/functionalism/">functionalism</a></span><span class="c7 c5">, in which mental states are defined by the roles they play in cognitive systems rather than their physical make-up. A system&rsquo;s functional organization refers to a description of the causal roles played by each of its components. Chalmers includes the &ldquo;fine-grained&rdquo; qualifier to refer to the level of detail at which the two systems produce the same behavior.</span></p></div><div><p class="c8"><a href="#ftnt_ref9" id="ftnt9">[9]</a><span class="c5 c7">&nbsp;Briefly, the Fading Qualia argument runs as follows: Suppose there is a functional isomorph of a human brain made from silicon, and suppose for argument&rsquo;s sake that the isomorph has no subjective experience. We can then imagine a series of intermediate stages between the human brain and the silicon isomorph, where at each stage a small part of the human brain is replaced with a functionally equivalent silicon alternative. If the entity at the end has no conscious experience, there are two possibilities: 1) either their experience gradually fades away as the parts are replaced, or 2) it suddenly disappears. Chalmers considers both of these outcomes to be logically possible but highly implausible. He therefore concludes that subjective experience must remain as the parts of the brain are replaced, and that the functional isomorph made from silicon would be conscious.</span></p></div><div><p class="c8"><a href="#ftnt_ref10" id="ftnt10">[10]</a><span class="c7 c5">&nbsp;Briefly, the Dancing Qualia argument runs as follows: Suppose the human brain and silicon functional isomorph described above have different conscious experiences. Again, consider the intermediate stages between them. There must be two stages that are sufficiently different that the two entities&rsquo; conscious experiences are different. Chalmers asks us to additionally imagine that at each stage the silicon replacement is also built as a backup circuit in the human brain, and a switch is installed that enables switching between the neural and silicon circuits. This would suggest that at some stage, the flip could be switched back and forth, and the person&rsquo;s conscious experience would &ldquo;dance&rdquo; back and forth, but they would not notice any change, even if they were paying full attention. Chalmers considers this outcome to be highly implausible, and therefore concludes that conscious experience of the human brain and silicon isomorph must be the same.</span></p></div><div><p class="c8"><a href="#ftnt_ref11" id="ftnt11">[11]</a><span class="c7 c18">&nbsp;</span><span class="c7 c5">While Chalmers accepts Searle&rsquo;s argument that every system implements </span><span class="c3 c19 c21 c23">some </span><span class="c7 c5">computation, he argues that his account avoids the result that every physical system implements </span><span class="c3 c19 c23 c21">every </span><span class="c7 c5">computation, and it is only the latter result that is problematic for computational accounts. There is </span><span class="c2 c16"><a class="c1" href="https://plato.stanford.edu/entries/computational-mind/#TriArg">ongoing discussion</a></span><span class="c7 c5">&nbsp;of these issues and their implications.</span></p></div><div><p class="c8"><a href="#ftnt_ref12" id="ftnt12">[12]</a><span class="c7 c5">&nbsp;While he claims that humans are probably unique among animals in that we can think about our self-models, he thinks many animals have self-models and that the evidence for animal consciousness is now &ldquo;far beyond any reasonable doubt.&rdquo;</span></p></div><div><p class="c8"><a href="#ftnt_ref13" id="ftnt13">[13]</a><span class="c7 c5">&nbsp;The global workspace is not a single area of the brain; it consists of a distributed set of cortical neurons.</span></p></div><div><p class="c8"><a href="#ftnt_ref14" id="ftnt14">[14]</a><span class="c7 c5">&nbsp;</span><span class="c2 c16"><a class="c1" href="https://philpapers.org/rec/BLOSCO">Block (2002)</a></span><span class="c7 c5">&nbsp;introduced this distinction. Mental content is access conscious where it is available by the system for use, e.g., in reasoning, speech, or action. Phenomenal consciousness refers to subjective experience. A mental state is phenomenally conscious when &ldquo;it is like&rdquo; something to be in that state.</span><span class="c7 c18">&nbsp;</span></p></div><div><p class="c8"><a href="#ftnt_ref15" id="ftnt15">[15]</a><span class="c7 c5">&nbsp;The criteria are global accessibility and activity; binding and unification; selection, plasticity, learning, and attention; intentionality; temporal thickness; values, emotions, and goals; embodiment, agency, and a notion of &ldquo;self.&rdquo;</span></p></div><div><p class="c8"><a href="#ftnt_ref16" id="ftnt16">[16]</a><span class="c7 c5">&nbsp;Compound stimuli is where the conditioned stimulus in classical conditioning is a compound of features, for example, from different senses. Second-order conditioning is where a conditioned stimulus is associated with another conditioned stimulus, allowing for a long chain between stimuli and actions. Trace conditioning is where there is a time gap between the conditioned and unconditioned stimulus. </span></p></div><div><p class="c8"><a href="#ftnt_ref17" id="ftnt17">[17]</a><span class="c7 c18">&nbsp;</span><span class="c7 c5">However, given that they also say that a conscious artificial entity could exist in a virtual environment, their approach, uniquely among the books, has aspects of both the computational and biological approaches.</span></p></div><div><p class="c8"><a href="#ftnt_ref18" id="ftnt18">[18]</a><span class="c7 c5">&nbsp;Koch suggests that even atoms may have some degree of integrated information and so some degree of consciousness, so presumably what is technically meant is that feedforward networks as a whole do not have consciousness over and above the degree of consciousness in the parts that make them up.</span></p></div><div><p class="c8"><a href="#ftnt_ref19" id="ftnt19">[19]</a><span class="c7 c5">&nbsp;See Footnote 5.</span></p></div></body></html>
</div>




    <hr>
    <div class="container newsletter-container ">
      <p>Subscribe to our newsletter to receive updates on our research and activities. We average one to two emails per year.</p>
      <div id="mc_embed_signup">
        <form action="//sentienceinstitute.us15.list-manage.com/subscribe/post?u=d898f823d035e0601866e68d6&amp;id=cbf2d915a6" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
          <div id="mc_embed_signup_scroll">
            <input type="email" value="" name="EMAIL" class="email form-input" id="mce-EMAIL" placeholder="Email address" required>
            <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
            <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_d898f823d035e0601866e68d6_cbf2d915a6" tabindex="-1" value=""></div>
            <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
          </div>
        </form>
      </div>
    </div>
    
    <footer class="footer">
      <div class="container">
        <div class="row">
          <div class="col-md-2">
            <div><span class="bold">Contact us: </span><a href="mailto:info@sentienceinstitute.org">info@sentienceinstitute.org</a></div>
            <div class="icons">
              <!-- <a href="/rss.xml"><i class="material-icons">rss_feed</i></a> -->
              <a href="https://www.facebook.com/sentienceinstitute"><img class="icon" src="../img/icons/icon_facebook_white.png"/></a>
              <a href="https://www.twitter.com/sentienceinst"><img class="icon" src="../img/icons/icon_twitter_white.png"/></a>
            </div>
          </div>
          <div class="col-md-10 last-column">
            <div>
               2018 Sentience Institute
            </div>
            <div>
              <a href="/terms">Terms and Conditions &amp; Privacy Policy</a>
            </div>
            <div>
              Thank you, <a href="https://weanimals.org/">Jo-Anne McArthur</a>, for granting us the use of so many photos.
            </div>
          </div>
        </div>
      </div>
    </footer>
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/1.19.1/TweenMax.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js" integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/js/bootstrap.min.js" integrity="sha384-vBWWzlZJ8ea9aCX4pEW3rVHjgjt7zpkNpZk+02D9phzyeVkE+jo0ieGizqPLForn" crossorigin="anonymous"></script>
    <script src="/js/ready.js?v=@version@"></script>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-100318911-1', 'auto');
      ga('send', 'pageview');

    </script>
    
  </body>
</html>
