<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

    <meta name="author" content="Sentience Institute" />

    <meta property="og:site_name" content="Sentience Institute" />
    <meta property="fb:app_id" content="302735083502826" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:site" content="@sentienceinst" />

    
    

    
    <meta property="description" content="In a previous post we explained our reasons for favoring the term “artificial sentience.” However, while this term captures what we truly care about — artificial entities with the capacity for positive and/or negative experiences — it may be too vague when we try to use it to make judgments about sentience in specific artificial entities." />
    <meta property="og:description" content="In a previous post we explained our reasons for favoring the term “artificial sentience.” However, while this term captures what we truly care about — artificial entities with the capacity for positive and/or negative experiences — it may be too vague when we try to use it to make judgments about sentience in specific artificial entities." />
    
    
    <title>Sentience Institute | Assessing Sentience in Artificial Entities</title>
    <meta property="title" content="Assessing Sentience in Artificial Entities" />
    <meta property="og:title" content="Assessing Sentience in Artificial Entities" />
    
    
    
    
    <meta property="og:url" content="http://www.sentienceinstitute.org/blog/assessing-sentience-in-artificial-entities" />
    <meta property="og:image" content="http://www.sentienceinstitute.org/img/blog/211221.png" />
    <meta name="twitter:image" content="http://www.sentienceinstitute.org/img/blog/211221.png" />
    


    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico?v=1">
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,600" rel="stylesheet">
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="../css/sentienceinstitute.css?v=2.0.1" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <link href="../css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <nav class="navbar navbar-inverse navbar-toggleable-sm fixed-top">
      <div class="container">
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarText" aria-controls="navbarText" aria-expanded="false">
          <span class="navbar-toggler-icon"></span>
        </button>
        <a class="navbar-brand" href="/">
          <!-- <img class="nav-logo" src="../img/logo/SI_logo_white_200px.png"/> -->
          <img class="nav-logo-brandmark" src="../img/logo/SI_brandmark_white_heavier_web.png"/>
          <div class="nav-logo-text">
            <span>Sentience</span>
            <span class="nav-logo-text-institute">Institute</span>
          </div>
        </a>
        <div id="navbarText" class="collapse navbar-collapse">
          <ul class="navbar-nav">
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">
                Research<span class="caret"></span>
              </a>
              <ul class="dropdown-menu">
                <li><a href="/research-agenda">Agenda</a></li>
                <li><a href="/reports">Reports</a></li>
                <li><a href="/aims-survey">Artificial Intelligence, Morality, and Sentience (AIMS) Survey</a></li>
                <li><a href="/aft-survey">Animals, Food, and Technology (AFT) Survey</a></li>
                <!-- <li><a href="/blog">Blog</a></li> -->
                <!-- <li><a href="/press">Press Releases</a></li> -->
              </ul>
            </li>
            <!-- <li class="nav-item"><a class="nav-link" destination="/media">Media</a></li> -->
            <li class="nav-item"><a class="nav-link" destination="/podcast">Podcast</a></li>
            <li class="nav-item"><a class="nav-link" destination="/blog">Blog</a></li>
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">
                About Us<span class="caret"></span>
              </a>
              <ul class="dropdown-menu">
                <!-- <li><a href="/mission">Our Mission</a></li> -->
                <li><a href="/perspective">Our Perspective</a></li>
                <li><a href="/team">Our Team</a></li>
                <li class="nav-item"><a class="nav-link" href="/get-involved">Get Involved</a></li>
                <li><a href="/transparency">Transparency</a></li>
                <!-- <li><a href="/faq">FAQ</a></li> -->
              </ul>
            </li>
            <li class="nav-item nav-donate"><a class="nav-link" destination="/donate">Donate</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>
    




<div class="container image-container" img-id="211221" style="background-image: url(/img/blog/211221.png);">
  
  
  

<div data-nosnippet class="container image-info-container">
  <div class="image-credit">
    <i class="material-icons photo-icon">photo_camera</i>
    <span>
      TheDigitalArtist / Pixabay
    </span>
  </div>
  <div class="image-title">
    <div class="image-title-text">
      Cyber Network
    </div>
    <div class="arrow-down"></div>
  </div>
</div>


  
</div>

<div class="container first-container gdoc-html-container blog-container assessing-sentience-in-artificial-entities-container">
  <div class="title">
    Assessing Sentience in Artificial Entities
  </div>
  <div class="author-info">
    
    <div class="author">
      <div class="author-img"><img src="../img/team/ali.png"/></div>
      <div class="author-name-and-role">
      <div class="author-name">Ali Ladak</div>
      <div class="author-role">Researcher</div>
    </div>
  </div>
  
  </div>
  <div class="date">
    December 21, 2021
  </div>
  <html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"></head><body class="c24"><p class="c3"><span class="c6 c19 c21">Edited by Janet Pauketat and Jacy Reese Anthis. Many thanks to Tobias Baumann, Michael Johnson, Jason Schukraft, Brian Tomasik, and Daniela Waldhorn for reviewing and providing helpful comments and discussion.</span></p><h1 class="c11" id="h.30j0zll"><span class="c9 c6">Table of Contents</span></h1><p class="c13"><span class="c0"><a class="c4" href="#h.1fob9te">Introduction</a></span></p><p class="c13"><span class="c0"><a class="c4" href="#h.3znysh7">The feature-based approach to assessing sentience</a></span></p><p class="c13"><span class="c0"><a class="c4" href="#h.2et92p0">Evaluating features of artificial entities</a></span></p><p class="c13"><span class="c0"><a class="c4" href="#h.tyjcwt">Relevant features</a></span></p><p class="c13"><span class="c0"><a class="c4" href="#h.3dy6vkm">Applying and weighing features</a></span></p><p class="c13"><span class="c0"><a class="c4" href="#h.1t3h5sf">Acting under uncertainty</a></span></p><h1 class="c11" id="h.1fob9te"><span class="c6 c9">Introduction</span></h1><p class="c3"><span class="c6">In a </span><span class="c2"><a class="c4" href="https://www.sentienceinstitute.org/blog/artificial-sentience-terminology">previous post</a></span><span class="c6">&nbsp;we explained our reasons for favoring the term &ldquo;artificial sentience.&rdquo; However, while this term captures what we truly care about &mdash; artificial entities with the capacity for positive and/or negative experiences &mdash; it may be </span><span class="c2"><a class="c4" href="https://www.sentienceinstitute.org/blog/what-is-sentience">too vague</a></span><span class="c6">&nbsp;when we try to use it to make judgments about sentience in specific artificial entities.</span><sup class="c6 c7"><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup><span class="c6">&nbsp;Since our judgments about which entities to grant moral consideration </span><span class="c2"><a class="c4" href="https://www.sentienceinstitute.org/perspective#moral-circle-expansion">depend on</a></span><span class="c0">&nbsp;whether and the extent to which we consider them to be sentient, this raises a potentially serious problem. </span></p><p class="c3"><span class="c6">One approach to this problem is to say that we do not currently need to know which artificial entities are sentient, only that some </span><span class="c2"><a class="c4" href="https://www.sentienceinstitute.org/blog/the-importance-of-artificial-sentience">future artificial entities</a></span><span class="c6">&nbsp;will be, and they may be excluded from society&rsquo;s moral circle. We should, therefore, encourage the expansion of the moral circle to include artificial entities despite not knowing exactly where the boundary should be drawn. Working out the specifics of which artificial entities are and are not sentient can be deferred to the future. Still, further clarity on this can be useful because of the </span><span class="c2"><a class="c4" href="https://en.wikipedia.org/wiki/Progress_in_artificial_intelligence">growing complexity</a></span><span class="c6">&nbsp;of artificial entities, as well as the </span><span class="c2"><a class="c4" href="https://link.springer.com/article/10.1007/s11948-021-00331-8">increasing attention</a></span><span class="c0">&nbsp;to their treatment as a social and moral issue. In this post, we operationalize the term &ldquo;artificial sentience&rdquo; and outline an initial, tentative framework for assessing sentience in artificial entities.</span></p><h1 class="c11" id="h.3znysh7"><span class="c9 c6">The feature-based approach to assessing sentience</span></h1><p class="c3"><span class="c6">A common approach in the literature on nonhuman animals is to identify features that are indicative of sentience, and then to test the extent to which different entities possess those features. This approach is relatively theory-neutral, relying on argument-by-analogy and inference to the best explanation, rather than starting with a specific theory of sentience and reasoning from that.</span><sup class="c6 c7"><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup><span class="c6">&nbsp;Focusing on features makes sense across a range of philosophical theories of consciousness, such as </span><span class="c2"><a class="c4" href="https://longtermrisk.org/the-eliminativist-approach-to-consciousness/">eliminativism</a></span><span class="c6">, in which the features either constitute consciousness itself or are empirical evidence of other features that constitute consciousness, or </span><span class="c2"><a class="c4" href="https://plato.stanford.edu/entries/dualism/">property dualism</a></span><span class="c6">, in which the features are only empirical evidence of the dualist property of consciousness. While this post does not cover such theories, see </span><span class="c2"><a class="c4" href="https://www.sentienceinstitute.org/blog/what-is-sentience">this blog post</a></span><span class="c0">&nbsp;for more detail on how Sentience Institute thinks about them.</span></p><p class="c3"><span class="c2"><a class="c4" href="https://www.sciencedirect.com/science/article/abs/pii/S0003347205801277">Bateson (1991)</a></span><span class="c6">&nbsp;proposed eight features that, taken together, would suggest that an animal is sentient, including the presence of nociceptors, analogous structures to the human cerebral cortex, and pathways connecting the nociceptors to higher brain structures. </span><span class="c2"><a class="c4" href="https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780199758784.001.0001/acprof-9780199758784">Varner (2012)</a></span><span class="c6">&nbsp;proposed a similar set of eight features. </span><span class="c2"><a class="c4" href="https://www.sciencedirect.com/science/article/abs/pii/S0003347214003431">Sneddon et al. (2014)</a></span><span class="c6">&nbsp;proposed 17 features, split into two categories: (1) whole animal responses to noxious stimuli, such as physiological responses, that differ to those of innocuous stimuli, and (2) motivational and behavioral change, such as willingness to pay a cost to avoid noxious stimuli. They also encourage that the features are considered as a whole and are not taken as indicators in isolation. In the context of phenomenal consciousness, </span><span class="c2"><a class="c4" href="https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood">Muehlhauser (2017)</a></span><span class="c6">&nbsp;proposed 42 features, arguing that given that the scientific/philosophical community does not currently have much confidence in the right theory of consciousness, we should consider a wide range of features that capture a range of plausible theories.</span><sup class="c6 c7"><a href="#ftnt3" id="ftnt_ref3">[3]</a></sup><span class="c6">&nbsp;</span><span class="c2"><a class="c4" href="https://www.sentienceinstitute.org/blog/what-is-sentience">Anthis (2018)</a></span><span class="c6">&nbsp;proposed just three features constituting sentience: reinforcement learning, moods, and integration.</span></p><p class="c3"><span class="c6">Others have applied this third-person feature-based approach to understand whether animals in specific taxa are sentient. The contemporary debate largely concerns invertebrates and fish. </span><span class="c2"><a class="c4" href="https://www.rethinkpriorities.org/blog/2019/6/7/invertebrate-sentience-a-useful-empirical-resource">Rethink Priorities</a></span><span class="c6">&nbsp;carried out in-depth research on the topic of invertebrate sentience, identifying 53 anatomical, physiological, cognitive, and behavioral features associated with sentience and evaluating the extent to which different entities possess those features. Animal Ethics outlined relevant </span><span class="c2"><a class="c4" href="https://www.animal-ethics.org/sentience-section/animal-sentience/invertebrate-sentience-a-review-of-the-neuroscientific-literature/">neuroscientific</a></span><span class="c6">&nbsp;features associated with invertebrate sentience, such as the number of neurons, the presence of specific brain structures such as the neocortex or midbrain (or their functional equivalents), and degree of centralization, as well as third-person </span><span class="c2"><a class="c4" href="https://www.animal-ethics.org/invertebrate-sentience-a-review-of-the-behavioral-evidence/">behavioral</a></span><span class="c0">&nbsp;features ranging across flexibility, emotion, and sociability, that are relevant to understanding sentience in invertebrates.</span></p><p class="c3"><span class="c2"><a class="c4" href="https://psycnet.apa.org/record/2010-00573-000">Braithwaite (2010)</a></span><span class="c6">&nbsp;concluded that fish are sentient on the basis of third-person features such as the presence of nociceptors, production of and responses to pain relieving drugs in response to noxious stimuli, and the activation of certain brain regions which are taken to be associated with conscious rather than reflexive processing of sensory inputs. On the other hand, the third person approach is also used to argue negatively for the sentience of animals; </span><span class="c2"><a class="c4" href="https://www.wellbeingintlstudiesrepository.org/animsent/vol1/iss3/1/">Key (2016)</a></span><span class="c6">, for example, argued that a lack of neocortical brain structures or their analogue in fish imply their lack of sentience.</span><sup class="c6 c7"><a href="#ftnt4" id="ftnt_ref4">[4]</a></sup><span class="c6">&nbsp;This raises the argument that in addition to considering features indicative of sentience, we should also consider &ldquo;defeaters&rdquo; that may invalidate or significantly downgrade our judgments of sentience (</span><span class="c2"><a class="c4" href="https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780190278014.001.0001/acprof-9780190278014">Tye, 2017</a></span><span class="c0">).</span></p><h1 class="c11" id="h.2et92p0"><span class="c9 c6">Evaluating features of artificial entities</span></h1><p class="c3"><span class="c6">Can the feature-based approach be applied to help us understand and make judgments about sentience in artificial entities? A couple of studies have made some attempt to do so: </span><span class="c2"><a class="c4" href="http://www.davidgamez.eu/papers/Gamez05_OrdinalProbabilityScale.pdf">Gamez (2005)</a></span><span class="c6">&nbsp;proposed a list of eight features for assessing consciousness in artificial entities, where the features were chosen so that artificial entities with more human-like capacities are more likely to be considered conscious,</span><sup class="c6 c7"><a href="#ftnt5" id="ftnt_ref5">[5]</a></sup><span class="c6">&nbsp;and </span><span class="c2"><a class="c4" href="https://www.frontiersin.org/articles/10.3389/fnsys.2019.00025/full">Pennartz et al. (2019)</a></span><span class="c0">&nbsp;considered how features indicative of consciousness in nonhuman animals can be applied to artificial entities. Overall, therefore, we think this approach can be usefully applied to the case of artificial entities, but there are several differences from the application to nonhuman animals that need to be considered.</span></p><p class="c3"><span class="c6">First, when it comes to identifying features associated with sentience in nonhuman animals it is typical to focus on biology. For example, the studies described in the previous section propose neuroanatomical features, such as the absence or presence of nociceptive cells or certain brain structures, and physiological features, such as the production of and response to analgesics. Clearly, these features are not appropriate in the case of artificial entities. Instead, we should focus on analogous features at the functional and algorithmic levels. For example, rather than the presence of physical nociceptors, we should consider the ability to detect harmful stimuli. This shift to focusing on functional and algorithmic rather than biological features requires us to assume that these higher levels are appropriate levels of analysis for understanding sentience, and that sentience can be realized in multiple substrates, not just biological ones.</span><sup class="c6 c7"><a href="#ftnt6" id="ftnt_ref6">[6]</a></sup><span class="c6">&nbsp;It may also be important to take into account the physical structure of artificial entities, which is important on some theories of consciousness.</span><sup class="c6 c7"><a href="#ftnt7" id="ftnt_ref7">[7]</a></sup></p><p class="c3"><span class="c6">Another common strategy to assess sentience in nonhuman animals is to refer to its evolved function. For example, it </span><span class="c2"><a class="c4" href="https://royalsocietypublishing.org/doi/full/10.1098/rstb.2019.0290">is argued</a></span><span class="c6">&nbsp;that pain serves the function of encouraging animals to avoid harmful stimuli, which in turn increases their chances of survival and reproduction, and therefore animals likely experience pain. This type of reasoning will generally be less reliable for artificial entities insofar as evolutionary selection is less influential in their development.</span><sup class="c6 c7"><a href="#ftnt8" id="ftnt_ref8">[8]</a></sup><span class="c6">&nbsp;At the extreme, artificial entities could be designed in an entirely ad hoc way, for example, with the capacity to experience pain that serves no function, and without the capacity to avoid that pain. We should also extrapolate from evolved behaviors, such as vocalizing pain, with caution, since they may not be relevant to artificial entities.</span><sup class="c6 c7"><a href="#ftnt9" id="ftnt_ref9">[9]</a></sup><span class="c6">&nbsp;</span><sup class="c6 c7"><a href="#ftnt10" id="ftnt_ref10">[10]</a></sup></p><p class="c3"><span class="c6">A third consideration is that the presence of higher-order capacities in artificial entities, such as </span><span class="c2"><a class="c4" href="https://www.science.org/doi/abs/10.1126/science.167.3914.86">self-recognition</a></span><span class="c6">, are not as strong evidence of sentience as in animals, where they are sometimes considered to be </span><span class="c2"><a class="c4" href="https://reducing-suffering.org/which-computations-do-i-care-about/#Criterion_of_Conscious_Feeling">sufficient conditions</a></span><span class="c6">. This is because artificial entities </span><span class="c2"><a class="c4" href="https://deepmind.com/research/case-studies/alphago-the-story-so-far">can be designed</a></span><span class="c6">&nbsp;to have very strong capabilities on some dimensions but very limited capabilities on others, whereas in animals higher-order capacities tend to be built upon more basic ones. Relatedly, artificial entities can be designed to superficially meet some features but still be unlikely to be sentient. For example, while </span><span class="c2"><a class="c4" href="https://www.nature.com/articles/nrn.2016.22">verbal reports</a></span><span class="c6">&nbsp;are typically considered to be strong evidence of consciousness, engineers could easily design a robot that plays a recording of the phrase &ldquo;</span><span class="c2"><a class="c4" href="https://reducing-suffering.org/consciousness-is-a-process-not-a-moment/#What_is_pain">I&rsquo;m in pain!</a></span><span class="c6">&rdquo; if it is dropped. On its own, we wouldn&rsquo;t want to take this &ldquo;verbal report&rdquo; as evidence that the robot is sentient. As </span><span class="c2"><a class="c4" href="https://reducing-suffering.org/which-computations-do-i-care-about/">Tomasik (2013)</a></span><span class="c0">&nbsp;argues, to be judged as sentient it may be that we require an entity to satisfy a range of criteria in a non-superficial way, and it may also be that the types of algorithms the entity runs to produce behavior are important in addition to the behavior itself.</span></p><h1 class="c11" id="h.tyjcwt"><span class="c9 c6">Relevant features</span></h1><p class="c3"><span class="c6">Taking into account the considerations described in the previous section, we can propose features that may be associated with sentience in artificial entities. Given the current lack of scientific consensus on how sentience works, especially its experiential aspect, we do not think it is possible to have a great deal of confidence in a core set of features. We instead propose, in line with </span><span class="c2"><a class="c4" href="https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood">Muehlhauser (2017)</a></span><span class="c6">&nbsp;and </span><span class="c2"><a class="c4" href="https://www.rethinkpriorities.org/blog/2019/6/7/invertebrate-sentience-a-useful-empirical-resource">Rethink Priorities (2019)</a></span><span class="c0">, that a relatively large number of features consistent with a range of plausible theories are considered.</span></p><p class="c3"><span class="c0">For example, we propose features in the table that cover both theories that emphasize attention and theories that emphasize integration of information. That is not to say we endorse either of these theories, or any other specific theory referred to in the table of features. Nor do we think that we need evidence that an entity needs to satisfy all (or even most) of the criteria in the table to be considered sentient. Rather, we think that an entity that satisfies more of the criteria more strongly warrants the attribution of sentience than an entity that satisfies fewer. In the next section we outline a method for making judgments about sentience based on the features in Table 1.</span></p><p class="c3"><span class="c6">The features we propose capture two general categories. First are features more related to valenced states, such as the detection of harmful stimuli. Second are features related to an entity&rsquo;s general capacity for experience, such as centralized information processing.</span><sup class="c6 c7"><a href="#ftnt11" id="ftnt_ref11">[11]</a></sup><span class="c0">&nbsp;Some features, such as attention directed on the source of harmful stimuli, capture both an entity&rsquo;s capacity for experience and to be in a valenced state, and therefore fall under both categories.</span></p><p class="c3"><span class="c0">Note that the features listed in Table 1 are not exhaustive, precise, or distinct. We have considered several of the most prominent theories relevant to sentience, but there are many theories that we have not considered. We have also focused at this point on negatively valenced states, but we note that positively valenced states may also be relevant. More research is needed to refine and potentially expand on this list.</span></p><p class="c3"><span class="c6 c19">Table 1: </span><span class="c0">Incomplete list of features indicative of sentience in artificial entities</span></p><a id="t.0491587adddd874486e7d8c14003899015d49bab"></a><a id="t.0"></a><table class="c27"><tbody><tr class="c14"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c21 c23">Feature</span></p><p class="c1 c15"><span class="c21 c23"></span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1"><span class="c21 c23">Reason for inclusion</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c21 c23">Category: Valence, Experience, Both</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c21 c23">Example sources referring to feature</span></p></td></tr><tr class="c14"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c0">Reinforcement learning</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1"><span class="c0">Reinforcement learning refers to learning via trial and error with one&rsquo;s environment and reward signals to achieve goals. It is sometimes argued that the reward signal is associated with valenced experience.</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c0">Both</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c2"><a class="c4" href="https://arxiv.org/abs/1410.8233">Tomasik (2014a)</a></span></p></td></tr><tr class="c14"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c0">Focused/selective attention on source of harmful stimuli</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1"><span class="c0">Attention is widely considered to be an important aspect of experiential states. Assuming it is, attention to harmful stimuli implies sentience.</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c0">Both</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c2"><a class="c4" href="https://psycnet.apa.org/record/2017-44151-017">Prinz (2011)</a></span><span class="c6">; </span><span class="c2"><a class="c4" href="https://www.frontiersin.org/articles/10.3389/frobt.2017.00060/full">Graziano (2017);</a></span><span class="c6">&nbsp;</span><span class="c2"><a class="c4" href="https://www.rethinkpriorities.org/blog/2019/6/13/invertebrate-sentience-table">Rethink Priorities (2019)</a></span></p></td></tr><tr class="c14"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c0">Detection of harmful stimuli</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1"><span class="c0">While nociception is reflexive, the capacity to detect harmful stimuli is plausibly a necessary condition for sentience.</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c0">Valence</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c2"><a class="c4" href="https://www.sciencedirect.com/science/article/abs/pii/S0003347205801277">Bateson (1991)</a></span></p></td></tr><tr class="c14"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c0">Aversive behavioral responses to harmful stimuli</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1"><span class="c0">While we may not expect the same behavioral responses as we find in animals, some kind of aversive behavioral responses would be indicative of sentience in artificial entities.</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c0">Valence</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c2"><a class="c4" href="https://longtermrisk.org/flavors-of-computation-are-flavors-of-consciousness/">Tomasik (2014b)</a></span></p></td></tr><tr class="c14"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c0">Aversive memory associations with harmful stimuli</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1"><span class="c0">This is an important aspect of pain in animals, though it may not be necessary for an artificial entity to form memories in response to painful experiences.</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c0">Valence</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c2"><a class="c4" href="https://longtermrisk.org/flavors-of-computation-are-flavors-of-consciousness/">Tomasik (2014b)</a></span></p></td></tr><tr class="c14"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c0">Mood-like states caused by harmful stimuli</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1"><span class="c6">If harmful stimuli affect an entity&rsquo;s overall mood, e.g., by making the entity more </span><span class="c2"><a class="c4" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3158593/">pessimistic</a></span><span class="c0">, it implies that the entity has both valence (since mood is an aspect of this), and that the system is integrated since multiple parts of the system are affected.</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c0">Both</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c2"><a class="c4" href="https://www.sentienceinstitute.org/blog/what-is-sentience">Anthis (2018)</a></span></p></td></tr><tr class="c14"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c0">Goal-directed behavior</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1"><span class="c6">Goal-directed behavior is the capacity to evaluate the consequences of different actions and make choose actions based on the value of those outcomes. </span><span class="c2"><a class="c4" href="https://www.frontiersin.org/articles/10.3389/fnsys.2019.00025/full?utm_campaign%3DMRK_1056122_55_Neuros_20190730_arts_A%26utm_medium%3DEMLF%26utm_source%3DF-AAE">Pennartz et al. (2019)</a></span><span class="c0">&nbsp;tie this process to consciousness in animals, though they argue it could be done without consciousness in artificial entities.</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c0">Both</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c2"><a class="c4" href="https://www.frontiersin.org/articles/10.3389/fnsys.2019.00025/full?utm_campaign%3DMRK_1056122_55_Neuros_20190730_arts_A%26utm_medium%3DEMLF%26utm_source%3DF-AAE">Pennartz et al. (2019)</a></span></p></td></tr><tr class="c14"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c0">Verbal reports of experiential states</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1"><span class="c6">Some artificial entities will have language capacities, and thus will be able to report on their own internal states. Verbal reports are often considered the strongest evidence of experience in humans. In AI, verbal reports will be more reliable where speech systems draw on information from other cognitive systems (that are processing information in the appropriate ways), rather than responding based on statistical models as with </span><span class="c2"><a class="c4" href="https://en.wikipedia.org/wiki/GPT-3">current AI</a></span><span class="c0">.</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c0">Both</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c2"><a class="c4" href="https://www.nature.com/articles/nrn.2016.22">Koch et al. (2016)</a></span><span class="c2">;</span><span class="c6">&nbsp;</span><span class="c2"><a class="c4" href="https://press.princeton.edu/books/hardcover/9780691180144/artificial-you">Schneider (2019)</a></span></p></td></tr><tr class="c14"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c0">Centralized information processing</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1"><span class="c0">Centralized information processing is widely considered to be important as it allows for disparate information from various cognitive subsystems to be integrated into a unified model of the world. This is analogous to a central nervous system in animals.</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c0">Experience</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c2"><a class="c4" href="https://www.rethinkpriorities.org/blog/2019/6/13/invertebrate-sentience-table">Rethink Priorities (2019)</a></span><span class="c2">;</span><span class="c6">&nbsp;</span><span class="c2"><a class="c4" href="https://www.animal-ethics.org/sentience-section/animal-sentience/invertebrate-sentience-a-review-of-the-neuroscientific-literature">Animal Ethics (2021)</a></span></p></td></tr><tr class="c14"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c0">Causal interconnections within cognitive system</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1"><span class="c0">Some theories hold that interconnections within a cognitive system, particularly feedback connections, are what give rise to experiential states.</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c0">Experience</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c2"><a class="c4" href="https://royalsocietypublishing.org/doi/full/10.1098/rstb.2014.0167">Tononi and Koch (2015)</a></span><span class="c6">; </span><span class="c2"><a class="c4" href="https://www.sciencedirect.com/science/article/abs/pii/S1364661306002373">Lamme (2006)</a></span></p></td></tr><tr class="c14"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c0">Global broadcasting of stimuli made available to multiple cognitive systems</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1"><span class="c0">According to Global Workspace Theory, a leading theory of consciousness, we experience stimuli when they are enhanced in a &ldquo;global workspace&rdquo; and made available for use by multiple cognitive systems, such as action selection, planning, and decision making.</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c0">Experience</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c2"><a class="c4" href="https://www.ingentaconnect.com/content/imp/jcs/1997/00000004/00000004/776">Baars (1997)</a></span><span class="c6">; </span><span class="c2"><a class="c4" href="https://www.sciencedirect.com/science/article/abs/pii/S0959438813002298">Dehaene et al. (2014)</a></span></p></td></tr><tr class="c14"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c0">Internal self-model</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1"><span class="c0">On some views a self-model is necessary for there to be a subject who has experiential states. In its simplest form, this may involve an entity being able to distinguish between oneself and one&rsquo;s environment.</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c0">Experience</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c2"><a class="c4" href="https://www.sciencedirect.com/science/article/abs/pii/S2212683X16300421">Reggia, Katz &amp; Huang (2016)</a></span><span class="c6">; </span><span class="c2"><a class="c4" href="http://www.scholarpedia.org/article/Self_models">Metzinger (2007)</a></span><span class="c2">;</span><span class="c25">&nbsp;</span><span class="c2"><a class="c4" href="https://www.williamcollinsbooks.co.uk/products/metazoa-animal-minds-and-the-birth-of-consciousness-peter-godfrey-smith-9780008321239/">Godfrey-Smith (2021)</a></span></p></td></tr><tr class="c14"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c0">Cognitive/behavioral flexibility</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1"><span class="c0">Flexible cognition and behavior typically require deliberate (and therefore plausibly conscious) processing of information, rather than responding to stimuli in a reflexive way.</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c0">Experience</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c2"><a class="c4" href="https://www.animal-ethics.org/invertebrate-sentience-a-review-of-the-behavioral-evidence/">Animal Ethics (2021)</a></span></p></td></tr><tr class="c14"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c0">Total processing power</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1"><span class="c0">Luke Muehlhauser argues, &ldquo;a brain with more total processing power is (all else equal) more likely to be performing a greater variety of computations (some of which might be conscious), and is also more likely to be conscious if consciousness depends on a brain passing some threshold of repeated, recursive, or &ldquo;integrated&rdquo; computations.&rdquo; </span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c0">Experience</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c2"><a class="c4" href="https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood">Muehlhauser (2017)</a></span></p></td></tr><tr class="c14"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c0">Higher-order representation of mental states</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1"><span class="c0">On higher-order theories, higher-order representation of one&rsquo;s mental states is necessary for sentience. This feature may be measured by capacities such as metacognition.</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c0">Experience</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c2"><a class="c4" href="https://plato.stanford.edu/entries/consciousness-higher/">Carruthers (2020)</a></span><span class="c2">; </span><span class="c2"><a class="c4" href="https://www.sciencedirect.com/science/article/abs/pii/S1364661311001057">Lau &amp; Rosenthal (2011)</a></span></p></td></tr><tr class="c14"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c0">Functional similarity with humans</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1"><span class="c2"><a class="c4" href="https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood">Muehlhauser (2017)</a></span><span class="c6">&nbsp;notes that since we are most confident humans are conscious but do not have a strong understanding of how other than that it involves information processing in the brain, it seems reasonable to think that, all else equal, animals whose brains are more neuroanatomically similar to humans are more likely to be conscious. For similar reasons, we may expect functional similarity with humans to be an indicator of sentience in artificial entities. </span><span>&nbsp; &nbsp; &nbsp;</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c0">Both</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c2"><a class="c4" href="https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood">Muehlhauser (2017)</a></span></p></td></tr><tr class="c14"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c0">Physical structure similarity to humans</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1"><span class="c6">On some theories of consciousness, the physical structure of a system is what matters for its mental states, rather than, say, the higher-level functions and algorithms the physical system implements. For example, </span><span class="c2"><a class="c4" href="https://mitpress.mit.edu/books/feeling-life-itself">Koch (2019)</a></span><span class="c6">&nbsp;argues that causal interconnectivity at the hardware level rather than just the higher algorithmic level is required for artificial sentience. Using similar reasoning as the previous feature, we may expect physical structure similarity to humans to be an indicator of sentience in artificial entities.</span><span class="c10">&nbsp; &nbsp; &nbsp;</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c0">Both</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c2"><a class="c4" href="https://psycnet.apa.org/record/1992-98304-000">Searle (1992);</a></span><span class="c6">&nbsp;</span><span class="c2"><a class="c4" href="https://mitpress.mit.edu/books/feeling-life-itself">Koch (2019)</a></span></p></td></tr><tr class="c14"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c0">Embodiment</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1"><span class="c6">Several authors have considered the importance of a body for sentience. For example, </span><span class="c2"><a class="c4" href="https://philpapers.org/rec/DAMTFO">Damasio (1999)</a></span><span class="c6">&nbsp;considers that feelings arise from an interaction between brain and body, and </span><span class="c2"><a class="c4" href="https://eprints.soton.ac.uk/253373/1/harnad92.turing.html">Harnad (1992)</a></span><span class="c6">&nbsp;extends the </span><span class="c2"><a class="c4" href="https://link.springer.com/chapter/10.1007/978-1-4020-6710-5_3">Turing Test</a></span><span class="c0">&nbsp;to include bodily capacities as a way of getting additional certainty that an entity is sentient.</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c0">Both</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c2"><a class="c4" href="https://eprints.soton.ac.uk/253373/1/harnad92.turing.html">Harnad (1992)</a></span><span class="c6">; </span><span class="c2"><a class="c4" href="https://philpapers.org/rec/DAMTFO">Damasio (1999)</a></span></p></td></tr><tr class="c14"><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c0">Susceptibility to cognitive illusions</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1"><span class="c6">Being susceptible to cognitive illusions, such as the </span><span class="c2"><a class="c4" href="https://www.illusionsindex.org/ir/mueller-lyer">M&uuml;ller-Lyer illusion</a></span><span class="c6">, would indicate that things appear a certain way to an entity, implying that they may have an experience. Note that susceptibility to illusions has been found in some </span><span class="c2"><a class="c4" href="https://link.springer.com/article/10.1007/s10071-015-0860-6">nonhuman animals</a></span><span class="c6">&nbsp;and </span><span class="c2"><a class="c4" href="https://journals.plos.org/plosone/article?id%3D10.1371/journal.pone.0056126">existing AI</a></span><span class="c0">.</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c0">Experience</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c1"><span class="c2"><a class="c4" href="https://arxiv.org/abs/1712.04020">Yampolskiy (2017)</a></span><span class="c2">; </span><span class="c2"><a class="c4" href="https://www.frontiersin.org/articles/10.3389/fnsys.2019.00025/full?utm_campaign%3DMRK_1056122_55_Neuros_20190730_arts_A%26utm_medium%3DEMLF%26utm_source%3DF-AAE">Pennartz et al. (2019)</a></span></p></td></tr></tbody></table><p class="c3 c15"><span class="c0"></span></p><h1 class="c11" id="h.3dy6vkm"><span class="c9 c6">Applying and weighing features</span></h1><p class="c3"><span class="c6">To assess sentience based on the features in the table, we propose taking a graded approach. For each additional feature in the table an entity satisfies, and for every increase in the extent to which an entity possesses one of the features, we should increase the degree of sentience we attribute to that entity.</span><sup class="c6 c7"><a href="#ftnt12" id="ftnt_ref12">[12]</a></sup><span class="c0">&nbsp;</span></p><p class="c3"><span class="c6">An important consideration is how much weight to give to the different features in our judgments. As </span><span class="c2"><a class="c4" href="https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780199758784.001.0001/acprof-9780199758784">Varner (2012)</a></span><span class="c6">&nbsp;notes, to do so we need a guiding theory, but there is no consensus on the right theory underlying sentience. Following </span><span class="c2"><a class="c4" href="https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood">Muehlhauser (2017)</a></span><span class="c6">, we therefore propose that decision-makers take a </span><span class="c2"><a class="c4" href="https://en.wikipedia.org/wiki/Bayesian_probability">Bayesian</a></span><span class="c0">&nbsp;approach reflecting their degree of confidence in different theories. For example, if an individual has a lot of confidence in theories that posit higher-order cognitive capacities are required for sentience, then higher-order features in the table should be given more weight. We expect that as the philosophical and empirical literature on this topic continues to grow, the empirical uncertainties entailed in this weighting will be reduced and perhaps the normative disagreements as well, with less room for subjectivity.</span></p><p class="c3"><span class="c6">As noted above, we should also consider defeaters to the conclusion that an artificial entity is sentient. One consideration in this context is the type of algorithms an artificial entity runs to produce its behavior. For example, if an artificial entity&rsquo;s behavior was entirely determined by a </span><span class="c2"><a class="c4" href="https://www.jstor.org/stable/2184371">giant lookup table</a></span><span class="c0">&nbsp;we may cap our judgment of its sentience no matter how complex or otherwise convincing its behavior.</span></p><h1 class="c11" id="h.1t3h5sf"><span class="c9 c6">Acting under uncertainty</span></h1><p class="c3"><span class="c6">A typical approach to allocating resources, particularly for individuals and organizations involved in </span><span class="c2"><a class="c4" href="https://www.sentienceinstitute.org/perspective#effective-altruism">effective altruism</a></span><span class="c6">, is to estimate, either explicitly or implicitly, the </span><span class="c2"><a class="c4" href="https://forum.effectivealtruism.org/tag/expected-value">expected value</a></span><span class="c6">&nbsp;of different actions. This is because the outcomes of our actions, which exist in the future, are inherently uncertain. This uncertainty can be </span><span class="c2"><a class="c4" href="https://docs.google.com/spreadsheets/d/11HsJLpq0Suf3SK_PmzzWpK1tr_BTd364j0l3xVvSCQw/edit#gid%3D1377543212">modelled</a></span><span class="c6">. Similarly, we propose that individuals should use estimates of degrees of sentience as an input when estimating the expected value of interventions affecting artificial entities. This type of approach has been </span><span class="c2"><a class="c4" href="https://www.charityentrepreneurship.com/animal-welfare-reports">applied</a></span><span class="c0">&nbsp;to estimate the expected value of working on interventions to benefit nonhuman animals.</span></p><p class="c3"><span class="c6">Rather than assessing interventions that affect artificial entities today, this method is likely better applied to address the question of interventions that will affect artificial entities in the future, who are more likely to possess features in Table 1. This will involve considering the likelihood that artificial entities with the features described in the table will exist in the future, and the resulting</span><span>&nbsp;</span><span class="c6">degree of sentience of those entities. Note that, even for a skeptic whose overall judgment about the degree of sentience in artificial entities is very small, given the </span><span class="c2"><a class="c4" href="https://longtermrisk.org/risks-of-astronomical-future-suffering/#Sentient_simulations">number</a></span><span class="c6">&nbsp;of artificial entities that may come into existence in the future, and their potential for </span><span class="c2"><a class="c4" href="https://centerforreducingsuffering.org/research/a-typology-of-s-risks">exclusion</a></span><span class="c0">&nbsp;from the moral circle, the expected value of addressing issues related to their wellbeing may still be high.</span></p><p class="c3"><span class="c6">However, </span><span class="c2"><a class="c4" href="https://www.sentienceinstitute.org/blog/prioritization-questions-for-artificial-sentience">multiple uncertainties</a></span><span class="c0">&nbsp;relevant to assessing the expected value of working on artificial sentience remain: What are the different types of entities that will likely exist in the future? What will be their moral statuses and capacities for welfare? How many of these entities will exist? Under what scenarios will they have positive and negative welfare, and how likely are these scenarios to be realized? Are there any interventions we can carry out that will improve their welfare? We intend to explore these questions in future posts.</span></p><hr class="c22"><div><p class="c1"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c6 c17">&nbsp;Arguably, this problem is compounded by the </span><span class="c2 c20"><a class="c4" href="https://plato.stanford.edu/entries/other-minds/">problem of other minds</a></span><span class="c16 c6">, and applies to all entities, not just artificial ones. Under this view, consciousness has a first-person nature &mdash; it is not possible to get inside an artificial entity and see if they have positive or negative experiences; we can only observe them from the third person.</span></p></div><div><p class="c1"><a href="#ftnt_ref2" id="ftnt2">[2]</a><span class="c6 c17">&nbsp;See </span><span class="c2 c20"><a class="c4" href="https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780190278014.001.0001/acprof-9780190278014">Tye (2017)</a></span><span class="c6 c17">&nbsp;and </span><span class="c2 c20"><a class="c4" href="https://rethinkpriorities.org/publications/invertebrate-sentience-useful-empirical-resource">Rethink Priorities (2019)</a></span><span class="c6 c17">. Another approach found in the literature takes as its start point specific theories of sentience and makes inferences about which entities are likely to be sentient in the context of those specific theories. See, for example, </span><span class="c2 c20"><a class="c4" href="https://www.pnas.org/content/113/18/4900?_e_pi_%3D7,PAGE_ID10,5279993613">Barron and Klein (2016)</a></span><span class="c6 c17">&nbsp;and </span><span class="c2 c20"><a class="c4" href="https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198843702.001.0001/oso-9780198843702">Carruthers (2020).</a></span></p></div><div><p class="c1"><a href="#ftnt_ref3" id="ftnt3">[3]</a><span class="c6 c17">&nbsp;Given that sentience can be considered as </span><span class="c2 c20"><a class="c4" href="https://www.sentienceinstitute.org/blog/what-is-sentience">a specific kind of consciousness</a></span><span class="c16 c6">, this research is still highly relevant.</span></p></div><div><p class="c1"><a href="#ftnt_ref4" id="ftnt4">[4]</a><span class="c6 c17">&nbsp;There is </span><span class="c2 c20"><a class="c4" href="https://royalsocietypublishing.org/doi/abs/10.1098/rsbl.2008.0397">some evidence</a></span><span class="c6 c17">&nbsp;that these analogues do exist in some species of fish. Moreover, the existence of neocortex-like structures as a requirement for sentience is itself a contested issue (</span><span class="c2 c20"><a class="c4" href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/consciousness-without-a-cerebral-cortex-a-challenge-for-neuroscience-and-medicine/C9B1B393176EF250D4AFBB3054A04E31">Merkel, 2007</a></span><span class="c16 c6">).</span></p></div><div><p class="c1"><a href="#ftnt_ref5" id="ftnt5">[5]</a><span class="c8">&nbsp;</span><span class="c16 c6">This list includes features such as how close the processing speed and size of the artificial system are to human levels.</span></p></div><div><p class="c1"><a href="#ftnt_ref6" id="ftnt6">[6]</a><span class="c8">&nbsp;</span><span class="c16 c6">The principle of substrate independence and the view that implementing the right types of algorithms would be sufficient for mental states are </span><span class="c2 c26 c20"><a class="c4" href="http://cogprints.org/319/1/computation.html">important</a></span><span class="c16 c6">&nbsp;in the fields of cognitive science and artificial intelligence, though they are not universally accepted.</span></p></div><div><p class="c1"><a href="#ftnt_ref7" id="ftnt7">[7]</a><span class="c8">&nbsp;</span><span class="c16 c6">By &ldquo;physical structure,&rdquo; we mean roughly the lowest level on </span><span class="c2 c20 c26"><a class="c4" href="https://en.wikipedia.org/wiki/David_Marr_(neuroscientist)#Levels_of_analysis">David Marr&rsquo;s levels of analysis</a></span><span class="c16 c6">. As an example of emphasizing the importance of this level, </span><span class="c2 c26 c20"><a class="c4" href="https://mitpress.mit.edu/books/feeling-life-itself">Koch (2019)</a></span><span class="c16 c6">&nbsp;argues that present-day computers cannot be conscious no matter what higher-level algorithms they implement because their physical hardware does not have anywhere near the right degree of causal interconnectivity. He notes, however, that future computers could be designed with the right physical structure for consciousness.</span></p></div><div><p class="c1"><a href="#ftnt_ref8" id="ftnt8">[8]</a><span class="c6 c17">&nbsp;It may be useful, however, in cases where artificial entities are modelled on biological brains, such as with </span><span class="c2 c20"><a class="c4" href="https://link.springer.com/chapter/10.1007/978-3-642-31674-6_19">whole brain emulations</a></span><span class="c6 c17">, or in some cases where </span><span class="c2 c20"><a class="c4" href="https://en.wikipedia.org/wiki/Evolutionary_algorithm">evolutionary algorithms</a></span><span class="c6 c16">&nbsp;are used.</span></p></div><div><p class="c1"><a href="#ftnt_ref9" id="ftnt9">[9]</a><span class="c16 c6">&nbsp;Vocalizing pain may be useful for some animals to alert others of their situation. Even if it is useful for an artificial entity to vocalize their pain in a similar way, they may not be designed with the capacity to do so.</span></p></div><div><p class="c1"><a href="#ftnt_ref10" id="ftnt10">[10]</a><span class="c8">&nbsp;</span><span class="c16 c6">This reasoning also applies when extrapolating from other, non-evolved behaviors in animals to artificial entities.</span></p></div><div><p class="c1"><a href="#ftnt_ref11" id="ftnt11">[11]</a><span class="c8">&nbsp;</span><span class="c16 c6">Many theories of consciousness focus on this general capacity for experience (for example, visual experience) rather than valenced experience per se. These features are also plausibly evidence of sentience.</span></p></div><div><p class="c1"><a href="#ftnt_ref12" id="ftnt12">[12]</a><span class="c16 c6">&nbsp;Alternatively, if one thinks of sentience as an all-or-nothing capacity rather than a graded capacity, one could apply a similar procedure to make judgments about the probability that an artificial entity is sentient. They may still wish to consider the degree of sentience as an additional factor (e.g., a probability distribution over different degrees).</span></p></div></body></html>
</div>




    <hr>
    <div class="container newsletter-container ">
      <p>Subscribe to our newsletter to receive updates on our research and activities. We average one to two emails per year.</p>
      <div id="mc_embed_signup">
        <form action="//sentienceinstitute.us15.list-manage.com/subscribe/post?u=d898f823d035e0601866e68d6&amp;id=cbf2d915a6" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
          <div id="mc_embed_signup_scroll">
            <input type="email" value="" name="EMAIL" class="email form-input" id="mce-EMAIL" placeholder="Email address" required>
            <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
            <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_d898f823d035e0601866e68d6_cbf2d915a6" tabindex="-1" value=""></div>
            <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
          </div>
        </form>
      </div>
    </div>
    
    <footer class="footer">
      <div class="container">
        <div class="row">
          <div class="col-md-2">
            <div><span class="bold">Contact us: </span><a href="mailto:info@sentienceinstitute.org">info@sentienceinstitute.org</a></div>
            <div class="icons">
              <!-- <a href="/rss.xml"><i class="material-icons">rss_feed</i></a> -->
              <a href="https://www.facebook.com/sentienceinstitute"><img class="icon" src="../img/icons/icon_facebook_white.png"/></a>
              <a href="https://www.twitter.com/sentienceinst"><img class="icon" src="../img/icons/icon_twitter_white.png"/></a>
            </div>
          </div>
          <div class="col-md-10 last-column">
            <div>
              © 2017–2025 Sentience Institute
            </div>
            <div>
              <a href="/terms">Terms and Conditions &amp; Privacy Policy</a>
            </div>
            <div>
              Thank you, <a href="https://weanimals.org/">Jo-Anne McArthur</a>, for granting us the use of so many photos.
            </div>
          </div>
        </div>
      </div>
    </footer>
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/1.19.1/TweenMax.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js" integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/js/bootstrap.min.js" integrity="sha384-vBWWzlZJ8ea9aCX4pEW3rVHjgjt7zpkNpZk+02D9phzyeVkE+jo0ieGizqPLForn" crossorigin="anonymous"></script>
    <script src="/js/ready.js?v=@version@"></script>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-100318911-1', 'auto');
      ga('send', 'pageview');

    </script>
    
  </body>
</html>
