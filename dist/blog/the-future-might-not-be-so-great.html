<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

    <meta name="author" content="Sentience Institute" />

    <meta property="og:site_name" content="Sentience Institute" />
    <meta property="fb:app_id" content="302735083502826" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:site" content="@sentienceinst" />

    
    

    
    <meta property="description" content="Many in the effective altruism (EA) community prioritize the reduction of extinction risks such as nuclear war and unsafe artificial intelligence. Is this the best approach to improving the long-term future?" />
    <meta property="og:description" content="Many in the effective altruism (EA) community prioritize the reduction of extinction risks such as nuclear war and unsafe artificial intelligence. Is this the best approach to improving the long-term future?" />
    
    
    <title>Sentience Institute | The Future Might Not Be So Great.</title>
    <meta property="title" content="The Future Might Not Be So Great." />
    <meta property="og:title" content="The Future Might Not Be So Great." />
    
    
    
    
    <meta property="og:url" content="http://www.sentienceinstitute.org/blog/the-future-might-not-be-so-great" />
    <meta property="og:image" content="http://www.sentienceinstitute.org/img/blog/220630.png" />
    


    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico?v=1">
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,600" rel="stylesheet">
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="../css/sentienceinstitute.css?v=2.0.1" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <link href="../css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <nav class="navbar navbar-inverse navbar-toggleable-sm fixed-top">
      <div class="container">
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarText" aria-controls="navbarText" aria-expanded="false">
          <span class="navbar-toggler-icon"></span>
        </button>
        <a class="navbar-brand" href="/">
          <!-- <img class="nav-logo" src="../img/logo/SI_logo_white_200px.png"/> -->
          <img class="nav-logo-brandmark" src="../img/logo/SI_brandmark_white_heavier_web.png"/>
          <div class="nav-logo-text">
            <span>Sentience</span>
            <span class="nav-logo-text-institute">Institute</span>
          </div>
        </a>
        <div id="navbarText" class="collapse navbar-collapse">
          <ul class="navbar-nav">
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">
                Research<span class="caret"></span>
              </a>
              <ul class="dropdown-menu">
                <li><a href="/research-agenda">Agenda</a></li>
                <li><a href="/foundational-questions-summaries">Foundational Questions for Animal Advocacy</a></li>
                <li><a href="/research">Reports</a></li>
                <li><a href="/aims-survey">Artificial Intelligence, Morality, and Sentience (AIMS) Survey</a></li>
                <li><a href="/aft-survey">Animals, Food, and Technology (AFT) Survey</a></li>
                <!-- <li><a href="/press">Press Releases</a></li> -->
              </ul>
            </li>
            <li class="nav-item"><a class="nav-link" destination="/media">Media</a></li>
            <li class="nav-item"><a class="nav-link" destination="/podcast">Podcast</a></li>
            <li class="nav-item"><a class="nav-link" destination="/blog">Blog</a></li>
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">
                About Us<span class="caret"></span>
              </a>
              <ul class="dropdown-menu">
                <!-- <li><a href="/mission">Our Mission</a></li> -->
                <li><a href="/perspective">Our Perspective</a></li>
                <li><a href="/team">Our Team</a></li>
                <li class="nav-item"><a class="nav-link" href="/get-involved">Get Involved</a></li>
                <li><a href="/transparency">Transparency</a></li>
                <!-- <li><a href="/faq">FAQ</a></li> -->
              </ul>
            </li>
            <li class="nav-item nav-donate"><a class="nav-link" destination="/donate">Donate</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>
    




<div class="container image-container" img-id="220630" style="background-image: url(/img/blog/220630.png);">
  
  
  

<div data-nosnippet class="container image-info-container">
  <div class="image-credit">
    <i class="material-icons photo-icon">photo_camera</i>
    <span>
      Chen Liu
    </span>
  </div>
  <div class="image-title">
    <div class="image-title-text">
      Stars in the sky during night time
    </div>
    <div class="arrow-down"></div>
  </div>
</div>


  
</div>

<div class="container first-container gdoc-html-container blog-container the-future-might-not-be-so-great-container">
  <div class="title">
    The Future Might Not Be So Great
  </div>
  <div class="author-info">
    
    <div class="author">
      <div class="author-img"><img src="../img/team/jacy.png"/></div>
      <div class="author-name-and-role">
      <div class="author-name">Jacy Reese Anthis<a class="author-twitter" href="https://twitter.com/jacyanthis"><img src="../img/icons/icon_twitter_blue.png"/></a></div>
      <div class="author-role">Co-Founder &amp; Research Fellow</div>
    </div>
  </div>
  
  </div>
  <div class="date">
    June 30, 2022
  </div>
  <html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"></head><body class="c48"><p class="c11"><span class="c12">Updated terminology on October 2, 2022.</p></span><p class="c11"><span class="c12">Many thanks for feedback and insight from Kelly Anthis, Tobias Baumann, Jan Brauner, Max Carpendale, Sasha Cooper, Sandro Del Rivo, Michael Dello-Iacovo, Michael Dickens, Anthony DiGiovanni, Marius Hobbhahn, Ali Ladak, Simon Knutsson, Greg Lewis, Kelly McNamara, John Mori, Thomas Moynihan, Caleb Ontiveros, Sean Richardson, Zachary Rudolph, Manny Rutinel, Stefan Schubert, Michael St. Jules, Nell Watson, Peter Wildeford, and Miranda Zhang. This essay is in part an early draft of an upcoming book chapter on the topic, and I will add the citation here when it is available.</span></p><p class="c11 c34"><span>Our lives are not our own. From womb to tomb, we are bound to others, past and present. And by each crime and every kindness, we birth our future. &#11835; </span><span class="c0 c12"><a class="c1" href="https://en.wikipedia.org/wiki/Cloud_Atlas_(film)">Cloud Atlas</a></span><span class="c0"><a class="c1" href="https://en.wikipedia.org/wiki/Cloud_Atlas_(film)">&nbsp;(2012)</a></span></p><p class="c11 c40"><span class="c2"></span></p><p class="c23 c44"><span class="c0"><a class="c1" href="#h.cjij05z8jj7o">Summary</a></span></p><p class="c39 c23"><span class="c0"><a class="c1" href="#h.zaoik1donb8j">Arguments on the Expected Value (EV) of Human Expansion</a></span></p><p class="c13"><span class="c0"><a class="c1" href="#h.wdhveypb60h2">Arguments for Positive Expected Value (EV)</a></span></p><p class="c22"><span class="c0"><a class="c1" href="#h.i8jadqryh40t">Historical Progress</a></span></p><p class="c22"><span class="c0"><a class="c1" href="#h.5baytopn0b3n">Value Through Intent</a></span></p><p class="c22"><span class="c0"><a class="c1" href="#h.i5hbkkxggbpk">Value Through Evolution</a></span></p><p class="c22"><span class="c0"><a class="c1" href="#h.zafnwxo3v9d0">Convergence of Patiency and Agency</a></span></p><p class="c22"><span class="c0"><a class="c1" href="#h.hufvw453drm2">Reasoned Cooperation</a></span></p><p class="c22"><span class="c0"><a class="c1" href="#h.d7d29jutple6">Discoverable Moral Reality</a></span></p><p class="c13"><span class="c0"><a class="c1" href="#h.1hnvjgzhbkke">Arguments for Negative Expected Value (EV)</a></span></p><p class="c22"><span class="c0"><a class="c1" href="#h.yqxf545n0wjd">Historical Harms</a></span></p><p class="c22"><span class="c0"><a class="c1" href="#h.a5ugscp8rk8y">Disvalue Through Intent</a></span></p><p class="c22"><span class="c0"><a class="c1" href="#h.cj42tdor20g9">Disvalue Through Evolution</a></span></p><p class="c22"><span class="c0"><a class="c1" href="#h.dkavzisascy6">Divergence of Patiency and Agency</a></span></p><p class="c22"><span class="c0"><a class="c1" href="#h.ob2msiw0w6xk">Threats</a></span></p><p class="c22"><span class="c0"><a class="c1" href="#h.6ts3ofc82t5o">Treadmills</a></span></p><p class="c13"><span class="c0"><a class="c1" href="#h.pc7tyzi8tqv7">Arguments that May Increase or Decrease Expected Value (EV)</a></span></p><p class="c22"><span class="c0"><a class="c1" href="#h.s2vnlwcyasm4">Conceptual Utility Asymmetry</a></span></p><p class="c22"><span class="c0"><a class="c1" href="#h.i1nvvizg6let">Empirical Utility Asymmetry</a></span></p><p class="c22"><span class="c0"><a class="c1" href="#h.6icu26ymksjp">Complexity Asymmetry</a></span></p><p class="c22"><span class="c0"><a class="c1" href="#h.n93t22al0vi3">Procreation Asymmetry</a></span></p><p class="c22"><span class="c0"><a class="c1" href="#h.f5vv1k9eapz5">EV of the Counterfactual</a></span></p><p class="c22"><span class="c0"><a class="c1" href="#h.387yhjtz1zjg">The Nature of Digital Minds, People, and Sentience</a></span></p><p class="c22"><span class="c0"><a class="c1" href="#h.yyrb27xqjro5">Life Despite Suffering</a></span></p><p class="c22"><span class="c0"><a class="c1" href="#h.aup1c4aveygv">The Nature of Value Refinement</a></span></p><p class="c22"><span class="c0"><a class="c1" href="#h.o79f9fmaec16">Scaling of Value and Disvalue</a></span></p><p class="c22"><span class="c0"><a class="c1" href="#h.4ix8ll31miid">EV of Human Expansion after Near-Extinction or Other Events</a></span></p><p class="c22"><span class="c0"><a class="c1" href="#h.dtb0z55gv9bl">The Zero Point of Value</a></span></p><p class="c39 c23"><span class="c0"><a class="c1" href="#h.onh7u8risudz">Related Work</a></span></p><p class="c13"><span class="c0"><a class="c1" href="#h.x0d6xp69ts5b">Terminology</a></span></p><p class="c39 c23"><span class="c0"><a class="c1" href="#h.7ab02ciioy49">What Does the EV Need to be to Prioritize Extinction Risks</a></span></p><p class="c39 c23"><span class="c0"><a class="c1" href="#h.4jjftlhxeg3m">Time-Sensitivity</a></span></p><p class="c39 c23"><span class="c0"><a class="c1" href="#h.3q0wzj3boa7y">Biases</a></span></p><p class="c39 c23"><span class="c0"><a class="c1" href="#h.ceibcg4xgxqs">Brief Thoughts on the Prioritization of Quality Risks</a></span></p><p class="c13"><span class="c0"><a class="c1" href="#h.2a4ch4j91e26">Importance</a></span></p><p class="c13"><span class="c0"><a class="c1" href="#h.vnskdt9lbf5g">Tractability</a></span></p><p class="c13"><span class="c0"><a class="c1" href="#h.tyn83i5szzbh">Neglectedness</a></span></p><p class="c23 c39"><span class="c0"><a class="c1" href="#h.ljn5h87knqaj">Future Research on the EV of Human Expansion</a></span></p><p class="c23 c32"><span class="c0"><a class="c1" href="#h.fk4pxl3k6152">References</a></span></p><h1 class="c31 c23 c15" id="h.cjij05z8jj7o"><span>Summary</span></h1><p class="c11"><span>The prioritization of extinction risk reduction depends on </span><span>an</span><span>&nbsp;assumption that the expected value (EV)</span><sup><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup><span>&nbsp;</span><span>of human</span><span>&nbsp;survival and </span><span>interstellar colonization is highly positive</span><span>. This essay lays out many arguments on the topic</span><span>. T</span><span>his matters because, i</span><span>nsofar as these arguments are compelling, we should shift some </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/longtermism">longtermist</a></span><span>&nbsp;</span><span>resources away from </span><span>extinction risks</span><span>. </span><span>Extinction risks are the most extreme category of </span><span class="c12">population risks</span><span>, which are risks to the number of individuals in the long-term future</span><span>. W</span><span>e could shift resources towards the other </span><span>type</span><span>&nbsp;of long-term risk, </span><span class="c12">quality risks</span><span>, which are risks to the </span><span>moral value</span><span>&nbsp;of individuals in the long-term future, such as whether they experience </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/sentience-1">suffering or happiness</a></span><span>.</span><sup><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup><span>&nbsp;</span><span>Promising approaches to improve the quality of the long-term future include some forms of AI safety, moral circle expansion, </span><span>cooperative game theory, digital minds</span><span>, and global priorities researc</span><span>h. T</span><span>here may be substantial overlap with extinction risk reduction approaches, but in this case and in general, much more research is needed.</span><span>&nbsp;</span><span>I think that the effective altruism (EA) emphasis on existential risk could be replaced by a mindset of </span><span>cautious longtermism:</span><span>&nbsp;</span><span>Rather</span><span>&nbsp;than ensuring humanity expands its reach throughout the universe, we must ensure that</span><span>&nbsp;the </span><span>universe will be better for it.</span></p><p class="c11"><span>I have spoken to many </span><span>longtermist</span><span>&nbsp;EAs about this </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/crucial-consideration">crucial consideration</a></span><span>, and f</span><span>or most of them, that was their first time explicitly considering the EV of human expansion.</span><sup><a href="#ftnt3" id="ftnt_ref3">[3]</a></sup><span>&nbsp;</span><span>My sense is that many more are considering it now,</span><span>&nbsp;and the community is growing more skeptical of </span><span>highly positive EV</span><span>&nbsp;as the correct estimate.</span><span>&nbsp;I&rsquo;m eager to hear more people&rsquo;s thoughts on the all-things-considered estimate of EV, and I discuss the limited work done on this topic to date in the </span><span class="c0"><a class="c1" href="#h.onh7u8risudz">&ldquo;Related Work&rdquo;</a></span><span>&nbsp;section.</span></p><p class="c11"><span>In the following table, I lay out the object-level arguments on the EV of human expansion, and the rest of the essay details meta-considerations (e.g., option value). The table also includes the strongest supporting arguments that increase the evidential weight of their corresponding argument and the strongest counterarguments that reduce the weight. </span><span>The arguments are not mutually exclusive and are merely intended as broad categories that reflect the most common and compelling arguments for at least some people (not necessarily me) on this topic. For example, </span><span class="c0"><a class="c1" href="#h.i8jadqryh40t">Historical Progress</a></span><span>&nbsp;and </span><span class="c0"><a class="c1" href="#h.5baytopn0b3n">Value Through Intent</a></span><span>&nbsp;have been intertwined insofar as </span><span>human</span><span>s intentionally create progress, so users of this table should be mindful that they do not overcount (e.g., double count) the same evidence. I handle this in my own thinking by splitting an overlapping piece of evidence among </span><span>its</span><span>&nbsp;categories in proportion to a rough sense of fit in those categories.</span><sup><a href="#ftnt4" id="ftnt_ref4">[4]</a></sup></p><p class="c11"><span>In the </span><span class="c0"><a class="c1" href="https://docs.google.com/spreadsheets/d/1t-OM6eh_XBMUierl7nhSJ7LcjnRmU3ACyMWk5E_PKXs">associated spreadsheet</a></span><span>, I list my own subjective evidential weight scores where positive numbers indicate evidence for +EV and negative numbers indicate evidence for -EV. I</span><span>t is helpful to think through these arguments with d</span><span>ifferent assignment and aggregation methods, such as linear or logarithmic scaling.</span><span>&nbsp;W</span><span>ith different methodologies to aggregate my own estimates or those of others, the total estimate is highly negative around </span><span>30% of the time, weakly negative 40%, and weakly positive 30%</span><span>. It is almost never highly positive.</span><span>&nbsp;I encourage people to make their own estimates, and all such estimates should be taken with golf balls of salt.</span><sup><a href="#ftnt5" id="ftnt_ref5">[5]</a></sup></p><p class="c11"><span>T</span><span>his is an atypical structure for an argumentative essay&mdash;laying out all the arguments, for and against, instead of laying out arguments for my position and rebutting the objections. </span><span>I think that we should detach argumentation from evaluation.</span><span>&nbsp;I&rsquo;m not aiming for maximum persuasiveness. Indeed, the thrust of my critique is that EAs have failed to consider these arguments in such a systematic way, either neglecting the assumption entirely or selecting only a handful of the multitude of evidence and reason we have available.</span><sup><a href="#ftnt6" id="ftnt_ref6">[6]</a></sup><span>&nbsp;Overall, my current thinking (primarily an average of several aggregations of quantified estimates and </span><span class="c0"><a class="c1" href="https://en.wikipedia.org/wiki/Aumann%2527s_agreement_theorem">Aumann</a></span><span>&nbsp;updating on others&rsquo; views) is that the EV of human expansion is not highly positive. For this and </span><span class="c0"><a class="c1" href="#h.ceibcg4xgxqs">other reasons</a></span><span>, I prioritize improving the quality of the long-term future rather than increasing its expected population.</span></p><h1 class="c31 c23 c15" id="h.zaoik1donb8j"><span>Arguments</span><span>&nbsp;on the Expected Value (EV) of Human Expansion</span></h1><a id="t.67dd777978f8a2df8d59757b9133fba5a0425fea"></a><a id="t.0"></a><table class="c29"><tr class="c10"><td class="c52" colspan="1" rowspan="1"><p class="c17"><span class="c7 c5">Argument Name</span></p></td><td class="c47" colspan="1" rowspan="1"><p class="c17"><span class="c7 c5">Description</span></p></td></tr><tr class="c36"><td class="c41" colspan="2" rowspan="1"><h2 class="c17 c15" id="h.wdhveypb60h2"><span class="c20 c45">Arguments for Positive Expected Value (EV)</span></h2></td></tr><tr class="c25"><td class="c24" colspan="1" rowspan="1"><h3 class="c8 c15" id="h.i8jadqryh40t"><span class="c7 c5">Historical Progress</span></h3></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span>Humanity has achieved great progress in adding value and reducing disvalue, especially since the Enlightenment, such as through declines in violence, oppression, disease, and poverty. In particular, explicit human values seem to have progressed alongside human behavior, which may more robustly extend into the long-term future. Many scholars have written persuasively on this evidence, most famously </span><span class="c5">Pinker (</span><span class="c0 c5"><a class="c1" href="https://stevenpinker.com/publications/better-angels-our-nature">2012</a></span><span class="c5">; </span><span class="c0 c5"><a class="c1" href="https://stevenpinker.com/publications/enlightenment-now-case-reason-science-humanism-and-progress">2018</a></span><span class="c5">)</span><span class="c2">.</span></p><ul class="c18 lst-kix_u95dfb6qapr6-0 start"><li class="c8 c9 li-bullet-0"><span>Support: This trend is particularly important insofar as we don&rsquo;t know the specific reasons for progress and should rely on relevant </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/inside-vs-outside-view">reference classes</a></span><span class="c2">&mdash;of which this seems the most relevant&mdash;more relative to conceptual arguments.</span></li><li class="c8 c9 li-bullet-0"><span>Support: This same sort of progress could continue into the long-term future as &ldquo;the beginning of infinity&rdquo; </span><span class="c5">(</span><span class="c0 c5"><a class="c1" href="https://en.wikipedia.org/wiki/The_Beginning_of_Infinity">Deutsch 2011</a></span><span class="c5">)</span><span>&nbsp;or &ldquo;the unceasing free yield of the Crusonia plant&rdquo; </span><span class="c5">(</span><span class="c0 c5"><a class="c1" href="https://press.stripe.com/stubborn-attachments">Cowen 2018</a></span><span class="c5">)</span><span class="c2">.</span></li><li class="c8 c9 li-bullet-0"><span>Counter:</span><span>&nbsp;</span><span>This progress has mostly been in relation to a narrow range of possible beings who may exist in the long-term future, calling into question its generalizability (e.g., to factory farmed animals and digital minds).</span><span>&nbsp;In general, t</span><span>he past trend of progress is consistent with a future trend of progress, stagnation, or decline, especially for beings and issues unlike those in the past.</span></li><li class="c8 c9 li-bullet-0"><span>Counter</span><span>: While material measures such as GDP have increased, increases in happiness or other subjective measures of wellbeing are much less clear, such as the General Social Survey showing little change in happiness since 1972 aside from a dip during the Covid-19 pandemic </span><span class="c5">(</span><span class="c0 c5"><a class="c1" href="https://gss.norc.org/">Smith et al. 2022</a></span><span class="c5">)</span><span>. The strength of such evidence depends on baselines, timings, and magnitude of changes, or lack thereof. Also see </span><span class="c0"><a class="c1" href="#h.6ts3ofc82t5o">&ldquo;Treadmills&rdquo;</a></span><span>&nbsp;below.</span></li><li class="c8 c9 li-bullet-0"><span class="c2">Counter: We may have traded off short-term harms for increased global catastrophic risks, such as climate change and nuclear weapons.</span></li><li class="c8 c9 li-bullet-0"><span>Counter: Further historical progress may be curtailed by </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/value-lock-in">value lock-in</a></span><span>, such as from advanced AI (e.g., </span><span class="c0"><a class="c1" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id%3D3464724">Crootof 2019</a></span><span class="c2">), value extrapolation processes, longer human lifespans, or high-speed interstellar colonization.</span></li><li class="c8 c9 li-bullet-0"><span>Counter: There have been many non-moral incentives for moral behavior </span><span>(e.g., growing the labor supply through human rights)</span><span>.</span><span class="c2">&nbsp;The relevance of this counterargument depends on whether moral progress is a particularly robust form of progress.</span></li><li class="c8 c9 li-bullet-0"><span>Counter: Many past humans would be quite unhappy with the way human morality changed after their lives, so this may not actually be progress if we weigh their views significantly </span><span class="c0"><a class="c1" href="https://www.sciencedirect.com/science/article/pii/S0016328721000641">(Anthis and Paez 2021)</a></span><span class="c2">.</span></li></ul></td></tr><tr class="c25"><td class="c24" colspan="1" rowspan="1"><h3 class="c8 c15" id="h.5baytopn0b3n"><span class="c19">Value Through Intent</span></h3></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span>As technology increases, it arguably seems that (i) humans exert more of their intent on the universe, and (ii) humans </span><span>tend to want good more than bad</span><span class="c2">.</span></p><ul class="c18 lst-kix_9o9vne3hywlv-0 start"><li class="c8 c9 li-bullet-0"><span>Support: Individuals consistently want value for themself (i.e</span><span>., selfishly</span><span class="c2">), which may be some evidence of wanting more value in general or at least protecting their own interests in the long-term future. </span></li><li class="c8 c9 li-bullet-0"><span>Counter: If </span><span>(ii) is false</span><span>, and huma</span><span>ns may tend to want bad more than good, then (i) is an argument for negative EV. See </span><span class="c0"><a class="c1" href="#h.a5ugscp8rk8y">&ldquo;Disvalue Through Intent.&rdquo;</a></span></li><li class="c8 c9 li-bullet-0"><span>Counter: While human intent may correlate with value, small discrepancies may be extremely large in optimized and strange long-term </span><span>trajectories</span><span class="c2">.</span></li><li class="c8 c9 li-bullet-0"><span>Counter: Human intent may be difficult to implement at scale, such as how corporations and governments develop their own incentive structures that do not always align with human intent and how the alignment of AI with its designers&rsquo; values seems very challenging (e.g., </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/posts/zzFbZyGP6iz8jLe9n/agi-ruin-a-list-of-lethalities">Yudkowsky 2022</a></span><span class="c2">). Many historical atrocities have been committed with purportedly good intentions.</span></li><li class="c8 c9 li-bullet-0"><span>Support/Counter:</span><span>&nbsp;</span><span>Two of the greatest sources of disvalue on Earth today,</span><span>&nbsp;</span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/farmed-animal-welfare">factory farmed animal suffering</a></span><span>&nbsp;and </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/wild-animal-welfare">wild animal suffering</a></span><span>, are arguably unintentional. This is evidence for</span><span>&nbsp;three</span><span>&nbsp;claims: (a) intent is of less importance; (b) intent is of reducing importance </span><span>over time</span><span class="c2">, insofar as factory farming is a new occurrence and wild animal suffering is new in humans&rsquo; ability to alleviate it; and lastly, claim (ii) above, i.e., that humans may tend to want good more than bad. The net weight of these three arguments is unclear.</span></li></ul></td></tr><tr class="c25"><td class="c24" colspan="1" rowspan="1"><h3 class="c8 c15" id="h.i5hbkkxggbpk"><span class="c7 c5">Value Through Evolution</span></h3></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span>Evolution (e.g., selection of genetic material over generations) selects for some forms of value and good moral attitudes, at least for oneself. Altruism and self-sacrifice can be selected for (e.g., in soldier ants), especially insofar as altruists care more about future generations. These forces may apply to the evolution of post-humans, AGIs, or minds created by an unaligned/rogue AGI. </span><span class="c0 c5"><a class="c1" href="https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/">Christiano (2013)</a></span><span>&nbsp;argues that </span><span>longtermist</span><span>&nbsp;values will be selected for over time, though it is unclear how this applies to non-temporal sorts of altruism.</span></p><ul class="c18 lst-kix_4dqdms4smpql-0 start"><li class="c8 c9 li-bullet-0"><span class="c2">Counter: Altruists may act to benefit present people and thus disproportionately neglect the future.</span></li><li class="c8 c9 li-bullet-0"><span>Support/Counter: This matters more insofar as one believes evolutionary forces </span><span>(not necessarily biological, e.g., it could be the evolution of AIs competing for resources) </span><span>will be more prevalent in the long-term future. </span><span class="c0"><a class="c1" href="https://reducing-suffering.org/the-future-of-darwinism/">Tomasik (2013)</a></span><span>&nbsp;argues that this &ldquo;remains unclear.&rdquo;</span></li></ul></td></tr><tr class="c25"><td class="c24" colspan="1" rowspan="1"><h3 class="c8 c15" id="h.zafnwxo3v9d0"><span class="c7 c5">Convergence of Patiency and Agency</span></h3></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span>Moral patients (i.e., beings who can have positive or negative value) may tend to be agents able to protect their own interests (e.g., to exit a situation when it is </span><span>disvaluable</span><span>). In other words, if more patients are agents, that&#39;s reason for optimism because such beings can use their power as agents to protect their moral interests as patients.<br><br>A society with many such beings</span><span>&nbsp;may be the most likely type of long-term society for various reasons, such as if patiency turns out to not be very useful for enacting the will of agents with power</span><span class="c2">: Insofar as digital minds are the most numerous minds in the long-term future, builders may be able to opt to not make minds doing potentially suffering-inducing activities have the capacity to suffer, though this depends on the usefulness of suffering and only applies to certain endeavors.</span></p></td></tr><tr class="c25"><td class="c24" colspan="1" rowspan="1"><h3 class="c8 c15" id="h.hufvw453drm2"><span class="c7 c5">Reasoned Cooperation</span></h3></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span class="c2">Agents tend to selfishly benefit from working together, such as in families, herds, villages, city-states, nations, and international trade. Such cooperation may protect the interests of future beings. For example, we could expect similar cooperation to evolve on alien worlds or in any evolutionary forces behind digital mind development.</span></p><ul class="c18 lst-kix_s9ck0zan5g7j-0 start"><li class="c8 c9 li-bullet-0"><span class="c2">Support: Depending on one&rsquo;s views on decision theory, acausal cooperation may even be possible.</span></li><li class="c8 c9 li-bullet-0"><span class="c2">Support: There are many examples of symbiosis (or even friendship) between different species.</span></li><li class="c8 c9 li-bullet-0"><span class="c2">Counter: Small, weird minds that lack political power are likely to be numerous in the long-term future. These minds may not be able to offer their own resources to create cooperative agreements, especially if they were created and have always been controlled by other agents.</span></li><li class="c8 c9 li-bullet-0"><span class="c2">Counter: This cooperation may be narrow, such as just within members of one&rsquo;s own species.</span></li></ul></td></tr><tr class="c25"><td class="c24" colspan="1" rowspan="1"><h3 class="c8 c15" id="h.d7d29jutple6"><span class="c7 c5">Discoverable Moral Reality</span></h3></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span>If there are </span><span>stance-independent moral facts</span><span class="c2">&nbsp;(e.g., divine moral truth), then future beings may discover and implement them.</span></p><ul class="c18 lst-kix_dp0dkpmouok-0 start"><li class="c8 c9 li-bullet-0"><span>Support: Strong similarities between different human value systems, such as the badness of suffering, may suggest discoverable moral reality or at least moral convergence </span><span class="c0 c5"><a class="c1" href="https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/">(Christiano 2013)</a></span><span class="c2">.</span></li><li class="c8 c9 li-bullet-0"><span>Counter: Such philosophical facts of the matter may be unlikely or entirely implausible </span><span class="c0 c5"><a class="c1" href="https://jacyanthis.com/Consciousness_Semanticism.pdf">(Anthis 2022)</a></span><span class="c2">.</span></li><li class="c8 c9 li-bullet-0"><span>Counter: Even if there are such facts, we may not care about them </span><span class="c0 c5"><a class="c1" href="https://reducing-suffering.org/why-the-modesty-argument-for-moral-realism-fails/">(Tomasik 2014)</a></span><span>. For example, if we discovered some aspect of reality that corresponded very well with our intuitions of stance-independent moral facts and that aspect dictated the creation of suffering, we may still not be compelled to create suffering. However, such facts may be so unlike what we currently know of reality that we should have limited confidence in how we would respond to them.</span></li></ul></td></tr><tr class="c25"><td class="c27" colspan="2" rowspan="1"><h2 class="c17 c15" id="h.1hnvjgzhbkke"><span class="c20 c45">Arguments for Negative Expected Value (EV)</span></h2></td></tr><tr class="c10"><td class="c24" colspan="1" rowspan="1"><h3 class="c8 c15" id="h.yqxf545n0wjd"><span class="c19">Historical </span><span class="c7 c5">Harms</span></h3></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span>Humanity has a very bad track record of harming other humans as well as domestic and wild animals. </span><span>The empirical evidence for disvalue seems clearest to people who have worked on human and animal rights issues because of salient firsthand experience with</span><span>&nbsp;cruel and callous humans</span><span>&nbsp;can be</span><span>, </span><span>particularly the unsettling &ldquo;seriousness of suffering&rdquo; (see the disturbing examples in </span><span class="c0"><a class="c1" href="https://reducing-suffering.org/on-the-seriousness-of-suffering/">Tomasik 2006</a></span><span class="c0"><a class="c1" href="https://reducing-suffering.org/on-the-seriousness-of-suffering/">&nbsp;for an introduction</a></span><span>). This is</span><span>&nbsp;a topic we are very tempted to ignore, downplay, or rationalize (see </span><span class="c0"><a class="c1" href="https://www.wiley.com/en-us/States%2Bof%2BDenial%253A%2BKnowing%2Babout%2BAtrocities%2Band%2BSuffering-p-9780745623924">Cohen 2001</a></span><span>&nbsp;and </span><span class="c0"><a class="c1" href="#h.3q0wzj3boa7y">&ldquo;Biases&rdquo;</a></span><span>&nbsp;below). </span><span>The largest sources of disvalue today are factory farming and wild animal suffering </span><span class="c0 c5"><a class="c1" href="https://forum.effectivealtruism.org/posts/ch5fq73AFn2Q72AMQ/why-animals-matter-for-effective-altruism">(</a></span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/posts/ch5fq73AFn2Q72AMQ/why-animals-matter-for-effective-altruism">Anthis </a></span><span class="c0 c5"><a class="c1" href="https://forum.effectivealtruism.org/posts/ch5fq73AFn2Q72AMQ/why-animals-matter-for-effective-altruism">201</a></span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/posts/ch5fq73AFn2Q72AMQ/why-animals-matter-for-effective-altruism">6b</a></span><span class="c0 c5"><a class="c1" href="https://forum.effectivealtruism.org/posts/ch5fq73AFn2Q72AMQ/why-animals-matter-for-effective-altruism">)</a></span><span>.</span></p><ul class="c18 lst-kix_69vrrr23nh87-0 start"><li class="c8 c9 li-bullet-0"><span>Support: This is particularly important insofar as small, weird minds that lack political power are likely to be numerous in the long-term future, especially if value-optimal structures (e.g., dolorium and hedonium) are built from such minds. Very little concern has been shown for such minds to date. See </span><span class="c0"><a class="c1" href="#h.o79f9fmaec16">&ldquo;Scaling of Value and Disvalue</a></span><span class="c0"><a class="c1" href="#h.o79f9fmaec16">&rdquo;</a></span><span>&nbsp;and </span><span class="c0"><a class="c1" href="#h.387yhjtz1zjg">&ldquo;The Nature of Digital Minds, People, and Sentience&rdquo;</a></span><span>&nbsp;below</span><span class="c2">.</span></li><li class="c8 c9 li-bullet-0"><span>Support/</span><span>Counter</span><span class="c2">: Insofar as we can reliably reason about the long-term future, the historical record itself becomes less important evidence relative to the reasons driving the historical record.</span></li></ul></td></tr><tr class="c10"><td class="c24" colspan="1" rowspan="1"><h3 class="c8 c15" id="h.a5ugscp8rk8y"><span class="c7 c5">Disvalue Through Intent</span></h3></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span>Many human intentions cause</span><span>&nbsp;harm to others, such as desires for power, status, and </span><span>novelty</span><span>. There are many plausible human interstellar endeavors that involve extensive disvalue in ways that may not be avoided with mere technological advancement (as, arguably, factory farming of animals will be avoided), such as &ldquo;recreation (e.g. safaris, war games), a labor force (e.g. colonists to distant parts of the galaxy, construction workers), scientific experiments, threats, (e.g. </span><span>threatening to create and torture beings that a rival cares about</span><span>), revenge, justice, religion, or even pure sadism&rdquo; </span><span class="c0 c5"><a class="c1" href="https://forum.effectivealtruism.org/posts/BY8gXSpGijypbGitT/why-i-prioritize-moral-circle-expansion-over-artificial">(Anthis 2018</a></span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/posts/BY8gXSpGijypbGitT/why-i-prioritize-moral-circle-expansion-over-artificial">b</a></span><span class="c0 c5"><a class="c1" href="https://forum.effectivealtruism.org/posts/BY8gXSpGijypbGitT/why-i-prioritize-moral-circle-expansion-over-artificial">)</a></span><span class="c2">.</span></p><ul class="c18 lst-kix_dcyjb0nlotaa-0 start"><li class="c8 c9 li-bullet-0"><span>Support: Two of the greatest sources of disvalue on Earth today, factory farming and wild animal suffering, </span><span>are </span><span>arguably intentional </span><span>insofar as humans have the ability to end these</span><span>&nbsp;and c</span><span>hoose not to do so</span><span>.</span></li><li class="c8 c9 li-bullet-0"><span>Counter: While human intent may tightly </span><span>correlate</span><span>&nbsp;with and often cause disvalue, small discrepancies may be extremely large in optimized and strange long-term trajectories (e.g., using advanced technologies to avoid sentience by carefully creating </span><span class="c0"><a class="c1" href="https://en.wikipedia.org/wiki/Philosophical_zombie">p-zombies</a></span><span class="c2">, if one believes those are possible).</span></li><li class="c8 c9 li-bullet-0"><span>Counter: </span><span>Human intent may be difficult to implement at scale, such as how corporations and governments develop their own incentive structures</span><span class="c2">. This is in part due to those human desires for power, status, etc.</span></li></ul></td></tr><tr class="c28"><td class="c24" colspan="1" rowspan="1"><h3 class="c8 c15" id="h.cj42tdor20g9"><span class="c7 c5">Disvalue Through Evolution</span></h3></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span class="c2">Evolution tends to produce more suffering than happiness, such as in wild animals.</span></p><ul class="c18 lst-kix_2il0kvc46pme-0 start"><li class="c8 c9 li-bullet-0"><span class="c2">Support: Biological evolution optimizes for the propagation of genes, a goal that often conflicts with individual welfare.</span></li><li class="c8 c9 li-bullet-0"><span>Counter: Rather than being viewed as more suffering, we can view this as a bias of overemphasizing suffering in our evaluations. See </span><span class="c0"><a class="c1" href="#h.3q0wzj3boa7y">&ldquo;Biases&rdquo;</a></span><span class="c2">&nbsp;below. (This Counter can be made against almost all arguments on this topic, but it seems particularly compelling here.)</span></li><li class="c8 c9 li-bullet-0"><span>Support/Counter: </span><span>This matters more insofar as one believes </span><span>evolutionary forces</span><span>&nbsp;(not necessarily biological, e.g., it could be the evolution of AIs competing for resources; there are many similar selection forces aside from Darwinian evolution per se) will be more prevalent in the long-term future. </span><span class="c0"><a class="c1" href="https://reducing-suffering.org/the-future-of-darwinism/">Tomasik (2013)</a></span><span>&nbsp;argues that this &ldquo;remains unclear.&rdquo;</span></li><li class="c8 c9 li-bullet-0"><span>Support/Counter: There are a number of explanations for this that can inform its plausibility in various long-term future scenarios, such as that disvalue and entropy </span><span>tend to be more sudden</span><span>&nbsp;in onset and duration than value and </span><span>negentrop</span><span class="c2">y, that value is more complex and challenging than disvalue, and that disvalue needs to counterbalance motivation. These forces may apply to the evolution of post-humans, AGIs, or minds created by an unaligned/rogue AGI.</span></li></ul></td></tr><tr class="c16"><td class="c24" colspan="1" rowspan="1"><h3 class="c8 c15" id="h.dkavzisascy6"><span class="c19">Divergence of Patiency and Agenc</span><span class="c19">y</span></h3></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span>Moral patients (i.e., beings who can have positive or negative value) may tend to not be agents able to protect their own interests (e.g., to exit a situation when it is </span><span>disvaluable</span><span>). This may be the most likely type of long-term society for various reasons </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/posts/BY8gXSpGijypbGitT/why-i-prioritize-moral-circle-expansion-over-reducing">(Anthis 2018)</a></span><span class="c2">.</span></p><ul class="c18 lst-kix_r72gsmot3afh-0 start"><li class="c8 c9 li-bullet-0"><span>Support: This is particularly important insofar as </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/artificial-sentience">artificial moral patients</a></span><span>&nbsp;are easy to create, particularly insofar as dolorium and hedonium or near-optimal resource expenditures are easily produced.</span></li></ul></td></tr><tr class="c28"><td class="c24" colspan="1" rowspan="1"><h3 class="c8 c15" id="h.ob2msiw0w6xk"><span class="c19">Threats</span></h3></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span>Disvalue can be used as a threat, </span><span>such as threatening to torture many simulated copies of another agent unless they hand over some of their interstellar resources.</span></p><ul class="c18 lst-kix_r72gsmot3afh-0"><li class="c8 c9 li-bullet-0"><span class="c2">Support: This is particularly important insofar as artificial moral patients are easy to create, particularly insofar as dolorium and hedonium or near-optimal resource expenditures are easily produced.</span></li><li class="c8 c9 li-bullet-0"><span class="c2">Support: There are arguably many examples of this even today, such as through terrorism and hefty prison sentences.</span></li><li class="c8 c9 li-bullet-0"><span class="c2">Counter: Threats may be prevented through precommitment, including noninterventionist norms.</span></li><li class="c8 c9 li-bullet-0"><span class="c2">Counter: Threats may only rarely need to be followed with action in order to have their intended effects.</span></li></ul></td></tr><tr class="c10"><td class="c24" colspan="1" rowspan="1"><h3 class="c8 c15" id="h.6ts3ofc82t5o"><span class="c19">Treadmills</span></h3></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span>Even people with </span><span>great material resources</span><span>&nbsp;can be very unhappy, including many of the best-off humans today, such as in the Easterlin Paradox </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/posts/gCDsAj3K5gcZvGgbg/will-faster-economic-growth-make-us-happier-the-relevance-of">(Plant 2022)</a></span><span>.</span></p><ul class="c18 lst-kix_2a5jm9s6srsd-0 start"><li class="c8 c9 li-bullet-0"><span>Counter: This may be in part due to a solvable challenge of making use of those material resources (e.g., tuning our minds to experience greater happiness), so the disconnect between resources and value may diminish over time </span><span>as we get better at it.</span></li><li class="c8 c9 li-bullet-0"><span>Counter: While treadmills may make materially well-off people less happy, their default state may not be below zero. If so, this is merely a </span><span>counterargument</span><span>&nbsp;for positive EV rather than a distinct argument for negative EV.</span></li></ul></td></tr><tr class="c25"><td class="c3" colspan="2" rowspan="1"><h2 class="c17 c15" id="h.pc7tyzi8tqv7"><span class="c5 c7">Arguments that May Increase or Decrease Expected Value (EV)</span></h2></td></tr><tr class="c25"><td class="c24" colspan="1" rowspan="1"><h3 class="c8 c15" id="h.s2vnlwcyasm4"><span class="c19">Conceptual </span><span class="c19">Utility Asymmetry</span></h3></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span>A </span><span>unit</span><span>&nbsp;of </span><span>disvalue</span><span class="c2">&nbsp;(e.g., suffering) may be larger in absolute value from a unit of value (e.g., happiness). This could be an axiological asymmetry between some natural units of disvalue and value, or empirical (see below).</span></p><ul class="c18 lst-kix_dp0dkpmouok-0"><li class="c8 c9 li-bullet-0"><span>Support/Counter: Conceptual utility asymmetry is the subject of such a rich philosophical debate (e.g., </span><span class="c0"><a class="c1" href="https://en.wikipedia.org/wiki/Better_Never_to_Have_Been">Benatar 2006</a></span><span>). </span><span>Personally, I think there is no natural unit of utility, and thus I set one unit of value and one unit of disvalue to be equal, such that there is no asymmetry.</span><span>&nbsp;However, empirical views that disvalue tends to be more common than value in a wide variety of scenarios or that disvalue can be produced with fewer resources (e.g., joules of energy) could be reframed as axiologically asymmetric units of disvalue and value by choosing certain (natural) units. If one takes an axiological view that disvalue always takes priority (i.e., a lexical view), this argument arguably outweighs all others. </span><span>For example, Epicurean philosophy has been stated as, &ldquo;The absence of all pain is rightly called pleasure&rdquo; </span><span class="c0"><a class="c1" href="https://en.wikipedia.org/wiki/Convivio">(Cicero 1931)</a></span><span>, and </span><span class="c0"><a class="c1" href="https://en.wikipedia.org/wiki/The_World_as_Will_and_Representation">Schopenhauer (1818, translation 2008)</a></span><span>&nbsp;states, &ldquo;For only pain and lack can be felt positively, and therefore they proclaim themselves; well-being, on the contrary, is merely negative.&rdquo;</span><span>&nbsp;More recent formulations include </span><span class="c0"><a class="c1" href="https://link.springer.com/chapter/10.1007/978-94-011-5566-3_9">Wolf (1997)</a></span><span>, </span><span class="c0"><a class="c1" href="https://longtermrisk.org/tranquilism/">Gloor (2017)</a></span><span>, and </span><span class="c0"><a class="c1" href="https://magnusvinding.com/2020/05/31/suffering-focused-ethics-defense-and-implications/">Vinding (2020)</a></span><span class="c2">.</span></li></ul></td></tr><tr class="c25"><td class="c24" colspan="1" rowspan="1"><h3 class="c8 c15" id="h.i1nvvizg6let"><span class="c19">Empirical </span><span class="c19">Utility </span><span class="c19">Asymmetry</span></h3></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span>A </span><span>unit</span><span>&nbsp;of </span><span>disvalue</span><span>&nbsp;(e.g., suffering) may be larger in absolute value from a unit of value (e.g., happiness), or vice versa. This could be an axiological asymmetry (see above) or an empirical asymmetry, such as between per-joule units of disvalue and value or between dolorium and hedonium. As described in </span><span class="c0"><a class="c1" href="https://www.sciencedirect.com/science/article/pii/S0016328721000641">Anthis and Paez (2021)</a></span><span class="c2">, when we imagine the largest values and disvalues (e.g., How many days of intense pleasure would you trade for intense pain?), the disvalues tend to seem larger.</span></p><p class="c8 c40"><span class="c2"></span></p><p class="c8"><span>This argument overlaps with most other arguments in this table, so users should be cautious about overcounting the same evidence.</span></p></td></tr><tr class="c25"><td class="c24" colspan="1" rowspan="1"><h3 class="c8 c15" id="h.6icu26ymksjp"><span class="c7 c5">Complexity Asymmetry</span></h3></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span>Disvalue may be simpler and thus easier to produce and more common than value. This is a variation of the Anna Karenina principle that failure tends to come from any of a number of factors, which was posed at least as early as Aristotle&rsquo;s </span><span class="c12">Nichomachean Ethics</span><span>. Value, on the other hand, may be more complex, a view favored by some in AI safety, such as </span><span class="c0"><a class="c1" href="https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes">Yudkowsky (2007)</a></span><span>. T</span><span>he opposite argument may obtain, though I have never heard anyone believe that claim.</span></p></td></tr><tr class="c25"><td class="c24" colspan="1" rowspan="1"><h3 class="c8 c15" id="h.n93t22al0vi3"><span class="c7 c5">Procreation Asymmetry</span></h3></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span>Bringing </span><span>value-positive people</span><span class="c2">&nbsp;into existence may be less valuable than adding value to existing people, but bringing negative-value people into existence may not be as different from adding disvalue to existing people&mdash;or vice versa.</span></p><ul class="c18 lst-kix_n112fp8au3c7-0 start"><li class="c8 c9 li-bullet-0"><span>Support/Counter: This is the subject of such a </span><span class="c0"><a class="c1" href="https://en.wikipedia.org/wiki/Asymmetry_(population_ethics)">rich philosophical debate</a></span><span class="c2">. Personally, I adopt a total view that does not view creating harmed or benefited beings differently from harming or benefiting current beings. If one takes a strong axiological view that making value-positive people does not matter, this argument arguably outweighs all others.</span></li></ul></td></tr><tr class="c25"><td class="c24" colspan="1" rowspan="1"><h3 class="c8 c15" id="h.f5vv1k9eapz5"><span class="c7 c5">EV of the Counterfactual</span></h3></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span class="c2">If humans do not expand, perhaps because we die off or stay on Earth, what will the EV of the universe be? This counterfactual EV can include wild animals on Earth (including those who could evolve a human-like society after many years of a humanless Earth if humans die off), alien civilizations (who may be very different from humans, such as evolving more like insects or solitary predators), value or disvalue in the universe as we know it (e.g., stars being born and dying, fundamental physics in which particles are attracted and repelled by each other, Boltzmann brains), parallel universes (whom we may otherwise affect, for better or worse, through acausal interactions or as-yet-undiscovered causal mechanisms), and simulators (if we live in a simulation).</span></p><p class="c8 c40"><span class="c2"></span></p><p class="c8"><span>Human expansion may lead to increases or decreases in the EV of these groups. Depending on what sort of expansion we&rsquo;re considering, such as if we curtail the +EV or -EV expansion of alien civilizations or if we attack or </span><span class="c0"><a class="c1" href="https://www.hedweb.com/object32.htm">rescue</a></span><span>&nbsp;them, this counterfactual may also include unaligned AI systems that kill humans or prevent our expansion but expand themselves, such as by paperclipping the universe (which may involve many paperclipping drones and von Neumann probes). The EV of aligned versus unaligned AI systems has been discussed in </span><span class="c0 c5"><a class="c1" href="https://longtermrisk.org/artificial-intelligence-and-its-implications-for-future-suffering">Tomasik (2015)</a></span><span>&nbsp;and </span><span class="c0 c5"><a class="c1" href="https://forum.effectivealtruism.org/posts/225Aq4P4jFPoWBrb5/cause-prioritization-for-downside-focused-value-systems">Gloor (2018)</a></span><span>&nbsp;from a total-suffering perspective, and it remains extremely unclear.</span></p></td></tr><tr class="c25"><td class="c24" colspan="1" rowspan="1"><h3 class="c8 c15" id="h.387yhjtz1zjg"><span class="c7 c5">The Nature of Digital Minds, People, and Sentience</span></h3></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span>While there are relatively clear advantages to digital minds over biological minds, such as the ability to self-modify, copy, and travel long distances, it is much less clear what life for digital minds would be like. For example, we do not know how much protection digital sentience will have over their own experiences (e.g., cryptographic security), how useful it will be to have many small minds versus few large minds, and how useful nesting of minds within each other will be. There are also many normative questions regarding the value of these different minds, such as group entities where the subunits are more distinct than subunits of a biological brain (e.g., What if a China brain were implemented in which there were tiny humans inside of each neuron in a normal human brain, passing around neurotransmitters?). Digital minds may also make up value-optimal structures (e.g., dolorium and hedonium). See </span><span class="c0"><a class="c1" href="#h.o79f9fmaec16">&ldquo;Scaling of Value and Disvalue</a></span><span class="c0"><a class="c1" href="#h.o79f9fmaec16">&rdquo;</a></span><span>&nbsp;below</span><span class="c2">.</span></p><p class="c8 c40"><span class="c2"></span></p><p class="c8"><span class="c2">This argument overlaps with most other arguments in this table, so users should be cautious about overcounting the same evidence.</span></p></td></tr><tr class="c25"><td class="c24" colspan="1" rowspan="1"><h3 class="c8 c15" id="h.yyrb27xqjro5"><span class="c19">Life Despite Suffering</span></h3></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span>Even in dire scenarios, many humans report a preference to live or have lived over to die or to have never been born. </span><span>This may suggest underappreciated value in even apparently disvalua</span><span>ble </span><span>lives</span><span>&nbsp;(e.g., aspects not covered in current moral frameworks) or, as with many psychological arguments, it can be viewed as a bias of overemphasizing value in our evaluations (e.g., </span><span class="c0"><a class="c1" href="https://en.wikipedia.org/wiki/Just-world_hypothesis">just world bias</a></span><span>, </span><span class="c0"><a class="c1" href="https://en.wikipedia.org/wiki/Death_anxiety">fear of death</a></span><span>). </span><span>See </span><span class="c0"><a class="c1" href="#h.3q0wzj3boa7y">&ldquo;Biases&rdquo;</a></span><span>&nbsp;below.</span></p></td></tr><tr class="c25"><td class="c24" colspan="1" rowspan="1"><h3 class="c8 c15" id="h.aup1c4aveygv"><span class="c7 c5">The Nature of Value Refinement</span></h3></td><td class="c6" colspan="1" rowspan="1"><p class="c11"><span>Many plausible trajectories of the long-term future involve some sort of value refinement, such as </span><span class="c0"><a class="c1" href="https://www.alignmentforum.org/tag/coherent-extrapolated-volition">coherent extrapolated volition</a></span><span>&nbsp;(</span><span class="c0"><a class="c1" href="https://intelligence.org/files/CEV.pdf">Yudkowsky 2004</a></span><span>)</span><span>, </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/indirect-normativity">indirect normativity</a></span><span>&nbsp;</span><span class="c0"><a class="c1" href="https://en.wikipedia.org/wiki/Superintelligence">(Bostrom 2014)</a></span><span>, and </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/long-reflection">long reflection</a></span><span>&nbsp;</span><span class="c0"><a class="c1" href="https://globalprioritiesinstitute.org/wp-content/uploads/GPI-Research-Agenda-December-2017.pdf">(Greaves and MacAskill 2017)</a></span><span>.</span><span class="c2">&nbsp;The effect of such processes on values depends on a range of questions such as: Whose values are refined? How important are value inputs at the beginning of refinement (i.e., to what extent are they locked in)? And what sort of moral considerations (e.g., thought experiments) has humanity not yet considered but may consider in such processes?</span></p><p class="c11"><span>Insofar</span><span>&nbsp;as one believes that AGI will be instrumental in humanity&rsquo;s future, even an AGI that is aligned in some way may not be good. It depends on what values are aligned with what aspect of the AGI.</span><sup><a href="#ftnt7" id="ftnt_ref7">[7]</a></sup><span class="c2">&nbsp;Especially concerning is that the AGI may only be aligned with human values and interests, only caring about nonhuman beings to the extent humans do, which may not be sufficient for net positive outcomes.</span></p><p class="c8 c23"><span class="c0 c5"><a class="c1" href="https://reducing-suffering.org/the-future-of-darwinism/">Tomasik (2013)</a></span><span>&nbsp;covers many of these arguments, e.g., &ldquo;Very likely our values will be lost to entropy or Darwinian forces beyond our control. However, there&#39;s some chance that we&#39;ll create a singleton in the next few centuries that includes goal-preservation mechanisms allowing our values to be &lsquo;locked in&rsquo; indefinitely. Even absent a singleton, as long as the vastness of space allows for distinct regions to execute on their own values without take-over by other powers, then we don&#39;t even need a singleton; we just need goal-preservation mechanisms</span><span>,&rdquo; as does </span><span class="c0"><a class="c1" href="https://reducing-suffering.org/will-future-civilization-eventually-achieve-goal-preservation">Tomasik (2017)</a></span><span class="c20">.</span></p></td></tr><tr class="c25"><td class="c24" colspan="1" rowspan="1"><h3 class="c8 c15" id="h.o79f9fmaec16"><span class="c7 c5">Scaling of Value and Disvalue</span></h3></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span class="c2">Sources of value and disvalue vary in magnitude, and some sources seem more likely to be value-optimized, such as dolorium as optimal suffering per unit of resource (e.g., joules of energy) or hedonium as optimal happiness. Forces such as human intent, resource accumulation, evolution, and runaway AI seem to be particularly optimizing. This consideration also depends on how values and optimization are viewed, such as what it means to optimize a layperson&rsquo;s intuitionist morality.</span></p><p class="c8 c40"><span class="c2"></span></p><p class="c8"><span>The more one cares about this sort of utilitronium or value-optimized sources (empirically or conceptually), the more such sources matter. One can also have different evaluations of dolorium and hedonium, such as whether they have a ratio of -1:1, -100:1, etc. (see </span><span class="c0"><a class="c1" href="http://reflectivedisequilibrium.blogspot.com/2012/03/are-pain-and-pleasure-equally-energy.html">Shulman 2012</a></span><span>&nbsp;and </span><span class="c0"><a class="c1" href="http://www.simonknutsson.com/reply-to-shulmans-are-pain-and-pleasure-equally-energy-efficient/">Knutsson 2017</a></span><span class="c2">&nbsp;for some discussion). This can also affect trade-offs within closer-to-zero ranges of value and disvalue.</span></p><p class="c8 c40"><span class="c2"></span></p><p class="c8"><span>More broadly, the gradient of possible positive and negative futures could make large differences in the best approach to reducing quality risks, such as jumping from one step of EV to the one above it (e.g., futures where digital </span><span>sentiences</span><span class="c2">&nbsp;are not seen as people to futures where they are). The larger the jump between steps, the more one should prioritize even small chances of moving up a step (e.g., avoiding an existential risk).</span></p><p class="c8 c40"><span class="c2"></span></p><p class="c8"><span class="c2">This argument overlaps with most other arguments in this table, so users should be cautious about overcounting the same evidence.</span></p></td></tr><tr class="c25"><td class="c24" colspan="1" rowspan="1"><h3 class="c8 c15" id="h.4ix8ll31miid"><span class="c7 c5">EV of Human Expansion after Near-Extinction or Other Events</span></h3></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span>While humans may survive and colonize the cosmos along what would seem a similar trajectory to our current one, there may be major events, such as an apocalyptic near-extinction event in which the human population is decimated but recovers. </span><span>It is very unclear how such events would affect the EV of human expansion. For example, post-near-extinction humans may have a newfound sense of global stewardship and camaraderie or they may have a newfound sense of resource scarcity and fear of each other. Similarly, humans after radical technology change such as life extension may have very different values, such as a resistance to changing their values the way new generations of humans do. </span><span class="c2">Near-extinction events may also select for certain demographics and ideologies.</span></p></td></tr><tr class="c25"><td class="c24" colspan="1" rowspan="1"><h3 class="c8 c15" id="h.dtb0z55gv9bl"><span class="c7 c5">The Zero Point of Value</span></h3></td><td class="c6" colspan="1" rowspan="1"><p class="c8"><span class="c2">Each argument for +EV and -EV depends on where one places the zero point of value. Some scenarios, such as an unaligned AI that carpets the universe with sentience that has a very limited amount of value (e.g., muzak and potatoes) and very limited amount of disvalue (e.g., boredom), may teeter on where one places the zero point between +EV and -EV.</span></p></td></tr></table><h1 class="c31 c23 c15" id="h.onh7u8risudz"><span>Related</span><span>&nbsp;Work</span></h1><p class="c11 c34"><span>Some of our successors might live lives and create worlds that, though failing to justify past suffering, would give us all, including some of those who have suffered, reasons to be glad that the Universe exists. &#11835; </span><span class="c0"><a class="c1" href="https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198778608.001.0001/oso-9780198778608">Derek Parfit (2017)</a></span></p><p class="c11"><span>The field of existential risk has intellectual roots as deep as human history in notions of &ldquo;apocalypse&rdquo; such as the end of the Mayan calendar. </span><span class="c0 c5"><a class="c1" href="https://mitpress.mit.edu/books/x-risk">Thomas Moynihan </a></span><span class="c0"><a class="c1" href="https://mitpress.mit.edu/books/x-risk">(</a></span><span class="c0 c5"><a class="c1" href="https://mitpress.mit.edu/books/x-risk">2020)</a></span><span>&nbsp;distinguishes apocalypse as having a sense to it or a justification, such as the actions of a supernatural deity, while &ldquo;extinction&rdquo; entails &ldquo;the ending of sense&rdquo; entirely. This notion of human extinction is traced back only to the Enlightenment beginning in the 1600s, and its most well-known articulation in the 21st century is under the category of existential risks (also known as x-risks), a term </span><span class="c0"><a class="c1" href="https://ora.ox.ac.uk/objects/uuid:827452c3-fcba-41b8-86b0-407293e6617c">coined in </a></span><span class="c0 c5"><a class="c1" href="https://ora.ox.ac.uk/objects/uuid:827452c3-fcba-41b8-86b0-407293e6617c">20</a></span><span class="c0"><a class="c1" href="https://ora.ox.ac.uk/objects/uuid:827452c3-fcba-41b8-86b0-407293e6617c">02 by philosopher Nick </a></span><span class="c0 c5"><a class="c1" href="https://ora.ox.ac.uk/objects/uuid:827452c3-fcba-41b8-86b0-407293e6617c">Bostrom</a></span><span class="c2">&nbsp;for risks &ldquo;where an adverse outcome would either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential.&rdquo;</span></p><p class="c11"><span>The most famous essay on existential risk is </span><span class="c0"><a class="c1" href="https://nickbostrom.com/astronomical/waste">&ldquo;Astronomical Waste&rdquo;</a></span><span class="c0 c5"><a class="c1" href="https://nickbostrom.com/astronomical/waste">&nbsp;(Bost</a></span><span class="c0"><a class="c1" href="https://nickbostrom.com/astronomical/waste">rom </a></span><span class="c0 c5"><a class="c1" href="https://nickbostrom.com/astronomical/waste">2003)</a></span><span>,</span><span>&nbsp;in which Bostrom argues that if humanizes could colonize the Virgo supercluster, the massive concentration of galaxies that includes our own Milky Way and 47,000 of its neighbors, then we could sustain approximately 10</span><span class="c37">38</span><span>&nbsp;human beings, an intuitively inconceivably large number. Bostrom argues that the priority of utilitarians should be to reduce existential risk and ensure we seize this cosmic endowment, though the leap from the importance of the long-term future to existential risk reduction is contentious </span><span class="c5">(e.g., </span><span class="c0 c5"><a class="c1" href="https://www.effectivealtruism.org/articles/a-proposed-adjustment-to-the-astronomical-waste-argument-nick-beckstead">Beckstead 2013b</a></span><span class="c5">)</span><span>. The field of existential risk studies has risen at pace with the growth of effective altruism (EA), with a number of seminal works summarizing and advancing the field </span><span class="c5">(</span><span class="c0 c5"><a class="c1" href="https://onlinelibrary.wiley.com/doi/10.1111/j.1539-6924.2007.00960.x">Matheny 2007</a></span><span class="c5">; </span><span class="c0 c5"><a class="c1" href="https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198570509.001.0001/isbn-9780198570509">Bostrom 2012</a></span><span class="c5">; </span><span class="c0 c5"><a class="c1" href="https://rucore.libraries.rutgers.edu/rutgers-lib/40469/">Beckstead 2013a</a></span><span class="c5">; </span><span class="c0 c5"><a class="c1" href="https://doi.org/10.1111/1758-5899.12002">Bostrom 2013</a></span><span class="c5">; </span><span class="c0 c5"><a class="c1" href="https://global.oup.com/academic/product/superintelligence-9780198739838">2014</a></span><span class="c5">; </span><span class="c0 c5"><a class="c1" href="https://futureoflife.org/book/life-3-0-being-human-in-the-age-of-artificial-intelligence/">Tegmark 2017</a></span><span class="c5">; </span><span class="c0 c5"><a class="c1" href="https://www.penguinrandomhouse.com/books/566677/human-compatible-by-stuart-russell/">Russell 2019</a></span><span class="c5">; </span><span class="c0 c5"><a class="c1" href="https://mitpress.mit.edu/books/x-risk">Moynihan 2020</a></span><span class="c5">; </span><span class="c0 c5"><a class="c1" href="https://www.hachettebooks.com/titles/toby-ord/the-precipice/9780316484893/">Ord 2020</a></span><span class="c5">; </span><span class="c0 c5"><a class="c1" href="https://www.basicbooks.com/titles/william-macaskill/what-we-owe-the-future/9781541618633/">MacAskill </a></span><span class="c0"><a class="c1" href="https://www.basicbooks.com/titles/william-macaskill/what-we-owe-the-future/9781541618633/">forthcoming</a></span><span class="c5">)</span><span class="c2">.</span></p><p class="c11"><span>Among existential risks, EAs have largely focused on population risks (particularly extinction risks); the term &ldquo;x-risk,&rdquo; which canonically refers to existential risk</span><span>, is often interpreted as extinction risk (see </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/posts/skPFH8LxGdKQsTkJy/clarifying-existential-risks-and-existential-catastrophes">Aird 2020a</a></span><span>)</span><span>. A critical assumption underlying this focus has been that the expected value of humanity&rsquo;s survival and interstellar colonization is very high.. This assumption largely goes unstated, but it was briefly acknowledged in </span><span class="c0 c5"><a class="c1" href="https://doi.org/10.7282/T35M649T">Beckstead</a></span><span class="c0 c5"><a class="c1" href="https://doi.org/10.7282/T35M649T">&nbsp;</a></span><span class="c0"><a class="c1" href="https://doi.org/10.7282/T35M649T">(</a></span><span class="c0 c5"><a class="c1" href="https://doi.org/10.7282/T35M649T">2013a)</a></span><span class="c2">:</span></p><p class="c11 c34"><span>Is the expected value of the future negative? Some serious people&mdash;including Parfit (2011, Volume 2, chapter 36), Williams (2006), and Schopenhauer (1942)&mdash;have wondered whether all of the suffering and injustice in the world </span><span>outweigh</span><span class="c2">&nbsp;all of the good that we&#39;ve had. I tend to think that our history has been worth it, that human well-being has increased for centuries, and that the expected value of the future is positive. But this is an open question, and stronger arguments pointing in either direction would be welcome.</span></p><p class="c11"><span class="c0 c5"><a class="c1" href="https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/">Christiano </a></span><span class="c0"><a class="c1" href="https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/">(</a></span><span class="c0 c5"><a class="c1" href="https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/">2013)</a></span><span>&nbsp;asked, &ldquo;Why might the future be good?&rdquo; though, as I understood it, that essay did not mention the possibility of a negative future. </span><span>I had also implicitly accepted the assumption of a good future until 2014, when I thought through the evidence and decided to prioritize moral circle expansion at the intersection of animal advocacy and longtermism </span><span class="c5">(</span><span class="c0 c5"><a class="c1" href="https://web.archive.org/web/20151106103159/http://thebestwecan.org/2014/07/20/how-do-we-reliably-impact-the-far-future/">Anthis 2014</a></span><span class="c5">)</span><span>. I brought it up on the old EA Forum in </span><span class="c0 c5"><a class="c1" href="https://forum.effectivealtruism.org/posts/NExT987oY5GbYkTiE/some-considerations-for-different-ways-to-reduce-x-risk">Anthis (2016a)</a></span><span>, and </span><span class="c0 c5"><a class="c1" href="https://forum.effectivealtruism.org/posts/kNKpyf4WWdKehgvRt/an-argument-for-why-the-future-may-be-good">West (2017)</a></span><span>&nbsp;detailed a version of the </span><span class="c0"><a class="c1" href="#h.5baytopn0b3n">&ldquo;Value Through Intent&rdquo;</a></span><span>&nbsp;argument. I also remember extensive Facebook threads around this time, though I do not have links to share. </span><span>I finally wrote up my thoughts on the topic in detail in </span><span class="c0 c5"><a class="c1" href="https://forum.effectivealtruism.org/posts/BY8gXSpGijypbGitT/why-i-prioritize-moral-circle-expansion-over-artificial">Anthis (2018b)</a></span><span>&nbsp;as part of a </span><span>prioritization argument for moral circle expansion over decreasing extinction risk through AI alignment</span><span>, and this essay is a follow-up to and refinement of those ideas.</span></p><p class="c11"><span>Later in 2018, </span><span class="c0 c5"><a class="c1" href="https://forum.effectivealtruism.org/posts/NfkEqssr7qDazTquW/the-expected-value-of-extinction-risk-reduction-is-positive?fbclid%3DIwAR2Si8qdOEqXdPujDfv6gDGLaTdevs4Tb_CALW0D2MHUC4Ot9evEAoem3Gw">Brauner and Grosse-Holz (2018)</a></span><span>&nbsp;published an EA Forum essay arguing that the expected value of extinction risk reduction is positive</span><span>. </span><span>In my opinion, it failed to consider many of the arguments on the topic</span><span>, as discussed in EA Forum comments and a rebuttal, also on the EA Forum, </span><span class="c0 c5"><a class="c1" href="https://forum.effectivealtruism.org/posts/RkPK8rWigSAybgGPe/a-longtermist-critique-of-the-expected-value-of-extinction-2">DiGiovanni (2021)</a></span><span>.</span><span>&nbsp;There is also a chapter in </span><span class="c0 c5"><a class="c1" href="https://www.basicbooks.com/titles/william-macaskill/what-we-owe-the-future/9781541618633/">MacAskill (</a></span><span class="c0"><a class="c1" href="https://www.basicbooks.com/titles/william-macaskill/what-we-owe-the-future/9781541618633/">forthcoming</a></span><span class="c0 c5"><a class="c1" href="https://www.basicbooks.com/titles/william-macaskill/what-we-owe-the-future/9781541618633/">)</a></span><span>&nbsp;covering similar ground as </span><span class="c5">Brauner and Grosse-Holz, with similar </span><span>arguments missing, in my opinion</span><span>. Overall, t</span><span class="c2">hese writings primarily focus on three arguments:</span></p><ol class="c18 lst-kix_bo32pnaqcwse-0 start" start="1"><li class="c9 c11 li-bullet-0"><span>the </span><span class="c0"><a class="c1" href="#h.5baytopn0b3n">&ldquo;Value Through Intent&rdquo;</a></span><span>&nbsp;or </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/posts/NfkEqssr7qDazTquW/the-expected-value-of-extinction-risk-reduction-is-positive?commentId%3DYBNdTmnm5HtDjKddL">&ldquo;will&rdquo; argument</a></span><span class="c2">, that insofar as humanity exerts its will, we tend to produce value rather than disvalue;</span></li><li class="c11 c9 li-bullet-0"><span class="c2">the likelihood that factory farming and wild animal suffering, the largest types of suffering today, will persist into the far future; and</span></li><li class="c11 c9 li-bullet-0"><span>axiological considerations, particularly the population ethics question of whether creating additional beings with positive welfare is morally good. This has been the main argument against increasing population from some negative utilitarians and other &ldquo;suffering-focused&rdquo; EAs, such as the </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/tag/center-on-long-term-risk">Center on Long-Term Risk (CLR)</a></span><span>&nbsp;and </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/center-for-reducing-suffering">Center for Reducing Suffering (CRS)</a></span><span>, since </span><span class="c0 c5"><a class="c1" href="https://reducing-suffering.org/on-the-seriousness-of-suffering/">Tomasik (2006)</a></span><span>.</span></li></ol><p class="c11"><span>These are three important considerations, b</span><span>ut as I argued above (and at least some of the authors would disagree), </span><span class="c2">they cover only a small portion of the total landscape of evidence and reason that we have available for estimating the EV of human expansion.</span></p><p class="c11"><span>Overall, I think the arguments against a highly positive EV of human expansion have been the most important blindspot of the EA community to date,</span><span>&nbsp;and it is the only major dissenting opinion I have with the core of the EA memeplex. </span><span>I</span><span>&nbsp;would guess o</span><span>ver 90% of </span><span>longtermist</span><span>&nbsp;EAs with whom I have raised this topic have never considered it before</span><span>, despite acknowledging during our conversation that the expected value being highly positive is a crucial assumption for prioritizing extinction risk and that it is on shaky ground&mdash;if not deciding that it is altogether mistaken. </span><span>While examining this assumption and deciding that the far future is not highly positive would not completely overhaul </span><span>longtermist</span><span>&nbsp;EA priorities, it would significantly change our focus. In particular, we should shift resources away from </span><span>extinction risk and towards quality risks, as well as towards global priorities research to better understand this and other crucial considerations.</span><span>&nbsp;I would be eager for more discussion of this topic, and the sort of evidence I expect to most change my mind is the cooperative game theory research done by </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/tag/center-on-long-term-risk">CLR</a></span><span>, the </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/center-for-human-compatible-artificial-intelligence">Center on Human-Compatible AI (CHAI)</a></span><span>, and others in AI safety; the moral circle expansion and digital minds research done by </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/sentience-institute">Sentience Institute (SI)</a></span><span>, </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/future-of-humanity-institute">Future of Humanity Institute (FHI)</a></span><span>, and others in longtermism and AI safety; and all sorts of exploration of concrete scenarios similar to </span><span class="c0 c12"><a class="c1" href="https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198754626.001.0001/isbn-9780198754626">The Age of Em</a></span><span class="c0"><a class="c1" href="https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198754626.001.0001/isbn-9780198754626">&nbsp;(Hanson 2016)</a></span><span>&nbsp;and AI takeoff &ldquo;training stories&rdquo; </span><span class="c0"><a class="c1" href="https://www.alignmentforum.org/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine">(Hubinger 2021)</a></span><span>. I expect fewer updates from more conceptual discourse like the works cited above on the EA Forum and this essay, but I still see them as valuable contributions. See further discussion in the </span><span class="c0"><a class="c1" href="#h.ljn5h87knqaj">&ldquo;Future Research on the EV of Human Expansion&rdquo;</a></span><span>&nbsp;subsection below.</span></p><h2 class="c33 c23 c15" id="h.x0d6xp69ts5b"><span>Terminology</span></h2><p class="c11"><span>I </span><span>separat</span><span>e the moral</span><span>&nbsp;value of the long-term future into two factors: </span><span class="c12">population</span><span>, the number of individuals at each point in time, and </span><span class="c12">quality</span><span>, the moral value of each individual&rsquo;s existence at each point in time. The moral value of the long-term future is thus the double sum of quality across individuals across time. Risks to the number of individuals (living sufficiently positive lives) are </span><span class="c12">population risks</span><span>,</span><span class="c12">&nbsp;</span><span>and risks to the quality of each individual life are </span><span class="c12">quality risks</span><span class="c2">.</span></p><p class="c11"><span>Extinction risks are a particular sort of population risk, those that would &ldquo;annihilate Earth-originating intelligent life,&rdquo; though I would also include threats towards populations of non-Earth-originating and non-intelligent (and perhaps even non-living) individuals who matter morally, and I get the sense that others have also favored this more inclusive definition. </span><span>Non-existential population risks could be a permanent halving of the population or a delay of one-third the universe&rsquo;s remaining lifetime in humanity&rsquo;s interstellar expansion, though there is no consensus on where exactly the cutoff is between existential and non-existential, though there does seem to be consensus that extinction of humans (with no creation of post-humans, such as </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/whole-brain-emulation">whole brain emulations</a></span><span>)</span><span class="c2">&nbsp;is existential.</span></p><p class="c11"><span>Quality risks are risks to the moral value of individuals who may exist in the long-term future. Existential quality risks are those that &ldquo;permanently and drastically curtail its potential&rdquo; moral value, such as all individuals being moved from positive to zero or positive to negative value. </span><span>Non-existential quality risks may include one-tenth of the future population dropping from highly positive to barely positive quality, one-fourth of the future population dropping from barely positive to barely negative quality, and so on. Again, this may be better understood as a spectrum of existentiality, rather than two neatly separated categories, because it is unclear at what point potential is permanently and drastically curtailed. Quality risks include </span><span class="c12">suffering risks</span><span>&nbsp;(also known as</span><span class="c12">&nbsp;s-risks</span><span>), &ldquo;risks of events that bring about suffering in cosmically significant amounts&rdquo; </span><span class="c5">(</span><span class="c0 c5"><a class="c1" href="https://longtermrisk.org/reducing-risks-of-astronomical-suffering-a-neglected-priority/">Althaus and Gloor 2016</a></span><span class="c5">; </span><span class="c0 c5"><a class="c1" href="https://foundational-research.org/risks-of-astronomical-future-suffering/">Tomasik 2011</a></span><span class="c5">), which was noted as </span><span>&ldquo;weirdly sidelined&rdquo; by total utilitarians</span><span class="c5">&nbsp;in </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/posts/n3WwTz4dbktYwNQ2j/critiques-of-ea-that-i-want-to-read">Rowe&rsquo;s (2022) &ldquo;Critiques of EA that I Want to Read.&rdquo;</a></span><span>&nbsp;</span></p><p class="c11"><span>These categories are not meant to coincide with the existential risk taxonomies of </span><span class="c0 c5"><a class="c1" href="https://ora.ox.ac.uk/objects/uuid:827452c3-fcba-41b8-86b0-407293e6617c">Bostrom (2002)</a></span><span>&nbsp;(bangs, crunches, shrieks, whimpers) or </span><span class="c0 c5"><a class="c1" href="https://doi.org/10.1111/1758-5899.12002">Bostrom (2013)</a></span><span>&nbsp;(human extinction, permanent stagnation, flawed realization, subsequent ruination), in part because those are worded in terms of positive potential rather than an aggregation of positive and negative outcomes. However, one can reasonably view some of those categories (e.g., shrieks and failed realizations) as including some positive, zero, or negative quality trajectories because they have a failed realization of positive potential. </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/posts/AJbZ2hHR4bmeZKznG/venn-diagrams-of-existential-global-and-suffering">Aird (2020b)</a></span><span class="c2">&nbsp;has some useful Venn diagrams of the overlaps of some long-term risks.</span></p><p class="c11"><span>The term </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/trajectory-change">&ldquo;trajectory change&rdquo;</a></span><span>&nbsp;has variously been used as a category that, from my understanding, includes the mitigation or exacerbation of all of the risks above, such as </span><span class="c0 c5"><a class="c1" href="https://doi.org/10.7282/T35M649T">Beckstead</a></span><span class="c0"><a class="c1" href="https://doi.org/10.7282/T35M649T">&rsquo;s</a></span><span class="c0 c5"><a class="c1" href="https://doi.org/10.7282/T35M649T">&nbsp;(2013a)</a></span><span>&nbsp;definition of trajectory changes as actions that &ldquo;slightly or significantly alter the world&rsquo;s development trajectory.&rdquo;</span></p><h1 class="c31 c23 c15" id="h.7ab02ciioy49"><span class="c26 c5">What Does the EV Need to be to Prioritize Extinction Risks</span></h1><p class="c11 c34"><span>Explosive forces, energy, materials, machinery will be available upon a scale which can annihilate whole nations. Despotisms and tyrannies will be able to prescribe the lives and even the wishes of their subjects in a manner never known since time began. If to these tremendous and awful powers is added the pitiless sub-human wickedness which we now see embodied in one of the most powerful reigning governments, who shall say that the world itself will not be wrecked, or indeed that it ought not to be wrecked? There are nightmares of the future from which a fortunate collision with some wandering star, reducing the earth to incandescent gas, might be a merciful deliverance. &#11835; </span><span class="c0"><a class="c1" href="https://www.nationalchurchillmuseum.org/fifty-years-hence.html">Winston Churchill (1931)</a></span></p><p class="c11"><span>Under </span><span class="c0"><a class="c1" href="https://en.wikipedia.org/wiki/Von_Neumann%25E2%2580%2593Morgenstern_utility_theorem">the standard definition of utility</a></span><span>, you should take actions with positive expected value (EV), not take actions with negative EV, and it doesn&rsquo;t matter if you take actions with zero EV.</span><sup><a href="#ftnt8" id="ftnt_ref8">[8]</a></sup><span>&nbsp;However, prioritization is plausibly much more complicated than this. </span><span>Is the EV of the action higher than counterfactual actions? Is EV the right approach for imperfect individual decision-makers? Is EV the right approach for a group of people working together? What is the track record for EV decision-making relative to other approaches? Etc. There are many different views that a reasonable person can come to on how best to navigate these conceptual and empirical questions, but </span><span>I believe that the EV needs to be highly positive to prioritize extinction risks</span><span>.</span></p><p class="c11"><span>As I discussed in </span><span class="c0 c5"><a class="c1" href="https://forum.effectivealtruism.org/posts/BY8gXSpGijypbGitT/why-i-prioritize-moral-circle-expansion-over-artificial">Anthis (2018b)</a></span><span>, </span><span>I think an intuitive but mistaken argument on this topic is that if we are uncertain about the EV or expect it is close to zero, we should favor reducing extinction risk to preserve option value. Fortunately I have heard this argument much less frequently in recent years, but it is still in </span><span class="c0"><a class="c1" href="https://80000hours.org/articles/existential-risks/#who-shouldnt-prioritise-safeguarding-the-future">a drop-down section of 80,000 Hours&rsquo; &ldquo;The Case for Reducing Existential Risks.&rdquo;</a></span><span>&nbsp; </span><span>This reasoning seems mistaken for two reasons:</span></p><p class="c11"><span>First, option value is only good insofar as we have control over the exercising of future options or expect those who have control to exercise it well.</span><span>&nbsp;In the course of human civilization, even the totality of the EA</span><span>&nbsp;movement has relatively little control over humanity&rsquo;s actions&mdash;</span><span>though arguably a lot more than most measures would make it appear due to our strategic approach, particularly targeting high-leverage domains such as advanced AI&mdash;and it is unclear that EA will retain even this modest level of control. The argument that option value is good because our d</span><span>escendants will use it well is circular</span><span class="c2">&nbsp;because the case against extinction risk reduction is primarily focused on humanity not using its options well (i.e., humanity not using its options well is both the premise and the conclusion). An argument that relies on the claim that is being contested is very limited. However, we have more control if one thinks extinction timelines are very short and, if one survives, they and their colleagues will have substantial control over humanity&rsquo;s actions; we also may be optimistic about human action despite being pessimistic about the future if we think nonhuman forces such as aliens and evolution are the decisive drivers of long-term disvalue.</span></p><p class="c11"><span>Second, continued human existence very plausibly limits option value in similar ways to nonexistence. </span><span>Whether we are in a time of perils or not, there is no easy &ldquo;off switch&rdquo; for which humanity can decide to let itself go extinct, especially with advanced technologies (e.g., spreading out through </span><span class="c0"><a class="c1" href="https://en.wikipedia.org/wiki/Self-replicating_spacecraft">von Neumann probes</a></span><span>).</span><span>&nbsp;</span><span>It is not as if we can or should reduce extinction risk in the 2020s then easily raise it in the 2030s based on further global priorities research.</span><span class="c2">&nbsp;Still, there is a greater variety of non-extinct than extinct civilizations, so insofar as we want to preserve a wide future of possibilities, that is reason to favor extinction risk reduction.</span></p><p class="c11"><span>Instead of option value, the more important considerations to me are (i) that we have other promising options with high EV such that extinction risk reduction needs to be more positive than these other options in order to justify prioritization and (ii) t</span><span>hat we should have some risk aversion and sandboxing of EV estimates such that we should sometimes treat close-to-zero values as zero.</span><span>&nbsp;</span><span class="c2">It&rsquo;s also unclear how to weigh the totality of evidence here, but insofar as it is weak and speculative&mdash;as with most questions about the long-term future&mdash;one may pull their estimate towards a prior, though it is unclear what that prior should be. If one thinks zero is a particularly common answer in an appropriate reference class, that could be reasonable, but it depends on many factors beyond the scope of this essay.</span></p><h1 class="c31 c23 c15" id="h.4jjftlhxeg3m"><span class="c26 c5">Time-Sensitivity</span></h1><p class="c11"><span>If we are allocating resources to both population and quality risks, one could argue that we should spend resources on population risks first because the quality of individual lives only matters insofar as those individuals exist. T</span><span>he opposite is true as well: </span><span>For example, if a quality of zero were locked in for the long-term future, then increasing or decreasing the population would have no moral value or disvalue. Outcomes of exactly zero quality might seem less likely than outcomes of exactly zero population, though this depends on the </span><span class="c0"><a class="c1" href="#h.f5vv1k9eapz5">&ldquo;EV of the Counterfactual&rdquo;</a></span><span class="c2">&nbsp;(e.g., life originating on other planets) and is more contentious for close-to-zero quantities.</span></p><p class="c11"><span>A</span><span>s with option value, the future depends on the past, so for every year that passes, the future has fewer degrees of freedom. This is most apparent in the development of advanced AI, in which its development may hinge on early-stage choices, such as selecting training regimes that are more likely to lead to its alignment with its designers&rsquo; value or selecting those values with which to align the AI (i.e., </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/value-lock-in/history">value lock-in</a></span><span>). </span><span>In general, </span><span>there are strong arguments for time-sensitivity for both types of trajectory change, especially with advanced technology&mdash;also </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/aging-research">life extension</a></span><span>&nbsp;and </span><span class="c0"><a class="c1" href="https://en.wikipedia.org/wiki/Self-replicating_spacecraft">von Neumann probes</a></span><span>&nbsp;in particular.</span></p><h1 class="c23 c15 c31" id="h.3q0wzj3boa7y"><span>Biases</span></h1><p class="c11 c34"><span>To our amazement we suddenly exist, after having for countless millennia not existed; in a short while we will again not exist, also for countless millennia. &nbsp;That cannot be right, says the heart. </span><span>&#11835;</span><span>&nbsp;</span><span class="c0"><a class="c1" href="https://en.wikipedia.org/wiki/The_World_as_Will_and_Representation">Arthur Schopenhauer (1818, translation 2008)</a></span></p><p class="c11"><span>We could be biased towards optimism or pessimism. </span><span>Among the demographics of EA, I think that we should probably be more worried about bias towards optimism. Extreme suffering, as described by </span><span class="c0"><a class="c1" href="https://reducing-suffering.org/on-the-seriousness-of-suffering/">Tomasik (2006)</a></span><span>,</span><span>&nbsp;is a topic that people are very tempted to ignore, downplay, or rationalize </span><span class="c0"><a class="c1" href="https://www.wiley.com/en-us/States%2Bof%2BDenial%253A%2BKnowing%2Babout%2BAtrocities%2Band%2BSuffering-p-9780745623924">(Cohen 2001)</a></span><span>. </span><span>In general, the prospect of future dystopias is uncomfortable and unpleasant to think about. Most of us dread the possibility that our legacy in the universe could be a tragic one, and such a gloomy outlook does not resonate with favored trends of techno-optimism or the heroic notion of saving humanity from extinction.</span><span>&nbsp;However, the sign of this bias can be flipped, such as in social groups where pessimism and doomsaying is in vogue.</span><span>&nbsp;</span><span>My experience is that people in EA and longtermism tend to be much more ready to dismiss pessimism and suffering-focused ethics than optimism and happiness-focused ethics, especially based on superficial claims that pessimism is driven by the personal dispositions and biases of its proponents. </span><span>For a more detailed discussion on biases related to (not) prioritizing suffering, see </span><span class="c0"><a class="c1" href="https://magnusvinding.files.wordpress.com/2020/05/suffering-focused-ethics.pdf">Vinding (2020)</a></span><span class="c2">.</span></p><p class="c11"><span>Additionally, given the default approach to longtermism and existential risk is to reduce extinction risk, and there has already been over a decade of focus on that, we should be very concerned about </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/status-quo-bias">status quo bias</a></span><span>&nbsp;and the incentive structure of EA as it is today</span><span>. This is one reason to encourage self-critique as individuals and as a community, such as with the </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/criticism-and-red-teaming-contest">Criticism and Red-Teaming Contest</a></span><span>. In fact, that contest is one reason I wrote this essay, though I was already committed to writing a book chapter on this topic before the contest was announced.</span></p><p class="c11"><span>I think we should focus more on the object-level arguments than on biases, but given how our answer to this question hinges on our intuitive estimates of extremely complicated figures, bias is probably more important than normal. I further discussed the merits of considering bias and listed many possible biases towards both moral circle expansion and reducing extinction risk through AI alignment in </span><span class="c0 c5"><a class="c1" href="https://forum.effectivealtruism.org/posts/BY8gXSpGijypbGitT/why-i-prioritize-moral-circle-expansion-over-artificial">Anthis (2018b)</a></span><span>.</span></p><p class="c11"><span class="c2">One conceptual challenge is that a tendency towards pessimism or optimism could either be accounted for as a bias that needs correction or as a fact about the relative magnitudes of value and disvalue. On one hand, we might say that the importance of disvalue in evolution (e.g. the constant danger of one misstep curtailing all future spread of one&rsquo;s genes) has made us care more about suffering than we should. On the other hand, we might say that it is a fact about how disvalue tends to be more common, subjectively worse, or objectively worse in the universe. </span></p><h1 class="c31 c23 c15" id="h.ceibcg4xgxqs"><span>Brief Thoughts on the Prioritization of Quality Risks</span></h1><p class="c11 c34"><span>People ask me to predict the future, when all I want to do is prevent it. Better yet, build it. Predicting the future is much too easy, anyway. You look at the people around you, the street you stand on, the visible air you breathe, and predict more of the same. To hell with more. I want better. </span><span>&#11835;</span><span>&nbsp;</span><span class="c0"><a class="c1" href="https://books.google.com/books/about/Yestermorrow.html?id%3Dgc3LDgAAQBAJ">Ray Bradbury (1979)</a></span></p><p class="c11"><span>I present a more detailed argument for the prioritization of quality risks (particularly moral circle expansion) over extinction risk reduction (particularly through certain sorts of AI research) in </span><span class="c0 c5"><a class="c1" href="https://forum.effectivealtruism.org/posts/BY8gXSpGijypbGitT/why-i-prioritize-moral-circle-expansion-over-artificial">Anthis (2018)</a></span><span>, but here I will briefly note some thoughts on </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/itn-framework-1">importance, tractability, and neglectedness</a></span><span>. Two related EA Forum posts are </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/posts/225Aq4P4jFPoWBrb5/cause-prioritization-for-downside-focused-value-systems">&ldquo;Cause Prioritization for Downside-Focused Value Systems&rdquo; (Gloor 2018)</a></span><span>&nbsp;and </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/posts/LpkXtFXdsRd4rG8Kb/reducing-long-term-risks-from-malevolent-actors">&ldquo;Reducing Long-Term Risks from Malevolent Actors&rdquo; (Althaus and Baumann 2020)</a></span><span>.</span><span>&nbsp;Additionally, at this early stage of the </span><span>longtermist</span><span>&nbsp;movement, the top priorities for population and quality risk may largely intersect. Both issues suggest foundational research of topics such as the nature of AI control and likely trajectories of the long-term future, community-building of thoughtful do-gooders, and field-building of institutional infrastructure to use for steering the long-term future.</span></p><h2 class="c23 c15 c33" id="h.2a4ch4j91e26"><span class="c5 c21">Importance</span></h2><p class="c11"><span class="c2">One important application of the EV of human expansion is to the &ldquo;importance&rdquo; of population and quality risks. Importance can be operationalized as the good done if the entire cause succeeded in solving its corresponding problem, such as the good done by eliminating or substantially reducing extinction risk, which is effectively zero if the EV of human expansion is zero and effectively negative if the EV of human expansion is negative.</span></p><p class="c11"><span>The importance of quality risk reduction is clearer, in the sense that the difference in quality between possible futures is clearer than the difference in extinction and non-extinction, and larger,</span><span>&nbsp; in the sense that while population risk entails only the range of zero-to-positive difference between human extinction and non-extinction (or population risk between zero population and some positive number of individuals) across quality risk entails the difference between the best quality humans could engender and the worst, across all possible population sizes. </span><span>This is arguably a weakness of the framework because we could categorize the quality risk cause area as smaller in importance (say, an increase of 1 trillion utils, i.e., units of goodness), and it would tend to become more tractable as we narrow the category.</span></p><h2 class="c33 c23 c15" id="h.vnskdt9lbf5g"><span class="c21 c5">Tractability</span></h2><p class="c11"><span>The tractability difference </span><span>between population and quality risk </span><span>seems the least clear of the three criteria</span><span>. My general approach is thinking through the most likely </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/theory-of-change">&ldquo;theories of change&rdquo;</a></span><span>&nbsp;or paths to impact and assessing them step-by-step. For example, one commonly discussed extinction risk reduction path to impact is </span><span class="c0"><a class="c1" href="https://www.alignmentforum.org/tag/agent-foundations">&ldquo;agent foundations,&rdquo;</a></span><span>&nbsp;building mathematical frameworks and formally proving claims about the behavior of intelligent agents, which would then allow us to build advanced AI systems more likely to do what we tell them to do, and then using these frameworks to build AGI or persuading the builders of AGI to use them. Quality-risk-focused AI safety strategies may be more focused on the </span><span class="c0"><a class="c1" href="https://www.alignmentforum.org/tag/outer-alignment">outer alignment</a></span><span>&nbsp;problem, ensuring that an AI&rsquo;s objective is aligned with the right values</span><span>, rather than just the </span><span class="c0"><a class="c1" href="https://www.alignmentforum.org/tag/inner-alignment">inner alignment</a></span><span>&nbsp;problem, ensuring that all actions of the AI are aligned with the objective</span><span>.</span><span>&nbsp;Also, we can influence quality by steering the &ldquo;direction&rdquo; or &ldquo;speed&rdquo; of the long-term future, approaches with potentially very different impact, hinging on factors such as the distribution of likely futures across value and likelihood (e.g., </span><span class="c0"><a class="c1" href="https://www..com/watch?v%3DNTV81NZSuKw">Anthis 2018c</a></span><span>; </span><span class="c0 c5"><a class="c1" href="https://doi.org/10.1016/j.futures.2021.102756">Anthis and Paez 2021</a></span><span>)</span><span class="c2">.</span></p><p class="c11"><span>One argument that I often hear on the tractability of trajectory changes is that changes need to &ldquo;stick&rdquo; or &ldquo;persist&rdquo; over long periods.</span><span>&nbsp;It is true that there needs to be a persistent change in the expected value (i.e., the </span><span class="c0"><a class="c1" href="https://en.wikipedia.org/wiki/Random_variable">random variable</a></span><span>&nbsp;or time series </span><span class="c0"><a class="c1" href="https://www.sciencedirect.com/science/article/pii/0304407690900939">regime</a></span><span>&nbsp;of value in the future), but I frequently hear the claim that there needs to be a persistent change in the </span><span class="c0"><a class="c1" href="https://en.wikipedia.org/wiki/Realization_(probability)">realization</a></span><span>&nbsp;of that value.</span><span>&nbsp;For example, if we successfully broker a peace deal between great powers, </span><span>neither the peace deal itself nor any other particular change in the world has to persist in order for this to have high long-term impact.</span><span class="c2">&nbsp;The series of values itself can have arbitrarily large variance, such as it being very likely that the peace deal is broken within a decade.</span></p><p class="c11"><span>For a sort of change to be intractable, it needs to not just lack persistence, but </span><span>to rubber band (i.e., create opposite-sign effects) back to its counterfactual</span><span>.</span><span class="c2">&nbsp;For example, if brokering a peace deal causes an equal and opposite reaction of anti-peace efforts, then that trajectory change is intractable. Moreover, we should not only consider rubber banding but dominoing (i.e., create same-sign effects), perhaps because of how this peace deal inspires other great powers to follow suit even if this particular deal is broken. There is much of this potential energy in the world waiting to be unlocked by thoughtful actors.</span></p><p class="c11"><span>The tractability of trajectory change has been the subject of research at Sentience Institute, including our </span><span class="c0"><a class="c1" href="http://sentienceinstitute.org/research">historical case studies</a></span><span>&nbsp;and </span><span class="c0"><a class="c1" href="https://www.sentienceinstitute.org/blog/how-tractable-is-changing-the-course-of-history">&ldquo;Harris&rsquo; (2019)&rdquo; How Tractable Is Changing the Course of History?&rdquo;</a></span></p><h2 class="c33 c23 c15" id="h.tyn83i5szzbh"><span class="c21 c5">Neglectedness</span></h2><p class="c11"><span>The neglectedness difference between population and quality risk seems the most clear. </span><span>There are far more EAs and longtermists working explicitly on population risks than on quality risks</span><span class="c2">&nbsp;(i.e., risks to the moral value of individuals in the long-term future). Two nuances for this claim are first that it may not be true for other relevant comparisons: For example, many people in the world are trying to change social institutions, such as different sides of the political spectrum trying to pull public opinion towards their end of the spectrum. This group seems much larger than people focused explicitly on extinction risks, and there are many other relevant reference classes. Second, it is not entirely clear whether extinction risk reduction and quality risk reduction face higher or lower returns to being less neglected (i.e., more crowded). It may be that so few people are focused on quality risks that marginal returns are actually lower than they would be if there were more people working on them (i.e., increasing returns).</span></p><h1 class="c31 c23 c15" id="h.ljn5h87knqaj"><span class="c26 c5">Future Research on the EV of Human Expansion</span></h1><p class="c11"><span>Because most events in the long-term future entail some sort of value or disvalue, most new information from </span><span>longtermist</span><span>&nbsp;research provides some evidence on the EV of human expansion. As stated above, I&rsquo;m particularly excited about cooperative game theory research (e.g., </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/center-on-long-term-risk">CLR</a></span><span>, </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/center-for-human-compatible-artificial-intelligence">CHAI</a></span><span>), moral circle expansion and digital minds research (e.g., </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/sentience-institute">SI</a></span><span>, </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/future-of-humanity-institute">FHI</a></span><span>), and exploration of concrete trajectories </span><span class="c5">(e.g., </span><span class="c0 c5"><a class="c1" href="https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198754626.001.0001/isbn-9780198754626">Hanson 2016</a></span><span class="c5">; </span><span class="c0 c5"><a class="c1" href="https://www.alignmentforum.org/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine">Hubinger 2021</a></span><span class="c5">)</span><span>. I&rsquo;m</span><span>&nbsp;relatively less excited (though still excited!), on the margin, by entirely armchair taxonomization and argumentation like that in this essay. That includes research on axiological asymmetries, such as more debate on </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/suffering-focused-ethics">suffering-focused ethics</a></span><span>&nbsp;or </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/population-ethics">population ethics</a></span><span class="c2">, though these can be more useful for other topics and perhaps other people considering this question. My lack of enthusiasm is largely because in the past 8 years of having this view that the EV of human expansion is not highly positive, very little of the new evidence has come from armchair reasoning and argumentation, despite that being more common (although what sort of research is most common depends on where one draws the boundaries because, again, so much research has implications for EV).</span></p><p class="c11"><span>In general, this is such an encompassing, big-picture topic that empirical data is extremely limited relative to scope, and it seems necessary to rely on qualitative intuitions, quantitative intuitions, or back-of-the-envelope calculations a la </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/posts/fogJKYXvqzkr9KCud/a-complete-quantitative-model-for-cause-selection">Dickens&rsquo; (2016) &ldquo;A Complete Quantitative Model for Cause Selection&rdquo;</a></span><span>&nbsp;or </span><span class="c0"><a class="c1" href="https://globalprioritiesinstitute.org/wp-content/uploads/Tarsney-Epistemic-Challenge-to-Longtermism.pdf">Tarsney&rsquo;s (2022) &ldquo;The Epistemic Challenge to Longtermism.&rdquo;</a></span><span>&nbsp;I would like to see a more systematic survey of such intuitions, ideally from 5-30 people who have read through this essay and the </span><span class="c0"><a class="c1" href="#h.onh7u8risudz">&ldquo;Related Work.&rdquo;</a></span><span>&nbsp;Ideally these would be stated as credible intervals or similar probability distributions, such that we can more easily quantify uncertainty in the overall estimate. As with all topics, I think we should </span><span class="c0"><a class="c1" href="https://en.wikipedia.org/wiki/Aumann%2527s_agreement_theorem">Aumann</a></span><span>&nbsp;update on each other&rsquo;s views, a process in which I split the difference between my belief and someone else&rsquo;s even if I do not know all the prior and posterior evidence on which they base their view. Of course, this is messy in the real world, for instance because we presumably should account not just for the few people with whom we happen to know their beliefs, but also for our expectations of the many people who also have a belief and even hypothetical people who could have a belief (e.g., unbiased versions of real-world people). It is also unclear whether normative (e.g., moral) views constitute the sort of belief that should be updated in this way, such as between people with fundamentally different value trade-offs between happiness and suffering.</span><sup><a href="#ftnt9" id="ftnt_ref9">[9]</a></sup><span>&nbsp;There are </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/moral-cooperation">cooperative reasons</a></span><span>&nbsp;to deeply account for others&rsquo; views, and one may choose to account for </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/moral-uncertainty">moral uncertainty</a></span><span>.</span><span class="c2">&nbsp;In general, I would be very interested in a survey that just asks for numbers like those in the table above and allows us to aggregate those beliefs in a variety of ways; a more detailed case for how that aggregation should work is beyond the scope of this essay.</span></p><p class="c11"><span>If you are persuaded by the arguments that the expected value of human expansion is not highly positive or that we should prioritize the quality of the long-term future, promising approaches include research, field-building, and community-building, such as at the </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/tag/center-on-long-term-risk">Center on Long-Term Risk</a></span><span>, </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/center-for-reducing-suffering">Center for Reducing Suffering</a></span><span>, </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/future-of-humanity-institute">Future of Humanity Institute</a></span><span>, </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/global-catastrophic-risk-institute">Global Catastrophic Risk Institute</a></span><span>, </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/legal-priorities-project">Legal Priorities Project</a></span><span>, and </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/open-philanthropy">Open Philanthropy</a></span><span>, and </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/sentience-institute">Sentience Institute</a></span><span>, as well as working at other AI safety and EA organizations with an eye towards ensuring that</span><span>, if we survive, the </span><span>universe is better for it.</span><span>&nbsp;Some of this work has substantial </span><span class="c0"><a class="c1" href="https://www.sentienceinstitute.org/blog/eoy2021">room for more funding</a></span><span>, and related jobs can be found at these organizations&rsquo; websites and on </span><span class="c0"><a class="c1" href="https://80000hours.org/job-board/">the 80,000 Hours job board</a></span><span class="c2">.</span></p><h1 class="c31 c23 c15" id="h.fk4pxl3k6152"><span class="c26 c5">References</span></h1><p class="c4"><span>Aird, Michael. 2020a. &ldquo;Clarifying Existential Risks and Existential Catastrophes.&rdquo; Effective Altruism Forum. </span><span>https://forum.effectivealtruism.org/posts/skPFH8LxGdKQsTkJy/clarifying-existential-risks-and-existential-catastrophes.</span></p><p class="c4"><span>&mdash;&mdash;&mdash;. 2020b. &ldquo;Venn Diagrams of Existential, Global, and Suffering Catastrophes.&rdquo; Effective Altruism Forum. </span><span>https://forum.effectivealtruism.org/posts/AJbZ2hHR4bmeZKznG/venn-diagrams-of-existential-global-and-suffering</span><span class="c2">.</span></p><p class="c4"><span class="c5">Althaus, David, and Tobias</span><span class="c5">&nbsp;Baumann. 2020. &ldquo;Reducing Long-Term Risks from Malevolent Actors.&rdquo; Effective Altruism Forum. https://forum.effectivealtruism.org/posts/LpkXtFXdsRd4rG8Kb/reducing-long-term-risks-from-malevolent-actors.</span></p><p class="c4"><span class="c5">Althaus, David, and Lukas G</span><span class="c5">loor. 2016. &ldquo;Reducing Risks of Astronomical Suffering: A Neglected Priority.&rdquo; Cen</span><span>ter on Long-Term Risk. </span><span class="c5">https://longtermrisk.org/reducing-risks-of-astronomical-suffering-a-neglected-priority/.</span></p><p class="c4"><span class="c5">Anthis, Jacy Reese. 2014. &ldquo;How Do We Reliably Impact the Far Future?&rdquo; The Best We Can. https://web.archive.org/web/20151106103159/http://thebestwecan.org/2014/07/20/how-do-we-reliably-impact-the-far-future/.</span></p><p class="c4"><span class="c5">&mdash;&mdash;&mdash;. 2016a. &ldquo;Some Considerations for Different Ways to Reduce X-Risk.&rdquo; </span><span>Effective Altruism Forum</span><span class="c2">. https://forum.effectivealtruism.org/posts/NExT987oY5GbYkTiE/some-considerations-for-different-ways-to-reduce-x-risk.</span></p><p class="c4"><span class="c2">&mdash;&mdash;&mdash;. 2016b. &ldquo;Why Animals Matter for Effective Altruism.&rdquo; Effective Altruism Forum. https://forum.effectivealtruism.org/posts/ch5fq73AFn2Q72AMQ/why-animals-matter-for-effective-altruism.</span></p><p class="c4"><span>&mdash;&mdash;&mdash;. 2018a. </span><span class="c12">The End of Animal Farming: How Scientists, Entrepreneurs, and Activists Are Building an Animal-Free Food System</span><span>. Boston: Beacon Press.</span></p><p class="c4"><span class="c5">&mdash;&mdash;&mdash;. 2018b. &ldquo;Why I Pr</span><span class="c5">ioritize Moral Circle Expansion Over Artificial Intelligence Alignment.&rdquo; Effective Altruism Forum. https://forum.effectivealtruism.org/posts/BY8gXSpGijypbGitT/why-i-prioritize-moral-circle-expansion-over-artificial.</span></p><p class="c4"><span>&mdash;&mdash;&mdash;. 2018c. &ldquo;Animals and the Far Future.&rdquo; EAGxAustralia. https://www..com/watch?v=NTV81NZSuKw.</span></p><p class="c4"><span class="c5">&mdash;&mdash;&mdash;. 2022. &ldquo;Consciousness Semanticism: A Precise Eliminativist Theory of Consciousness.&rdquo; In </span><span class="c5 c12">Biologically Inspired Cognitive Architectures 2021</span><span class="c5">, edited by Valentin V. Klimov and David J. Kelley, 1032:20&ndash;41. Studies in Computational Intelligence. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-96993-6_3.</span></p><p class="c4"><span class="c5">Anthis, Jacy Reese, and Eze Paez. 2021. &ldquo;Moral Circle Expansion: A Promising Strategy to Impact the Far Future.&rdquo; </span><span class="c5 c12">Futures</span><span class="c5">&nbsp;130: 102756. </span><span class="c0 c5"><a class="c1" href="https://doi.org/10.1016/j.futures.2021.102756">https://doi.org/10.1016/j.futures.2021.102756</a></span><span class="c2">.</span></p><p class="c4"><span>Askell, Amanda, Yuntao Bai, Anna Chen, et al. &ldquo;A General Language Assistant as a Laboratory for Alignment.&rdquo; ArXiv. https://arxiv.org/abs/2112.00861.</span></p><p class="c4"><span class="c5">Beckstead, Nic</span><span>k</span><span class="c5">. 2013a. &ldquo;On the Overwhelming Importance of Shaping the Far Future.&rdquo; Rutgers University. https://doi.org/10.7282/T35M649T.</span></p><p class="c4"><span>&mdash;&mdash;&mdash;</span><span class="c5">. </span><span class="c5">2013b. &ldquo;A Proposed Adjustment to the Astronomical Waste Argument.&rdquo; </span><span>e</span><span class="c5">ffectivealtruism.</span><span>o</span><span class="c5">rg. https://www.ef</span><span class="c5">fectivealtruism.org/articles/a-proposed-adjustment-to-the-astronomical-waste-argument-nick-beckstead.</span></p><p class="c4"><span class="c5">Benatar, David. 2006. </span><span class="c5 c12">Better Never to Have Been: The Harm of Coming into Existence</span><span class="c5">. New York: Clarendon Press.</span></p><p class="c4"><span class="c5">Bostrom, Nick. 2002. &ldquo;Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards.&rdquo; </span><span class="c5 c12">Journal of Evolution and Technology</span><span class="c5">&nbsp;9. https://ora.ox.ac.uk/objects/uuid:827452c3-fcba-41b8-86b0-407293e6617c.</span></p><p class="c4"><span class="c5">&mdash;&mdash;&mdash;. 2003. &ldquo;Astronomical Waste: The Opportunity Cost of Delayed Technological Development.&rdquo; </span><span class="c5 c12">Utilitas</span><span class="c2">&nbsp;15 (3): 308&ndash;14. https://doi.org/10.1017/S0953820800004076.</span></p><p class="c4"><span>&mdash;&mdash;&mdash;. 2003. &ldquo;Moral uncertainty &ndash; towards a solution?&rdquo; </span><span class="c12">Overcoming Bias</span><span>. https://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html.</span></p><p class="c4"><span class="c5">&mdash;&mdash;&mdash;. 2012. </span><span class="c5 c12">Global Catastrophic Risks</span><span class="c5">. Repr. Oxford: Oxford University Press.</span></p><p class="c4"><span class="c5">&mdash;&mdash;&mdash;. 2013. &ldquo;Existential Risk Prevention as Global Priority.&rdquo; </span><span class="c5 c12">Global Policy</span><span class="c5">&nbsp;4 (1): 15&ndash;31. https://doi.org/10.1111/1758-5899.12002.</span></p><p class="c4"><span class="c5">&mdash;&mdash;&mdash;. 2014. </span><span class="c5 c12">Superintelligence: Paths, Dangers, Strategies</span><span class="c2">. Oxford: Oxford University Press.</span></p><p class="c4"><span>Bradbury, Ray. 1979. &ldquo;Beyond 1984: The People Machines.&rdquo; In </span><span class="c12">Yestermorrow: Obvious Answers to Impossible Futures.</span></p><p class="c4"><span class="c5">Brauner, Jan M., and Friederike M. Gr</span><span class="c5">osse-Holz. 2018. &ldquo;The Expected Value of Extinction Risk Reduction Is Positive.&rdquo; Effective Altruism Forum. https://forum.effectivealtruism.org/posts/NfkEqssr7qDazTquW/the-expected-value-of-extinction-risk-reduction-is-positive.</span></p><p class="c4"><span>Cicero, Marcus Tullius. 1931 </span><span class="c12">Cicero: De Finibus</span><span class="c2">. Translated by H. Harris Rackham. 2nd ed. Vol. XVII. Loeb Classical Library. Cambridge: Harvard University Press.</span></p><p class="c4"><span class="c5">Christiano, Paul. 2013. &ldquo;Why Might the Fu</span><span class="c5">ture Be Good?&rdquo; Rational Altruist. https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/.</span></p><p class="c4"><span class="c2">Churchill, Winston. 1931. &ldquo;Fifty Years Hence. https://www.nationalchurchillmuseum.org/fifty-years-hence.html.</span></p><p class="c4"><span class="c5">Cowen, Tyler. 2018. </span><span class="c30 c5 c12">Stubborn Attachments: A Vision for a Society of Free, Prosperous, and Responsible Individuals.</span></p><p class="c4"><span>Crootof, Rebecca. 2019. &ldquo;&#39;Cyborg Justice&#39; and the Risk of Technological-Legal Lock-In.&rdquo; </span><span class="c12">119 Columbia Law Review Forum </span><span class="c2">233.</span></p><p class="c4"><span class="c5">Deutsch, David. 2011. </span><span class="c5 c12">The Beginning of Infinity: Explanations That Transform the World</span><span class="c5">. London: Allen Lane.</span></p><p class="c4"><span class="c5">Dickens, Michael. 2016. &ldquo;A Complete</span><span class="c5">&nbsp;Quantitative Model for Cause Selection.&rdquo; </span><span>Effective Altruism Forum</span><span class="c5">. https://forum.effectivealtruism.org/posts/fogJKYXvqzkr9KCud/a-complete-quantitative-model-for-cause-selection.</span></p><p class="c4"><span class="c5">DiGiovanni, Anthony. 2021. &ldquo;A Lo</span><span>ng</span><span class="c2">termist Critique of &lsquo;The Expected Value of Extinction Risk Reduction Is Positive.&rsquo;&rdquo; Effective Altruism Forum. https://forum.effectivealtruism.org/posts/RkPK8rWigSAybgGPe/a-longtermist-critique-of-the-expected-value-of-extinction-2.</span></p><p class="c4"><span>Gloor, Lukas. 2017. &ldquo;Tranquilism.&rdquo; Center on Long-Term Risk. https://longtermrisk.org/tranquilism/.</span></p><p class="c4"><span>&mdash;&mdash;&mdash;</span><span class="c5">. 2018. &ldquo;Cause Prioritization for Downside-Focused Value Systems.&rdquo; </span><span>Effective Altruism Forum. </span><span class="c5">https://forum.effectivealtruism.</span><span class="c5">org/posts/225Aq4P4jFPoWBrb5/cause-prioritization-for-downside-focused-value-systems.</span></p><p class="c4"><span class="c5">Greaves, Hilary, and Will MacAskill. 2017. &ldquo;A Research Agenda for the Global Priorities Institute.&rdquo; https://globalprioritiesinstitute.org/wp-content/uploads/GPI-Research-Agenda-December-2017.pdf.</span></p><p class="c4"><span class="c5">Hanson, Robin. 2016. </span><span class="c5 c12">The Age of Em: Work, Love, and Life When Robots Rule the Earth</span><span class="c5">. First Edition. Oxford: Oxford University Press.</span></p><p class="c4"><span class="c2">Harris, Jamie. 2019. &ldquo;How Tractable Is Changing the Course of History?&rdquo; Sentience Institute. http://www.sentienceinstitute.org/blog/how-tractable-is-changing-the-course-of-history.</span></p><p class="c4"><span>Hobbhan, Marius, Eric Landgrebe, and Beth Barnes. &ldquo;Reflection Mechanisms as an Alignment target: A Survey.&rdquo; </span><span class="c12">LessWrong</span><span>. https://www.lesswrong.com/posts/XyBWkoaqfnuEyNWXi/reflection-mechanisms-as-an-alignment-target-a-survey-1.</span></p><p class="c4"><span class="c5">Hubinger, Evan. 2021. &ldquo;How Do We Become </span><span class="c2">Confident in the Safety of a Machine Learning System? - AI Alignment Forum.&rdquo; AI Alignment Forum. https://www.alignmentforum.org/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine.</span></p><p class="c4"><span>Knutsson, Simon. 2017. &ldquo;Reply to Shulman&rsquo;s &lsquo;Are Pain and Pleasure Equally Energy-Efficient?&rsquo;&rdquo; http://www.simonknutsson.com/reply-to-shulmans-are-pain-and-pleasure-equally-energy-efficient/.</span></p><p class="c4"><span class="c5">MacAskill, William. </span><span>Forthcoming (2022)</span><span class="c5">. </span><span class="c5 c12">What We Owe the Future: A Million-Year View</span><span class="c5">. New York: Basic Books.</span></p><p class="c4"><span class="c5">Matheny, Jason G. 2007. &ldquo;Reducing the Risk of Human Extinction.&rdquo; </span><span class="c5 c12">Risk Analysis</span><span class="c5">&nbsp;27 (5): 1335&ndash;44. https://doi.org/10.1111/j.1539-6924.2007.00960.x.</span></p><p class="c4"><span class="c5">Moynihan, Thomas. 2020. </span><span class="c5 c12">X-Risk: How Humanity Discovered Its Own Extinction</span><span class="c5">. Falmouth: Urbanomic.</span></p><p class="c4"><span class="c5">Ord, Toby. 2020. </span><span class="c5 c12">The Precipice: Existential Risk and the Future of Humanity</span><span class="c5">. New York: Hachette Books.</span></p><p class="c4"><span>Parfit, Derek. 2017. </span><span class="c12">On What Matters: Volume Three</span><span class="c2">. Oxford: Oxford University Press.</span></p><p class="c4"><span class="c5">Pinker, Steven. 2012. </span><span class="c5 c12">The Better Angels of Our Nature</span><span class="c5">. New York Toronto London: Penguin Books.</span></p><p class="c4"><span class="c5">&mdash;&mdash;&mdash;. 2018. </span><span class="c5 c12">Enlightenment Now</span><span class="c2">. New York, New York: Viking, an imprint of Penguin Random House LLC.</span></p><p class="c4"><span class="c2">Plant, Michael. 2022. &ldquo;Will faster economic growth make us happier? The relevance of the Easterlin Paradox to Progress Studies.&rdquo; Effective Altruism Forum. https://forum.effectivealtruism.org/posts/gCDsAj3K5gcZvGgbg/will-faster-economic-growth-make-us-happier-the-relevance-of.</span></p><p class="c4"><span>Rowe, Abraham. 2022. &ldquo;Critiques of EA that I Want to Read.&rdquo; Effective Altruism Forum. https://forum.effectivealtruism.org/posts/n3WwTz4dbktYwNQ2j/critiques-of-ea-that-i-want-to-read.</span></p><p class="c4"><span class="c5">Russell, Stuart J. 2019. </span><span class="c5 c12">Human Compatible: Artificial Intelligence and the Problem of Control</span><span class="c2">. New York: Viking.</span></p><p class="c4"><span>Schopenhauer, Arthur. 2008 [1818]. </span><span class="c12">The World as Will and Representation</span><span class="c2">. New York: Routledge.</span></p><p class="c4"><span>Shulman, Carl. 2012. &ldquo;Are Pain and Pleasure Equally Energy-Efficient?&rdquo; </span><span class="c12">Reflective Disequillibrium</span><span class="c2">. http://reflectivedisequilibrium.blogspot.com/2012/03/are-pain-and-pleasure-equally-energy.html. </span></p><p class="c4"><span class="c2">Smith, Tom W., Peter Marsden, Michael Hout, and Jibum Kim. 2022. &ldquo;General Social Surveys, 1972-2022.&rdquo; National Opinion Research Center. https://www.norc.org/PDFs/COVID%20Response%20Tracking%20Study/Historic%20Shift%20in%20Americans%20Happiness%20Amid%20Pandemic.pdf.</span></p><p class="c4"><span>Tarsney, Christian. 2022. &ldquo;The Epistemic Challenge to Longtermism.&rdquo; Global Priorities Institute. https://globalprioritiesinstitute.org/wp-content/uploads/Tarsney-Epistemic-Challenge-to-Longtermism.pdf.</span></p><p class="c4"><span class="c5">Tegmark, Max. 2017. </span><span class="c5 c12">Life 3.0: Being Human in the Age of Artificial Intelligence</span><span class="c5">. New York: Alfred A. Knopf.</span></p><p class="c4"><span class="c5">Tomasik, Brian. 2006. &ldquo;On the Seriousness</span><span class="c5">&nbsp;of Suffering.&rdquo; Essays on Reducing Suffering. https://reducing-suffering.org/on-the-seriousness-of-suffering/.</span></p><p class="c4"><span class="c5">&mdash;&mdash;&mdash;. 2011. &ldquo;Risks of Astronomical Future Suffering.&rdquo; Foundational Research Institute. https://foundational-research.org/risks-of-astronomical-future-suffering/.</span></p><p class="c4"><span class="c5">&mdash;&mdash;&mdash;. 2013a. &ldquo;The Future of Darwinism.&rdquo; Essays on Reducing Suffering. https://reducing-suffering.org/the-future-of-darwinism/.</span></p><p class="c4"><span class="c5">&mdash;&mdash;&mdash;. 2013b. &ldquo;Values Spreading Is Often More Important than Extinction Risk.&rdquo; Essays on Reducing Suffering. https://reducing-suffering.org/values-spreading-often-important-extinction-risk/.</span></p><p class="c4"><span class="c5">&mdash;&mdash;&mdash;. 2014. &ldquo;Why the Modesty Argument for Moral Realism Fails.&rdquo; Essays on Reducing Suffering. https://reducing-suffering.org/why-the-modesty-argument-for-moral-realism-fails/.</span></p><p class="c4"><span class="c5">&mdash;&mdash;&mdash;. 2015. &ldquo;Artificial Intelligence and Its Implications for Future Suffering.&rdquo; Center on Long-Term Risk. https://longtermrisk.org/artificial-intelligence-and-its-implications-for-future-suffering/.</span></p><p class="c4"><span class="c5">&mdash;&mdash;&mdash;. 2017. &ldquo;Will Future Civilization Eventually Achieve Goal Preservation?&rdquo; </span><span class="c2">Essays on Reducing Suffering. https://reducing-suffering.org/will-future-civilization-eventually-achieve-goal-preservation/.</span></p><p class="c4"><span>Vinding, Magnus. 2020. </span><span class="c12">&#8203;&#8203;&#8203;&#8203;Suffering-Focused Ethics: Defense and Implications</span><span class="c2">. Ratio Ethica.</span></p><p class="c4"><span class="c5">West, Ben. 2017. &ldquo;An Argument for Why the Future May Be Good.&rdquo; </span><span>Effective Altruism Forum</span><span class="c2">. https://forum.effectivealtruism.org/posts/kNKpyf4WWdKehgvRt/an-argument-for-why-the-future-may-be-good.</span></p><p class="c4"><span>Wolf, Clark. 1997. &ldquo;Person-Affecting Utilitarianism and Population Policy; or, Sissy Jupe&rsquo;s Theory of Social Choice.&rdquo; In </span><span class="c12">Contingent Future Persons</span><span>, eds. Nick Fotion and Jan C. Heller. Dordrecht: Springer Dordrecht. https://doi.org/10.1007/978-94-011-5566-3_9.</span></p><p class="c4"><span class="c5">Yudkowsky, Eliezer. 2004. &ldquo;Coherent Extrapolated Volition.&rdquo; The Singularity Institute. https://intelligence.org/files/CEV.pdf.</span></p><p class="c4"><span class="c5">&mdash;&mdash;&mdash;. 2007. &ldquo;The Hidden Complexity of Wishes.&rdquo; LessWrong. https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes.</span></p><hr class="c50"><div><p class="c8 c23"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c38">&nbsp;</span><span>For the sake of brevity, while I have my own views of moral value and disvalue, I don&rsquo;t tie this essay to any particular view (e.g., utilitarianism). For example, it can include subjective goods (valuable for a person) and objective goods (valuable regardless of people), and it can be understood as estimates or direct observation of realist good (stance-independent) or anti-realist good (stance-dependent). Some may also have moral aims aside from maximizing expected &ldquo;value&rdquo; per se, at least for certain senses of &ldquo;expected&rdquo; and &ldquo;value.&rdquo; There is a substantial philosophical literature on such topics that I will not wade into, and I believe such non-value-based arguments can be mapped onto value-based arguments with minimal loss (e.g., not having a duty to make happy people can be mapped onto there being no value in making happy people). In general, I will try to keep this blog post modular and at most briefly summarize extant literature.</span></p></div><div><p class="c8 c23"><a href="#ftnt_ref2" id="ftnt2">[2]</a><span>&nbsp;</span><span>Both population risks and quality risks can be </span><span class="c0 c12"><a class="c1" href="https://forum.effectivealtruism.org/topics/existential-risk">existential risks</a></span><span>&mdash;though </span><span>longtermist</span><span>&nbsp;EAs have usually defaulted to a focus on population risks, particularly extinction risks.</span></p></div><div><p class="c8 c23"><a href="#ftnt_ref3" id="ftnt3">[3]</a><span class="c38">&nbsp;</span><span>For the sake of brevity, I analyze human survival and interstellar colonization together under the label &ldquo;human expansion.&rdquo; I gloss over possible futures in which humanity survives but does not </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/space-colonization">colonize space</a></span><span>.</span></p></div><div><p class="c8 c23"><a href="#ftnt_ref4" id="ftnt4">[4]</a><span>&nbsp;For example, t</span><span>he portion of historical progress made through market mechanisms is split among </span><span class="c0"><a class="c1" href="#h.i8jadqryh40t">Historical Progress</a></span><span>&nbsp;insofar as this is a large historical trend, </span><span class="c0"><a class="c1" href="#h.5baytopn0b3n">Value Through Intent</a></span><span>&nbsp;insofar as humans intentionally progressed in this way, </span><span class="c0"><a class="c1" href="#h.i5hbkkxggbpk">Value Through Evolution</a></span><span>&nbsp;insofar as selection increased the prevalence of these mechanisms, and </span><span class="c0"><a class="c1" href="#h.hufvw453drm2">Reasoned Cooperation</a></span><span>&nbsp;insofar as the intentional change was through reasoned cooperation</span><span>. How is this splitting calculated? I punt to future work, but in general, I mean some sort of causal attribution measure. For example, if I grow an apple tree that is caused by both rain and soil nutrients, then I would assign more causal force to rain if and only if reducing rain by one standard deviation would inhibit growth more than reducing soil nutrients by one standard deviation. Related measures include </span><span class="c0"><a class="c1" href="https://en.wikipedia.org/wiki/Shapley_value">Shapley values</a></span><span>&nbsp;and </span><span class="c0"><a class="c1" href="https://stats.stackexchange.com/questions/379744/comparison-between-shap-shapley-additive-explanation-and-lime-local-interpret">LIME</a></span><span class="c2">.</span></p></div><div><p class="c8 c23"><a href="#ftnt_ref5" id="ftnt5">[5]</a><span>&nbsp;I do not provide specific explanations for these weights because they are meant as intuitive, subjective estimates of the linear weight of the argument as laid out in the description column. As discussed elsewhere, unpacking these weights into probability distributions and back-of-the-envelope estimates is a promising direction for better estimating the EV of human expansion.</span></p></div><div><p class="c8 c23"><a href="#ftnt_ref6" id="ftnt6">[6]</a><span class="c38">&nbsp;</span><span>I do not provide specific explanations for the weights in </span><span class="c0"><a class="c1" href="https://docs.google.com/spreadsheets/d/1t-OM6eh_XBMUierl7nhSJ7LcjnRmU3ACyMWk5E_PKXs/">the spreadsheet</a></span><span>&nbsp;because they are meant as intuitive, subjective estimates of the linear weight of the argument as laid out in the description column. As discussed elsewhere, unpacking these weights into probability distributions and back-of-the-envelope estimates is a promising direction for better estimating the EV of human expansion.</span><span>&nbsp;</span><span>The evaluations rely on a wide range of empirical, conceptual, and intuitive evidence. </span><span>These numbers should be taken with many grains of salt, but as the &ldquo;superforecasting&rdquo; literature evidences, it can be useful to </span><span class="c0"><a class="c1" href="https://jacyanthis.com/big-questions">quantify seemingly hard-to-quantify questions</a></span><span>. The weights in this table are meant as linear, and t</span><span>he linear sum is -7</span><span>. There are many approaches we could take to aggregating such evidence, reasoning, and intuitions; we could entirely avoid quantification entirely and take the gestalt of these arguments. If taken as logarithms of 2 (e.g., take 0 as 0, take 1 as 2, take 10 as 2^10=1024) as the prior that EA arguments tend to vary in weight by doubling rather than linear scaling, then the mean is </span><span>-410</span><span>. Again, these are just two of the many possible ways to aggregate arguments on this topic. Also, for methodological clarity at the risk of droning, I assign weights constantly across arguments (e.g., 2 arguments of weight +2 are the same evidential weight as 4 arguments of weight +1), though other assignment methods are reasonable, and again, other divisions of the arguments (i.e., other numbers of rows in the table) are reasonable and would make no difference in my own additive total, though they could change the exponential total and other aggregations.</span></p></div><div><p class="c8 c23"><a href="#ftnt_ref7" id="ftnt7">[7]</a><span>&nbsp;In my opinion, there are many different values involved in developing and deploying an AI system, so the distinction between </span><span class="c0"><a class="c1" href="https://www.alignmentforum.org/tag/inner-alignment">inner</a></span><span>&nbsp;and </span><span class="c0"><a class="c1" href="https://www.alignmentforum.org/tag/outer-alignment">outer</a></span><span>&nbsp;alignment is rarely precise in practice. Much of identifying and aligning with &ldquo;good&rdquo; or &ldquo;correct&rdquo; values can be described as outer alignment. In general, I think of AI value alignment as a long series of mechanisms from the causal factors that create human values (which themselves can be thought of as objective functions) to a tangled web of objectives in each human brain (e.g., values, desires, preferences) to a tangled web of social objectives aggregated across humans (e.g., voting, debates, parliaments, marketplaces) to a tangled web of objectives communicated from humans to machines (e.g., material values in game-playing AI, training data, training labels, architectures) to a tangled web of emergent objectives in the machines (e.g., parametric architectures in the neural net, (smoothed) sets of possible actions in domain, (smoothed) sets of possible actions out of domain) and finally to the machine actions (i.e., what it actually does in the world). We can reasonably refer to the alignment of any of these objects with any of the other objects in this long, tangled continuum of values. Two examples of outer alignment work that I have in mind here are </span><span class="c0"><a class="c1" href="https://arxiv.org/abs/2112.00861">Askell et al. (2021) &ldquo;A General Language Assistant as a Laboratory for Alignment&rdquo;</a></span><span>&nbsp;and Hobbhan et al. (2022) </span><span class="c0"><a class="c1" href="https://www.lesswrong.com/posts/XyBWkoaqfnuEyNWXi/reflection-mechanisms-as-an-alignment-target-a-survey-1">&ldquo;Reflection Mechanisms as an Alignment Target: A Survey.&rdquo;</a></span></p></div><div><p class="c8 c23"><a href="#ftnt_ref8" id="ftnt8">[8]</a><span>&nbsp;I&rsquo;m not persuaded by, and I don&rsquo;t account for, </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/tag/moral-uncertainty">moral uncertainty</a></span><span>&nbsp;because I don&rsquo;t think a </span><span class="c0"><a class="c1" href="#h.d7d29jutple6">&ldquo;Discoverable Moral Reality&rdquo;</a></span><span>&nbsp;is plausible, and I doubt I would be persuaded to act in accordance with it if it did exist (e.g., to cause suffering if suffering were stance-indepently good)&mdash;though it is unclear what it would even mean for a vague, stance-independent phenomenon to exist </span><span class="c0"><a class="c1" href="https://jacyanthis.com/Consciousness_Semanticism.pdf">(Anthis 2022)</a></span><span>. Moreover, I&rsquo;m not compelled by arguments to account for any sort of anti-realist moral uncertainty, views which are arguably better not even described as &ldquo;uncertainty&rdquo; (</span><span>e.g., weighting my future self&rsquo;s morals, such as after a personality-altering brain injury</span><span>&nbsp;or taking rationality- and intelligence-increasing nootropics; across different moral frameworks, such as in a moral parliament, e.g., </span><span class="c0"><a class="c1" href="https://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html">Bostrom 2009</a></span><span>). Of course, I still account for </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/moral-cooperation">moral cooperation</a></span><span>&nbsp;and </span><span class="c0"><a class="c1" href="https://forum.effectivealtruism.org/topics/bayes-theorem">standard empirical uncertainty</a></span><span>.</span></p></div><div><p class="c8 c23"><a href="#ftnt_ref9" id="ftnt9">[9]</a><span>&nbsp;There are many things to say about how </span><span class="c0"><a class="c1" href="https://en.wikipedia.org/wiki/Aumann%2527s_agreement_theorem">Aumann&rsquo;s Agreement Theorem</a></span><span>&nbsp;obtains in the real world. For example, Andrew Critch </span><span class="c0"><a class="c1" href="https://www.lesswrong.com/posts/u7o7HtChnZ5x8SqvA/axrp-episode-3-negotiable-reinforcement-learning-with-andrew">states</a></span><span>&nbsp;that the &ldquo;common priors&rdquo; assumption &ldquo;seems extremely unrealistic for the real world.&rdquo; I&rsquo;m not sure if I disagree with this, but when I describe Aumann updating, I&rsquo;m not referring to a specific prior-to-posterior Bayesian update; I&rsquo;m referring to the equal treatment of all the evidence going into my belief with all the evidence going into my interlocutor&rsquo;s belief. If nothing else, this can be viewed as an aggregation of evidence in which each agent is still left with aggregating their evidence and prior, but I don&rsquo;t like approaching such questions with a bright line between prior and posterior except in a specific prior-to-posterior Bayesian update (e.g., you believe the sky is blue but then walk outside one day and see it looks red; how should this change your belief?)</span><span>.</span></p></div></body></html>

</div>




    <hr>
    <div class="container newsletter-container ">
      <p>Subscribe to our newsletter to receive updates on our research and activities. We average one to two emails per year.</p>
      <div id="mc_embed_signup">
        <form action="//sentienceinstitute.us15.list-manage.com/subscribe/post?u=d898f823d035e0601866e68d6&amp;id=cbf2d915a6" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
          <div id="mc_embed_signup_scroll">
            <input type="email" value="" name="EMAIL" class="email form-input" id="mce-EMAIL" placeholder="Email address" required>
            <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
            <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_d898f823d035e0601866e68d6_cbf2d915a6" tabindex="-1" value=""></div>
            <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
          </div>
        </form>
      </div>
    </div>
    
    <footer class="footer">
      <div class="container">
        <div class="row">
          <div class="col-md-2">
            <div><span class="bold">Contact us: </span><a href="mailto:info@sentienceinstitute.org">info@sentienceinstitute.org</a></div>
            <div class="icons">
              <!-- <a href="/rss.xml"><i class="material-icons">rss_feed</i></a> -->
              <a href="https://www.facebook.com/sentienceinstitute"><img class="icon" src="../img/icons/icon_facebook_white.png"/></a>
              <a href="https://www.twitter.com/sentienceinst"><img class="icon" src="../img/icons/icon_twitter_white.png"/></a>
            </div>
          </div>
          <div class="col-md-10 last-column">
            <div>
               2018 Sentience Institute
            </div>
            <div>
              <a href="/terms">Terms and Conditions &amp; Privacy Policy</a>
            </div>
            <div>
              Thank you, <a href="https://weanimals.org/">Jo-Anne McArthur</a>, for granting us the use of so many photos.
            </div>
          </div>
        </div>
      </div>
    </footer>
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/1.19.1/TweenMax.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js" integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/js/bootstrap.min.js" integrity="sha384-vBWWzlZJ8ea9aCX4pEW3rVHjgjt7zpkNpZk+02D9phzyeVkE+jo0ieGizqPLForn" crossorigin="anonymous"></script>
    <script src="/js/ready.js?v=@version@"></script>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-100318911-1', 'auto');
      ga('send', 'pageview');

    </script>
    
  </body>
</html>
